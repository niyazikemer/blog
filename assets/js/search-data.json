{
  
    
        "post0": {
            "title": "Resnet - Implementation from scratch",
            "content": "This is my sister&#39;s dog. He&#39;s not really a good boy but that&#39;s ok we still love him. . What is Resnet? . Video 1 - What is Resnet? &amp; Implementation of the Basic Block - Resnet From Scratch . . from fastbook: Resnet: chapter-14 . Note: In this chapter, we will build on top of the CNNs introduced in the previous chapter and explain to you the ResNet (residual network) architecture. It was introduced in 2015 by Kaiming He et al. in the article &quot;Deep Residual Learning for Image Recognition&quot; and is by far the most used model architecture nowadays. More recent developments in image models almost always use the same trick of residual connections, and most of the time, they are just a tweak of the original ResNet. . Training of networks of different depth (courtesy of Kaiming He et al.) . . Note: In 2015, the authors of the ResNet paper noticed something that they found curious. Even after using batchnorm, they saw that a network using more layers was doing less well than a network using fewer layers—and there were no other differences between the models. Most interestingly, the difference was observed not only in the validation set, but also in the training set; so, it wasn&#8217;t just a generalization issue, but a training issue. As the paper explains: . . How Resnet works. . from fastbook: Resnet: chapter-14 . Note: Again, this is rather inaccessible prose—so let&#8217;s try to restate it in plain English! If the outcome of a given layer is x, when using a ResNet block that returns y = x+block(x) we&#8217;re not asking the block to predict y, we are asking it to predict the difference between y and x. So the job of those blocks isn&#8217;t to predict certain features, but to minimize the error between x and the desired y. A ResNet is, therefore, good at learning about slight differences between doing nothing and passing though a block of two convolutional layers (with trainable weights). This is how these models got their name: they&#8217;re predicting residuals (reminder: &quot;residual&quot; is prediction minus target). . . What does that mean? . A simple ResNet block (courtesy of Kaiming He et al.) . Resnet Stages, Basic Blocks and Layers. . ResNet Architecture (courtesy of Kaiming He et al.) . What does Basic Block look like? . A simple ResNet block (courtesy of Kaiming He et al.) . . create a basic block/test the block with random tensor with random channel numbers/check downsample . Pytorch Resnet Implementation . import torch import torch.nn as nn import torchvision.models as models . models.resnet34() . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True) ) . Resnet From Scratch . Start with basic blocks: . class BasicBlock(nn.Module): def __init__(self,in_chs, out_chs): super().__init__() if in_chs==out_chs: self.stride=1 else: self.stride=2 self.conv1 = nn.Conv2d(in_chs,out_chs,kernel_size=3, padding=1,stride=self.stride,bias=False) self.bn1 = nn.BatchNorm2d(out_chs) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(out_chs,out_chs,kernel_size=3, padding=1,stride=1,bias=False) self.bn2 = nn.BatchNorm2d(out_chs) if in_chs==out_chs: self.downsample=None else: self.downsample= nn.Sequential(#nn.AvgPool2d(2,2), nn.Conv2d(in_chs,out_chs, kernel_size=1,stride=2,bias=False), nn.BatchNorm2d(out_chs)) def forward(self,x): skip_conn=x x=self.conv1(x) x=self.bn1(x) x=self.relu(x) x=self.conv2(x) x=self.bn2(x) if self.downsample: skip_conn=self.downsample(skip_conn) x+=skip_conn x=self.relu(x) return x . . Note: Turns out AvgPool experiment didn&#8217;t work. I&#8217;d thought that maybe getting an avarage of channels before conv2d downsampling could improve the result since kernel size 1 and stride 2 couse some information loss. (I believe :-)) . x=torch.randn(1,64,112,112) basic_block=BasicBlock(64,128) basic_block(x).shape . torch.Size([1, 128, 56, 56]) . . Note: This is what I expect. Deccrease the image size by half and double the number of channels. . #input = torch.randn(1, 64, 128, 128) #output = m(input) #output.shape . . Note: &#8217;repeat&#8217; is repeatitation of Basic blocks in corresponding stages. . repeat=[3,4,6,3] channels=[64,128,256,512] . Visualize channels and stages. . in_chans=64 for sta,(rep,out_chans) in enumerate(zip(repeat,channels)): for n in range(rep): print(sta,in_chans,out_chans) in_chans=out_chans . 0 64 64 0 64 64 0 64 64 1 64 128 1 128 128 1 128 128 1 128 128 2 128 256 2 256 256 2 256 256 2 256 256 2 256 256 2 256 256 3 256 512 3 512 512 3 512 512 . Create Basic Block Stages . def make_block(basic_b=BasicBlock,repeat=[3,4,6,3],channels=[64,128,256,512]): in_chans=channels[0] stages=[] for sta,(rep,out_chans) in enumerate(zip(repeat,channels)): blocks=[] for n in range(rep): blocks.append(basic_b(in_chans,out_chans)) #print(sta,in_chans,out_chans) in_chans=out_chans stages.append((f&#39;conv{sta+2}_x&#39;,nn.Sequential(*blocks))) #print(stages) return stages . Complete the Resnet implementation . class ResneTTe34(nn.Module): def __init__(self,num_classes): super().__init__() #stem self.conv1=nn.Conv2d(3,64, kernel_size=7, stride=2,padding=3,bias=False) self.bn1=nn.BatchNorm2d(64) self.relu=nn.ReLU() self.max_pool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1) # res-stages self.stage_modules= make_block() for stage in self.stage_modules: self.add_module(*stage) self.avg_pool=nn.AdaptiveAvgPool2d(output_size=(1,1)) self.fc=nn.Linear(512,num_classes,bias=True) #self.softmax=nn.Softmax(dim=1) def forward(self,x): x=self.conv1(x) x=self.bn1(x) x=self.relu(x) x=self.max_pool(x) x=self.conv2_x(x) x=self.conv3_x(x) x=self.conv4_x(x) x=self.conv5_x(x) x=self.avg_pool(x) x=torch.flatten(x,1) x=self.fc(x) #x=self.softmax(x) return x . . Note: I don&#8217;t why but softmax didn&#8217;t work well. . x=torch.randn(1,3,224,224) my_resnette=ResneTTe34(10) my_resnette(x).shape . torch.Size([1, 10]) . after conv layers: torch.Size([1, 512, 7, 7]) | after avg_pool: torch.Size([1, 512, 1, 1]) | after flatten: torch.Size([1, 512]) | after fc: torch.Size([1, 10]) | . Training with Fast AI . Video - 2 - Resnet Class Implementation - Resnet From Scratch . . import fastbook fastbook.setup_book() from fastai.vision.all import * . Dataset: IMAGENETTE_160 . A subset of 10 easily classified classes from Imagenet: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute. . fast.ai imagenette dataset . path = untar_data(URLs.IMAGENETTE_160) data_block=DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(160), batch_tfms=[*aug_transforms(min_scale=0.5, size=160), Normalize.from_stats(*imagenet_stats)], ) dls = data_block.dataloaders(path, bs=512) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . dls.c . 10 . . Note: dls.c Number of classes in the dataloaders. . dls.show_batch(max_n=12) . New Resnet instance: . rn=ResneTTe34(10) rn . ResneTTe34( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (conv2_x): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (conv3_x): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (conv4_x): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (conv5_x): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=10, bias=True) ) . Create a learner. . . Note: Detailed information about fastai learner class: Documentation . learn = Learner(dls, rn, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . . Note: Learn more about fastai learning rate finder: here . learn.lr_find() . SuggestedLRs(valley=0.00010964782268274575) . The Learning Rate Finder . Cyclical Learning Rates for Training Neural Networks . Training: . learn.fit_one_cycle(20, 0.000109) . epoch train_loss valid_loss accuracy time . 0 | 2.380851 | 2.387236 | 0.103439 | 00:10 | . 1 | 2.212096 | 2.009294 | 0.277962 | 00:11 | . 2 | 1.998115 | 1.962252 | 0.363312 | 00:11 | . 3 | 1.782758 | 1.822328 | 0.417580 | 00:11 | . 4 | 1.590943 | 1.815316 | 0.444586 | 00:11 | . 5 | 1.441378 | 1.804929 | 0.473121 | 00:11 | . 6 | 1.309173 | 1.451521 | 0.548025 | 00:11 | . 7 | 1.202027 | 1.469768 | 0.544204 | 00:11 | . 8 | 1.106232 | 1.137969 | 0.628790 | 00:10 | . 9 | 1.023413 | 1.444246 | 0.564331 | 00:11 | . 10 | 0.944548 | 1.267145 | 0.609936 | 00:11 | . 11 | 0.878883 | 1.263278 | 0.608662 | 00:10 | . 12 | 0.824138 | 1.046869 | 0.673631 | 00:11 | . 13 | 0.780820 | 0.930782 | 0.704713 | 00:11 | . 14 | 0.738063 | 0.935515 | 0.697070 | 00:10 | . 15 | 0.693144 | 0.887937 | 0.720510 | 00:11 | . 16 | 0.660048 | 1.019186 | 0.687643 | 00:10 | . 17 | 0.623636 | 0.907101 | 0.716688 | 00:10 | . 18 | 0.594946 | 0.891949 | 0.715924 | 00:11 | . 19 | 0.583335 | 0.914399 | 0.711083 | 00:11 | . Some Results: . Original Resnet34 lrfind graph was smoother and I am investigating now . | My Implementation baseline accuracy: %62, PyTorch resnet implementation:74 . | after setting linear chanel bias=True: %64 . | after setting conv layers bias=False: %65 . | after softmax removed: %72 . | training 50 epochs IMAGENETTE_160 :%78 . | training 20 epochs with bigger images IMAGENETTE_320 :%82 . | . Video - 3 - Training &#39;My Resnet&#39; - Resnet From Scratch . . Pytorch&#39;s Resnet34 implementation for Benchmark . resnet=models.resnet34() . learn_resnet = Learner(dls, resnet, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . learn_resnet.lr_find() . SuggestedLRs(valley=0.0004786300996784121) . Looks smoother. . learn_resnet.fit_one_cycle(20, 0.000478) . epoch train_loss valid_loss accuracy time . 0 | 6.478988 | 6.033509 | 0.117197 | 00:12 | . 1 | 5.071272 | 4.286289 | 0.204331 | 00:11 | . 2 | 3.680538 | 2.191735 | 0.353376 | 00:11 | . 3 | 2.801381 | 3.466609 | 0.344204 | 00:12 | . 4 | 2.248471 | 1.444590 | 0.537580 | 00:12 | . 5 | 1.869702 | 2.074517 | 0.430064 | 00:12 | . 6 | 1.602450 | 1.283586 | 0.594140 | 00:12 | . 7 | 1.397508 | 1.226493 | 0.589299 | 00:12 | . 8 | 1.226750 | 0.929667 | 0.713885 | 00:12 | . 9 | 1.112665 | 1.189254 | 0.607898 | 00:12 | . 10 | 1.005993 | 1.106539 | 0.647643 | 00:12 | . 11 | 0.917422 | 1.082780 | 0.669045 | 00:12 | . 12 | 0.841141 | 1.346959 | 0.602803 | 00:12 | . 13 | 0.777760 | 0.885834 | 0.729682 | 00:12 | . 14 | 0.713249 | 1.043039 | 0.678981 | 00:12 | . 15 | 0.660397 | 1.161693 | 0.661656 | 00:12 | . 16 | 0.619790 | 0.947324 | 0.710318 | 00:12 | . 17 | 0.576243 | 0.822031 | 0.744204 | 00:12 | . 18 | 0.549599 | 0.861511 | 0.734522 | 00:12 | . 19 | 0.519339 | 0.836370 | 0.741656 | 00:11 | . A little better result. . A test for 50 epochs. (%5 better) . rn_higher_epoch=ResneTTe34(10) learn_higher_epoch = Learner(dls, rn_higher_epoch, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . learn_higher_epoch.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . SuggestedLRs(valley=0.00013182566908653826) . learn_higher_epoch.fit_one_cycle(50, 0.000131) . epoch train_loss valid_loss accuracy time . 0 | 2.377973 | 2.386434 | 0.125860 | 00:11 | . 1 | 2.262577 | 2.187544 | 0.198981 | 00:11 | . 2 | 2.131813 | 2.076762 | 0.288153 | 00:11 | . 3 | 1.982456 | 1.810484 | 0.386497 | 00:11 | . 4 | 1.822840 | 2.068272 | 0.374777 | 00:11 | . 5 | 1.665031 | 1.880948 | 0.445096 | 00:11 | . 6 | 1.527851 | 1.564145 | 0.496306 | 00:11 | . 7 | 1.407204 | 1.601933 | 0.506242 | 00:11 | . 8 | 1.302459 | 1.382655 | 0.555414 | 00:11 | . 9 | 1.205924 | 1.412100 | 0.577580 | 00:11 | . 10 | 1.120911 | 1.306462 | 0.595669 | 00:11 | . 11 | 1.056995 | 1.286385 | 0.591338 | 00:11 | . 12 | 0.991353 | 1.091827 | 0.653503 | 00:11 | . 13 | 0.939019 | 0.991272 | 0.692229 | 00:11 | . 14 | 0.889291 | 1.482610 | 0.576560 | 00:11 | . 15 | 0.843552 | 1.134017 | 0.648662 | 00:12 | . 16 | 0.798295 | 1.556873 | 0.567898 | 00:11 | . 17 | 0.750999 | 1.271683 | 0.629299 | 00:11 | . 18 | 0.714964 | 1.453092 | 0.597452 | 00:12 | . 19 | 0.693228 | 1.105892 | 0.668025 | 00:11 | . 20 | 0.664791 | 1.161920 | 0.665733 | 00:11 | . 21 | 0.633090 | 1.229070 | 0.645605 | 00:11 | . 22 | 0.615386 | 0.935700 | 0.711847 | 00:11 | . 23 | 0.581805 | 1.142192 | 0.669554 | 00:11 | . 24 | 0.558872 | 1.138849 | 0.676688 | 00:12 | . 25 | 0.538580 | 0.860732 | 0.742166 | 00:12 | . 26 | 0.515516 | 0.977736 | 0.709554 | 00:12 | . 27 | 0.501689 | 1.064165 | 0.684076 | 00:12 | . 28 | 0.477749 | 0.935355 | 0.724586 | 00:12 | . 29 | 0.450755 | 0.950616 | 0.712611 | 00:12 | . 30 | 0.417243 | 1.084173 | 0.695032 | 00:11 | . 31 | 0.406682 | 1.021061 | 0.713885 | 00:11 | . 32 | 0.387076 | 1.004474 | 0.714140 | 00:11 | . 33 | 0.359906 | 0.797884 | 0.766115 | 00:11 | . 34 | 0.337195 | 0.814482 | 0.771465 | 00:11 | . 35 | 0.317810 | 0.904211 | 0.754395 | 00:11 | . 36 | 0.305618 | 0.925233 | 0.745732 | 00:11 | . 37 | 0.290424 | 0.813434 | 0.765096 | 00:11 | . 38 | 0.270168 | 0.856214 | 0.763312 | 00:12 | . 39 | 0.254928 | 0.915280 | 0.753631 | 00:11 | . 40 | 0.240487 | 0.881618 | 0.754395 | 00:11 | . 41 | 0.223493 | 0.808955 | 0.778344 | 00:11 | . 42 | 0.213208 | 0.911270 | 0.758726 | 00:12 | . 43 | 0.198452 | 0.811407 | 0.776306 | 00:11 | . 44 | 0.189459 | 0.895555 | 0.754904 | 00:11 | . 45 | 0.187922 | 0.868764 | 0.765350 | 00:11 | . 46 | 0.180785 | 0.811440 | 0.775541 | 00:11 | . 47 | 0.178707 | 0.826532 | 0.773503 | 00:11 | . 48 | 0.174185 | 0.817364 | 0.775796 | 00:11 | . 49 | 0.170915 | 0.805405 | 0.779873 | 00:11 | . Another test with bigger images. (IMAGENETTE_320) . path_320 = untar_data(URLs.IMAGENETTE_320) data_block_320=DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(320), batch_tfms=[*aug_transforms(min_scale=0.5, size=224), Normalize.from_stats(*imagenet_stats)], ) dls_320 = data_block_320.dataloaders(path_320, bs=256) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . rn_IM_320=ResneTTe34(10) learn__IM_320 = Learner(dls_320, rn_IM_320, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . learn__IM_320.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . SuggestedLRs(valley=0.00010964782268274575) . learn__IM_320.fit_one_cycle(20, 0.0001096) . epoch train_loss valid_loss accuracy time . 0 | 2.245787 | 2.379868 | 0.198471 | 00:20 | . 1 | 1.967387 | 2.148099 | 0.340127 | 00:20 | . 2 | 1.657673 | 1.542151 | 0.509554 | 00:20 | . 3 | 1.432339 | 1.860907 | 0.455287 | 00:20 | . 4 | 1.241912 | 1.308743 | 0.576051 | 00:20 | . 5 | 1.107367 | 1.259022 | 0.604331 | 00:20 | . 6 | 1.004726 | 1.330731 | 0.598217 | 00:20 | . 7 | 0.916032 | 0.902091 | 0.714140 | 00:20 | . 8 | 0.846182 | 1.346130 | 0.601783 | 00:20 | . 9 | 0.766183 | 0.942265 | 0.716178 | 00:20 | . 10 | 0.710231 | 0.875285 | 0.710828 | 00:20 | . 11 | 0.655968 | 0.812030 | 0.745732 | 00:20 | . 12 | 0.600506 | 0.853020 | 0.732739 | 00:20 | . 13 | 0.553901 | 0.720496 | 0.776051 | 00:20 | . 14 | 0.515389 | 0.691851 | 0.782420 | 00:20 | . 15 | 0.475770 | 0.630577 | 0.800510 | 00:20 | . 16 | 0.432293 | 0.619373 | 0.805860 | 00:20 | . 17 | 0.403326 | 0.561097 | 0.823694 | 00:20 | . 18 | 0.388080 | 0.583485 | 0.814013 | 00:21 | . 19 | 0.378337 | 0.578927 | 0.812739 | 00:20 | . Resources: . Resnet paper by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1512.03385 , https://arxiv.org/pdf/1512.03385.pdf . W&amp;B Paper Reading Group: ResNets by Aman Arora . W&amp;B Fastbook Reading Group — 14. ResNet . Practical Deep Learning for Coders Book (fastbook) . https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb . Live Coding Session on ResNet by Aman Arora . [Classic] Deep Residual Learning for Image Recognition (Paper Explained) by Yannic Kilcher . Andrew Ng Resnet videos. . . .",
            "url": "https://niyazikemer.com/fastbook/2021/10/24/resnet-live.html",
            "relUrl": "/fastbook/2021/10/24/resnet-live.html",
            "date": " • Oct 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Chapter 13 - Convolutional Neural Networks",
            "content": ". #!pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . [[chapter_convolutions]] Convolutional Neural Networks . In &lt;&gt; we learned how to create a neural network recognizing images. We were able to achieve a bit over 98% accuracy at distinguishing 3s from 7s—but we also saw that fastai&#39;s built-in classes were able to get close to 100%. Let&#39;s start trying to close the gap.&lt;/p&gt; In this chapter, we will begin by digging into what convolutions are and building a CNN from scratch. We will then study a range of techniques to improve training stability and learn all the tweaks the library usually applies for us to get great results. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Magic of Convolutions . One of the most powerful tools that machine learning practitioners have at their disposal is feature engineering. A feature is a transformation of the data which is designed to make it easier to model. For instance, the add_datepart function that we used for our tabular dataset preprocessing in &lt;&gt; added date features to the Bulldozers dataset. What kinds of features might we be able to create from images?&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; jargon:Feature engineering: Creating new transformations of the input data in order to make it easier to model. . In the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that. On the other hand, the number 3 is characterized by a diagonal edge in one direction at the top left and bottom right of the digit, the opposite diagonal at the bottom left and top right, horizontal edges at the middle, top, and bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that information as our features, instead of raw pixels? . It turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a convolution. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book! . A convolution applies a kernel across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Applying a kernel to one location . The 7×7 grid to the left is the image we&#39;re going to apply the kernel to. The convolution operation multiplies each element of the kernel by each element of a 3×3 block of the image. The results of these multiplications are then added together. The diagram in &lt;&gt; shows an example of applying a kernel to a single location in the image, the 3×3 block around cell 18.&lt;/p&gt; Let&#39;s do this with code. First, we create a little 3×3 matrix like so: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; top_edge = tensor([[-1,-1,-1], [ 0, 0, 0], [ 1, 1, 1]]).float() . We&#39;re going to call this our kernel (because that&#39;s what fancy computer vision researchers call these). And we&#39;ll need an image, of course: . path = untar_data(URLs.MNIST_SAMPLE) . Path.BASE_PATH = path . im3 = Image.open(path/&#39;train&#39;/&#39;3&#39;/&#39;12.png&#39;) show_image(im3); . Now we&#39;re going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we&#39;ll add them up, like so: . im3_t = tensor(im3) im3_t[0:3,0:3] * top_edge . tensor([[-0., -0., -0.], [0., 0., 0.], [0., 0., 0.]]) . (im3_t[0:3,0:3] * top_edge).sum() . tensor(0.) . Not very interesting so far—all the pixels in the top-left corner are white. But let&#39;s pick a couple of more interesting spots: . df = pd.DataFrame(im3_t[:10,:20]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . . There&#39;s a top edge at cell 5,8. Let&#39;s repeat our calculation there: . (im3_t[4:7,6:9] * top_edge).sum() . tensor(762.) . There&#39;s a right edge at cell 8,18. What does that give us?: . (im3_t[7:10,17:20] * top_edge).sum() . tensor(-29.) . As you can see, this little calculation is returning a high number where the 3×3-pixel square represents a top edge (i.e., where there are low values at the top of the square, and high values immediately underneath). That&#39;s because the -1 values in our kernel have little impact in that case, but the 1 values have a lot. . Let&#39;s look a tiny bit at the math. The filter will take any window of size 3×3 in our images, and if we name the pixel values like this: . $$ begin{matrix} a1 &amp; a2 &amp; a3 a4 &amp; a5 &amp; a6 a7 &amp; a8 &amp; a9 end{matrix}$$ . it will return $-a1-a2-a3+a7+a8+a9$. If we are in a part of the image where $a1$, $a2$, and $a3$ add up to the same as $a7$, $a8$, and $a9$, then the terms will cancel each other out and we will get 0. However, if $a7$ is greater than $a1$, $a8$ is greater than $a2$, and $a9$ is greater than $a3$, we will get a bigger number as a result. So this filter detects horizontal edges—more precisely, edges where we go from bright parts of the image at the top to darker parts at the bottom. . Changing our filter to have the row of 1s at the top and the -1s at the bottom would detect horizontal edges that go from dark to light. Putting the 1s and -1s in columns versus rows would give us filters that detect vertical edges. Each set of weights will produce a different kind of outcome. . Let&#39;s create a function to do this for one location, and check it matches our result from before: . def apply_kernel(row, col, kernel): return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum() . apply_kernel(5,7,top_edge) . tensor(762.) . But note that we can&#39;t apply it to the corner (e.g., location 0,0), since there isn&#39;t a complete 3×3 square there. . Mapping a Convolution Kernel . We can map apply_kernel() across the coordinate grid. That is, we&#39;ll be taking our 3×3 kernel, and applying it to each 3×3 section of our image. For instance, &lt;&gt; shows the positions a 3×3 kernel can be applied to in the first row of a 5×5 image.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Applying a kernel across a grid . To get a grid of coordinates we can use a nested list comprehension, like so: . [[(i,j) for j in range(1,5)] for i in range(1,5)] . [[(1, 1), (1, 2), (1, 3), (1, 4)], [(2, 1), (2, 2), (2, 3), (2, 4)], [(3, 1), (3, 2), (3, 3), (3, 4)], [(4, 1), (4, 2), (4, 3), (4, 4)]] . . Note: Nested List Comprehensions: Nested list comprehensions are used a lot in Python, so if you haven&#8217;t seen them before, take a few minutes to make sure you understand what&#8217;s happening here, and experiment with writing your own nested list comprehensions. . Here&#39;s the result of applying our kernel over a coordinate grid: . rng = range(1,27) top_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng]) show_image(top_edge3); . Looking good! Our top edges are black, and bottom edges are white (since they are the opposite of top edges). Now that our image contains negative numbers too, matplotlib has automatically changed our colors so that white is the smallest number in the image, black the highest, and zeros appear as gray. . We can try the same thing for left edges: . left_edge = tensor([[-1,1,0], [-1,1,0], [-1,1,0]]).float() left_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng]) show_image(left_edge3); . As we mentioned before, a convolution is the operation of applying such a kernel over a grid in this way. In the paper &quot;A Guide to Convolution Arithmetic for Deep Learning&quot; there are many great diagrams showing how image kernels can be applied. Here&#39;s an example from the paper showing (at the bottom) a light blue 4×4 image, with a dark blue 3×3 kernel being applied, creating a 2×2 green output activation map at the top. . Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent Dumoulin and Francesco Visin) . Look at the shape of the result. If the original image has a height of h and a width of w, how many 3×3 windows can we find? As you can see from the example, there are h-2 by w-2 windows, so the image we get has a result as a height of h-2 and a width of w-2. . We won&#39;t implement this convolution function from scratch, but use PyTorch&#39;s implementation instead (it is way faster than anything we could do in Python). . Convolutions in PyTorch . Convolution is such an important and widely used operation that PyTorch has it built in. It&#39;s called F.conv2d (recall that F is a fastai import from torch.nn.functional, as recommended by PyTorch). The PyTorch docs tell us that it includes these parameters: . input:: input tensor of shape (minibatch, in_channels, iH, iW) | weight:: filters of shape (out_channels, in_channels, kH, kW) | . Here iH,iW is the height and width of the image (i.e., 28,28), and kH,kW is the height and width of our kernel (3,3). But apparently PyTorch is expecting rank-4 tensors for both these arguments, whereas currently we only have rank-2 tensors (i.e., matrices, or arrays with two axes). . The reason for these extra axes is that PyTorch has a few tricks up its sleeve. The first trick is that PyTorch can apply a convolution to multiple images at the same time. That means we can call it on every item in a batch at once! . The second trick is that PyTorch can apply multiple kernels at the same time. So let&#39;s create the diagonal-edge kernels too, and then stack all four of our edge kernels into a single tensor: . diag1_edge = tensor([[ 0,-1, 1], [-1, 1, 0], [ 1, 0, 0]]).float() diag2_edge = tensor([[ 1,-1, 0], [ 0, 1,-1], [ 0, 0, 1]]).float() edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge]) edge_kernels.shape . torch.Size([4, 3, 3]) . To test this, we&#39;ll need a DataLoader and a sample mini-batch. Let&#39;s use the data block API: . mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label) dls = mnist.dataloaders(path) xb,yb = first(dls.valid) xb.shape . torch.Size([64, 1, 28, 28]) . By default, fastai puts data on the GPU when using data blocks. Let&#39;s move it to the CPU for our examples: . xb,yb = to_cpu(xb),to_cpu(yb) . One batch contains 64 images, each of 1 channel, with 28×28 pixels. F.conv2d can handle multichannel (i.e., color) images too. A channel is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions [channels, rows, columns]. . We&#39;ll see how to handle more than one channel later in this chapter. Kernels passed to F.conv2d need to be rank-4 tensors: [channels_in, features_out, rows, columns]. edge_kernels is currently missing one of these. We need to tell PyTorch that the number of input channels in the kernel is one, which we can do by inserting an axis of size one (this is known as a unit axis) in the first location, where the PyTorch docs show in_channels is expected. To insert a unit axis into a tensor, we use the unsqueeze method: . edge_kernels.shape,edge_kernels.unsqueeze(1).shape . (torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3])) . This is now the correct shape for edge_kernels. Let&#39;s pass this all to conv2d: . edge_kernels = edge_kernels.unsqueeze(1) . batch_features = F.conv2d(xb, edge_kernels) batch_features.shape . torch.Size([64, 4, 26, 26]) . The output shape shows we gave 64 images in the mini-batch, 4 kernels, and 26×26 edge maps (we started with 28×28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually: . show_image(batch_features[0,0]); . The most important trick that PyTorch has up its sleeve is that it can use the GPU to do all this work in parallel—that is, applying multiple kernels, to multiple images, across multiple channels. Doing lots of work in parallel is critical to getting GPUs to work efficiently; if we did each of these operations one at a time, we&#39;d often run hundreds of times slower (and if we used our manual convolution loop from the previous section, we&#39;d be millions of times slower!). Therefore, to become a strong deep learning practitioner, one skill to practice is giving your GPU plenty of work to do at a time. . It would be nice to not lose those two pixels on each axis. The way we do that is to add padding, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added. . Strides and Padding . With appropriate padding, we can ensure that the output activation map is the same size as the original image, which can make things a lot simpler when we construct our architectures. &lt;&gt; shows how adding padding allows us to apply the kernels in the image corners.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A convolution with padding . With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map, as we can see in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin) . If we add a kernel of size ks by ks (with ks an odd number), the necessary padding on each side to keep the same shape is ks//2. An even number for ks would require a different amount of padding on the top/bottom and left/right, but in practice we almost never use an even filter size. . So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application, as in &lt;&gt;. This is known as a stride-2 convolution. The most common kernel size in practice is 3×3, and the most common padding is 1. As you&#39;ll see, stride-2 convolutions are useful for decreasing the size of our outputs, and stride-1 convolutions are useful for adding layers without changing the output size.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding (courtesy of Vincent Dumoulin and Francesco Visin) . In an image of size h by w, using a padding of 1 and a stride of 2 will give us a result of size (h+1)//2 by (w+1)//2. The general formula for each dimension is (n + 2*pad - ks)//stride + 1, where pad is the padding, ks, the size of our kernel, and stride is the stride. . Let&#39;s now take a look at how the pixel values of the result of our convolutions are computed. . Understanding the Convolution Equations . To explain the math behind convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing CNNs from different viewpoints. In fact, it&#39;s so clever, and so helpful, we&#39;re going to show it here too! . Here&#39;s our 3×3 pixel image, with each pixel labeled with a letter: . . And here&#39;s our kernel, with each weight labeled with a Greek letter: . . Since the filter fits in the image four times, we have four results: . . &lt;&gt; shows how we applied the kernel to each section of the image to yield each result.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Applying the kernel . The equation view is in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The equation . Notice that the bias term, b, is the same for each section of the image. You can consider the bias as part of the filter, just like the weights (α, β, γ, δ) are part of the filter. . Here&#39;s an interesting insight—a convolution can be represented as a special kind of matrix multiplication, as illustrated in &lt;&gt;. The weight matrix is just like the ones from traditional neural networks. However, this weight matrix has two special properties:&lt;/p&gt; The zeros shown in gray are untrainable. This means that they’ll stay zero throughout the optimization process. | Some of the weights are equal, and while they are trainable (i.e., changeable), they must remain equal. These are called shared weights. | The zeros correspond to the pixels that the filter can&#39;t touch. Each row of the weight matrix corresponds to one application of the filter. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Convolution as matrix multiplication . Now that we understand what a convolution is, let&#39;s use them to build a neural net. . Our First Convolutional Neural Network . There is no reason to believe that some particular edge filters are the most useful kernels for image recognition. Furthermore, we&#39;ve seen that in later layers convolutional kernels become complex transformations of features from lower levels, but we don&#39;t have a good idea of how to manually construct these. . Instead, it would be best to learn the values of the kernels. We already know how to do this—SGD! In effect, the model will learn the features that are useful for classification. . When we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network (CNN). . Creating the CNN . Let&#39;s go back to the basic neural network we had in &lt;&gt;. It was defined like this:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . We can view a model&#39;s definition: . simple_net . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . We now want to create a similar architecture to this linear model, but using convolutional layers instead of linear. nn.Conv2d is the module equivalent of F.conv2d. It&#39;s more convenient than F.conv2d when creating an architecture, because it creates the weight matrix for us automatically when we instantiate it. . Here&#39;s a possible architecture: . broken_cnn = sequential( nn.Conv2d(1,30, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(30,1, kernel_size=3, padding=1) ) . One thing to note here is that we didn&#39;t need to specify 28×28 as the input size. That&#39;s because a linear layer needs a weight in the weight matrix for every pixel, so it needs to know how many pixels there are, but a convolution is applied over each pixel automatically. The weights only depend on the number of input and output channels and the kernel size, as we saw in the previous section. . Think about what the output shape is going to be, then let&#39;s try it and see: . broken_cnn(xb).shape . torch.Size([64, 1, 28, 28]) . This is not something we can use to do classification, since we need a single output activation per image, not a 28×28 map of activations. One way to deal with this is to use enough stride-2 convolutions such that the final layer is size 1. That is, after one stride-2 convolution the size will be 14×14, after two it will be 7×7, then 4×4, 2×2, and finally size 1. . Let&#39;s try that now. First, we&#39;ll define a function with the basic parameters we&#39;ll use in each convolution: . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res . . Important: Refactoring: Refactoring parts of your neural networks like this makes it much less likely you&#8217;ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing. . When we use a stride-2 convolution, we often increase the number of features at the same time. This is because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time. . jargon:channels and features: These two terms are largely used interchangeably, and refer to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution. Features is never used to refer to the input data, but channels can refer to either the input data (generally channels are colors) or activations inside the network. . Here is how we can build a simple CNN: . simple_cnn = sequential( conv(1 ,4), #14x14 conv(4 ,8), #7x7 conv(8 ,16), #4x4 conv(16,32), #2x2 conv(32,2, act=False), #1x1 Flatten(), ) . j:I like to add comments like the ones here after each convolution to show how large the activation map will be after each layer. These comments assume that the input size is 28*28 . Now the network outputs two activations, which map to the two possible levels in our labels: . simple_cnn(xb).shape . torch.Size([64, 2]) . We can now create our Learner: . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) . To see exactly what&#39;s going on in the model, we can use summary: . learn.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 14 x 14 Conv2d 40 True ReLU ____________________________________________________________________________ 64 x 8 x 7 x 7 Conv2d 296 True ReLU ____________________________________________________________________________ 64 x 16 x 4 x 4 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 2 x 2 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 2 x 1 x 1 Conv2d 578 True ____________________________________________________________________________ [] Flatten ____________________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f4ec3f2fee0&gt; Loss function: &lt;function cross_entropy at 0x7f4f13a71820&gt; Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Note that the output of the final Conv2d layer is 64x2x1x1. We need to remove those extra 1x1 axes; that&#39;s what Flatten does. It&#39;s basically the same as PyTorch&#39;s squeeze method, but as a module. . Let&#39;s see if this trains! Since this is a deeper network than we&#39;ve built from scratch before, we&#39;ll use a lower learning rate and more epochs: . learn.fit_one_cycle(2, 0.01) . epoch train_loss valid_loss accuracy time . 0 | 0.059677 | 0.040094 | 0.985770 | 00:05 | . 1 | 0.018634 | 0.025003 | 0.990677 | 00:04 | . Success! It&#39;s getting closer to the resnet18 result we had, although it&#39;s not quite there yet, and it&#39;s taking more epochs, and we&#39;re needing to use a lower learning rate. We still have a few more tricks to learn, but we&#39;re getting closer and closer to being able to create a modern CNN from scratch. . Understanding Convolution Arithmetic . We can see from the summary that we have an input of size 64x1x28x28. The axes are batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order. The first layer is: . m = learn.model[0] m . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) . So we have 1 input channel, 4 output channels, and a 3×3 kernel. Let&#39;s check the weights of the first convolution: . m[0].weight.shape . torch.Size([4, 1, 3, 3]) . The summary shows we have 40 parameters, and 4*1*3*3 is 36. What are the other four parameters? Let&#39;s see what the bias contains: . m[0].bias.shape . torch.Size([4]) . We can now use this information to clarify our statement in the previous section: &quot;When we use a stride-2 convolution, we often increase the number of features because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time.&quot; . There is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let&#39;s ignore the batch axis to keep things simple. So for each of 14*14=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that&#39;s 196*288=56_448 multiplications at this layer. The next layer will have 7*7*(1168-16)=56_448 multiplications. . What happened here is that our stride-2 convolution halved the grid size from 14x14 to 7x7, and we doubled the number of filters from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper. But we know that the deeper layers have to compute semantically rich features (such as eyes or fur), so we wouldn&#39;t expect that doing less computation would make sense. . Another way to think of this is based on receptive fields. . Receptive Fields . The receptive field is the area of an image that is involved in the calculation of a layer. On the book&#39;s website, you&#39;ll find an Excel spreadsheet called conv-example.xlsx that shows the calculation of two stride-2 convolutional layers using an MNIST digit. Each layer has a single kernel. &lt;&gt; shows what we see if we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precedents.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Immediate precedents of Conv2 layer . Here, the cell with the green border is the cell we clicked on, and the blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Let&#39;s now click trace precedents again, to see what cells are used to calculate these inputs. &lt;&gt; shows what happens.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Secondary precedents of Conv2 layer . In this example, we have just two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This 7×7 area is the receptive field in the input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers. . As you see from this example, the deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer. A large receptive field means that a large amount of the input image is used to calculate each activation in that layer is. We now know that in the deeper layers of the network we have semantically rich features, corresponding to larger receptive fields. Therefore, we&#39;d expect that we&#39;d need more weights for each of our features to handle this increasing complexity. This is another way of saying the same thing we mentioned in the previous section: when we introduce a stride-2 conv in our network, we should also increase the number of channels. . When writing this particular chapter, we had a lot of questions we needed answers for, to be able to explain CNNs to you as best we could. Believe it or not, we found most of the answers on Twitter. We&#39;re going to take a quick break to talk to you about that now, before we move on to color images. . A Note About Twitter . We are not, to say the least, big users of social networks in general. But our goal in writing this book is to help you become the best deep learning practitioner you can, and we would be remiss not to mention how important Twitter has been in our own deep learning journeys. . You see, there&#39;s another part of Twitter, far away from Donald Trump and the Kardashians, which is the part of Twitter where deep learning researchers and practitioners talk shop every day. As we were writing this section, Jeremy wanted to double-check that what we were saying about stride-2 convolutions was accurate, so he asked on Twitter: . . A few minutes later, this answer popped up: . . Christian Szegedy is the first author of Inception, the 2014 ImageNet winner and source of many key insights used in modern neural networks. Two hours later, this appeared: . . Do you recognize that name? You saw it in &lt;&gt;, when we were talking about the Turing Award winners who established the foundations of deep learning today!&lt;/p&gt; Jeremy also asked on Twitter for help checking our description of label smoothing in &lt;&gt; was accurate, and got a response again from directly from Christian Szegedy (label smoothing was originally introduced in the Inception paper):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Many of the top people in deep learning today are Twitter regulars, and are very open about interacting with the wider community. One good way to get started is to look at a list of Jeremy&#39;s recent Twitter likes, or Sylvain&#39;s. That way, you can see a list of Twitter users that we think have interesting and useful things to say. . Twitter is the main way we both stay up to date with interesting papers, software releases, and other deep learning news. For making connections with the deep learning community, we recommend getting involved both in the fast.ai forums and on Twitter. . That said, let&#39;s get back to the meat of this chapter. Up until now, we have only shown you examples of pictures in black and white, with one value per pixel. In practice, most colored images have three values per pixel to define their color. We&#39;ll look at working with color images next. . Color Images . A colour picture is a rank-3 tensor: . im = image2tensor(Image.open(image_bear())) im.shape . torch.Size([3, 1000, 846]) . show_image(im); . The first axis contains the channels, red, green, and blue: . _,axs = subplots(1,3) for bear,ax,color in zip(im,axs,(&#39;Reds&#39;,&#39;Greens&#39;,&#39;Blues&#39;)): show_image(255-bear, ax=ax, cmap=color) . We saw what the convolution operation was for one filter on one channel of the image (our examples were done on a square). A convolutional layer will take an image with a certain number of channels (three for the first layer for regular RGB color images) and output an image with a different number of channels. Like our hidden size that represented the numbers of neurons in a linear layer, we can decide to have as many filters as we want, and each of them will be able to specialize, some to detect horizontal edges, others to detect vertical edges and so forth, to give something like we studied in &lt;&gt;.&lt;/p&gt; In one sliding window, we have a certain number of channels and we need as many filters (we don&#39;t use the same kernel for all the channels). So our kernel doesn&#39;t have a size of 3 by 3, but ch_in (for channels in) is 3 by 3. On each channel, we multiply the elements of our window by the elements of the coresponding filter, then sum the results (as we saw before) and sum over all the filters. In the example given in &lt;&gt;, the result of our conv layer on that window is red + green + blue.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Convolution over an RGB image . So, in order to apply a convolution to a color picture we require a kernel tensor with a size that matches the first axis. At each location, the corresponding parts of the kernel and the image patch are multiplied together. . These are then all added together, to produce a single number, for each grid location, for each output feature, as shown in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Adding the RGB filters . Then we have ch_out filters like this, so in the end, the result of our convolutional layer will be a batch of images with ch_out channels and a height and width given by the formula outlined earlier. This give us ch_out tensors of size ch_in x ks x ks that we represent in one big tensor of four dimensions. In PyTorch, the order of the dimensions for those weights is ch_out x ch_in x ks x ks. . Additionally, we may want to have a bias for each filter. In the preceding example, the final result for our convolutional layer would be $y_{R} + y_{G} + y_{B} + b$ in that case. Like in a linear layer, there are as many bias as we have kernels, so the biases is a vector of size ch_out. . There are no special mechanisms required when setting up a CNN for training with color images. Just make sure your first layer has three inputs. . There are lots of ways of processing color images. For instance, you can change them to black and white, change from RGB to HSV (hue, saturation, and value) color space, and so forth. In general, it turns out experimentally that changing the encoding of colors won&#39;t make any difference to your model results, as long as you don&#39;t lose information in the transformation. So, transforming to black and white is a bad idea, since it removes the color information entirely (and this can be critical; for instance, a pet breed may have a distinctive color); but converting to HSV generally won&#39;t make any difference. . Now you know what those pictures in &lt;&gt; of &quot;what a neural net learns&quot; from the Zeiler and Fergus paper mean! This is their picture of some of the layer 1 weights which we showed:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . This is taking the three slices of the convolutional kernel, for each output feature, and displaying them as images. We can see that even though the creators of the neural net never explicitly created kernels to find edges, for instance, the neural net automatically discovered these features using SGD. . Now let&#39;s see how we can train these CNNs, and show you all the techniques fastai uses under the hood for efficient training. . Improving Training Stability . Since we are so good at recognizing 3s from 7s, let&#39;s move on to something harder—recognizing all 10 digits. That means we&#39;ll need to use MNIST instead of MNIST_SAMPLE: . path = untar_data(URLs.MNIST) . path.ls() . (#2) [Path(&#39;training&#39;),Path(&#39;testing&#39;)] . The data is in two folders named training and testing, so we have to tell GrandparentSplitter about that (it defaults to train and valid). We did do that in the get_dls function, which we create to make it easy to change our batch size later: . def get_dls(bs=64): return DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;,&#39;testing&#39;), get_y=parent_label, batch_tfms=Normalize() ).dataloaders(path, bs=bs) dls = get_dls() . Remember, it&#39;s always a good idea to look at your data before you use it: . dls.show_batch(max_n=9, figsize=(4,4)) . Now that we have our data ready, we can train a simple model on it. . A Simple Baseline . Earlier in this chapter, we built a model based on a conv function like this: . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res . Let&#39;s start with a basic CNN as a baseline. We&#39;ll use the same one as earlier, but with one tweak: we&#39;ll use more activations. Since we have more numbers to differentiate, it&#39;s likely we will need to learn more filters. . As we discussed, we generally want to double the number of filters each time we have a stride-2 layer. One way to increase the number of filters throughout our network is to double the number of activations in the first layer–then every layer after that will end up twice as big as in the previous version as well. . Important: But there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3×3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn&#8217;t really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they&#8217;re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features: . def simple_cnn(): return sequential( conv(1 ,8, ks=5), #14x14 conv(8 ,16), #7x7 conv(16,32), #4x4 conv(32,64), #2x2 conv(64,10, act=False), #1x1 Flatten(), ) . As you&#39;ll see in a moment, we can look inside our models while they&#39;re training in order to try to find ways to make them train better. To do this we use the ActivationStats callback, which records the mean, standard deviation, and histogram of activations of every trainable layer (as we&#39;ve seen, callbacks are used to add behavior to the training loop; we&#39;ll explore how they work in &lt;&gt;):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.callback.hook import * . We want to train quickly, so that means training at a high learning rate. Let&#39;s see how we go at 0.06: . def fit(epochs=1): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit(epochs, 0.06) return learn . learn = fit() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 2.304775 | 2.307039 | 0.098200 | 00:21 | . This didn&#39;t train at all well! Let&#39;s find out why. . One handy feature of the callbacks passed to Learner is that they are made available automatically, with the same name as the callback class, except in snake_case. So, our ActivationStats callback can be accessed through activation_stats. I&#39;m sure you remember learn.recorder... can you guess how that is implemented? That&#39;s right, it&#39;s a callback called Recorder! . ActivationStats includes some handy utilities for plotting the activations during training. plot_layer_stats(idx) plots the mean and standard deviation of the activations of layer number idx, along with the percentage of activations near zero. Here&#39;s the first layer&#39;s plot: . learn.activation_stats.plot_layer_stats(0) . . Important: Generally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training. Activations near zero are particularly problematic, because it means we have computation in the model that&#8217;s doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer... which will then create more zeros. Here&#8217;s the penultimate layer of our network: . learn.activation_stats.plot_layer_stats(-2) . As expected, the problems get worse towards the end of the network, as the instability and zero activations compound over layers. Let&#39;s look at what we can do to make training more stable. . Increase Batch Size . . Important: One way to make training more stable is to increase the batch size. Larger batches have gradients that are more accurate, since they&#8217;re calculated from more data. On the downside, though, a larger batch size means fewer batches per epoch, which means less opportunities for your model to update weights. Let&#8217;s see if a batch size of 512 helps: . dls = get_dls(512) . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 2.316074 | 2.301938 | 0.113500 | 00:11 | . Let&#39;s see what the penultimate layer looks like: . learn.activation_stats.plot_layer_stats(-2) . Again, we&#39;ve got most of our activations near zero. Let&#39;s see what else we can do to improve training stability. . 1cycle Training . Our initial weights are not well suited to the task we&#39;re trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly, as we&#39;ve seen. We probably don&#39;t want to end training with a high learning rate either, so that we don&#39;t skip over a minimum. But we want to train at a high learning rate for the rest of the training period, because we&#39;ll be able to train more quickly that way. Therefore, we should change the learning rate during training, from low, to high, and then back to low again. . Leslie Smith (yes, the same guy that invented the learning rate finder!) developed this idea in his article &quot;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&quot;. He designed a schedule for learning rate separated into two phases: one where the learning rate grows from the minimum value to the maximum value (warmup), and one where it decreases back to the minimum value (annealing). Smith called this combination of approaches 1cycle training. . Important: 1cycle training allows us to use a much higher maximum learning rate than other types of training, which gives two benefits: . By training with higher learning rates, we train faster—a phenomenon Smith named super-convergence. | By training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss. . Note: The second point is an interesting and subtle one; it is based on the observation that a model that generalizes well is one whose loss would not change very much if you changed the input by a small amount. If a model trains at a large learning rate for quite a while, and can find a good loss when doing so, it must have found an area that also generalizes well, because it is jumping around a lot from batch to batch (that is basically the definition of a high learning rate). The problem is that, as we have discussed, just jumping to a high learning rate is more likely to result in diverging losses, rather than seeing your losses improve. So we don&#8217;t jump straight to a high learning rate. Instead, we start at a low learning rate, where our losses do not diverge, and we allow the optimizer to gradually find smoother and smoother areas of our parameters by gradually going to higher and higher learning rates. Then, once we have found a nice smooth area for our parameters, we want to find the very best part of that area, which means we have to bring our learning rates down again. This is why 1cycle training has a gradual learning rate warmup, and a gradual learning rate cooldown. Many researchers have found that in practice this approach leads to more accurate models and trains more quickly. That is why it is the approach that is used by default for fine_tune in fastai. | . In &lt;&gt; we&#39;ll learn all about momentum in SGD. Briefly, momentum is a technique where the optimizer takes a step not only in the direction of the gradients, but also that continues in the direction of previous steps. Leslie Smith introduced the idea of cyclical momentums in &quot;A Disciplined Approach to Neural Network Hyper-Parameters: Part 1&quot;. It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase.&lt;/p&gt; We can use 1cycle training in fastai by calling fit_one_cycle: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def fit(epochs=1, lr=0.06): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, lr) return learn . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.207010 | 0.077605 | 0.975600 | 00:11 | . We&#39;re finally making some progress! It&#39;s giving us a reasonable accuracy now. . We can view the learning rate and momentum throughout training by calling plot_sched on learn.recorder. learn.recorder (as the name suggests) records everything that happens during training, including losses, metrics, and hyperparameters such as learning rate and momentum: . learn.recorder.plot_sched() . Smith&#39;s original 1cycle paper used a linear warmup and linear annealing. As you can see, we adapted the approach in fastai by combining it with another popular approach: cosine annealing. fit_one_cycle provides the following parameters you can adjust: . lr_max:: The highest learning rate that will be used (this can also be a list of learning rates for each layer group, or a Python slice object containing the first and last layer group learning rates) | div:: How much to divide lr_max by to get the starting learning rate | div_final:: How much to divide lr_max by to get the ending learning rate | pct_start:: What percentage of the batches to use for the warmup | moms:: A tuple (mom1,mom2,mom3) where mom1 is the initial momentum, mom2 is the minimum momentum, and mom3 is the final momentum | . Let&#39;s take a look at our layer stats again: . learn.activation_stats.plot_layer_stats(-2) . The percentage of near-zero weights is getting much better, although it&#39;s still quite high. . We can see even more about what&#39;s going on in our training using color_dim, passing it a layer index: . learn.activation_stats.color_dim(-2) . color_dim was developed by fast.ai in conjunction with a student, Stefano Giomo. Stefano, who refers to the idea as the colorful dimension, provides an in-depth explanation of the history and details behind the method. The basic idea is to create a histogram of the activations of a layer, which we would hope would follow a smooth pattern such as the normal distribution (colorful_dist). . Histogram in &#39;colorful dimension&#39; . To create color_dim, we take the histogram shown on the left here, and convert it into just the colored representation shown at the bottom. Then we flip it on its side, as shown on the right. We found that the distribution is clearer if we take the log of the histogram values. Then, Stefano describes: . :The final plot for each layer is made by stacking the histogram of the activations from each batch along the horizontal axis. So each vertical slice in the visualisation represents the histogram of activations for a single batch. The color intensity corresponds to the height of the histogram, in other words the number of activations in each histogram bin. &lt;&gt; shows how this all fits together.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Summary of the colorful dimension (courtesy of Stefano Giomo) . This illustrates why log(f) is more colorful than f when f follows a normal distribution because taking a log changes the Gaussian in a quadratic, which isn&#39;t as narrow. . So with that in mind, let&#39;s take another look at the result for the penultimate layer: . learn.activation_stats.color_dim(-2) . This shows a classic picture of &quot;bad training.&quot; We start with nearly all activations at zero—that&#39;s what we see at the far left, with all the dark blue. The bright yellow at the bottom represents the near-zero activations. Then, over the first few batches we see the number of nonzero activations exponentially increasing. But it goes too far, and collapses! We see the dark blue return, and the bottom becomes bright yellow again. It almost looks like training restarts from scratch. Then we see the activations increase again, and collapse again. After repeating this a few times, eventually we see a spread of activations throughout the range. . It&#39;s much better if training can be smooth from the start. The cycles of exponential increase and then collapse tend to result in a lot of near-zero activations, resulting in slow training and poor final results. One way to solve this problem is to use batch normalization. . Batch Normalization . To fix the slow training and poor final results we ended up with in the previous section, we need to fix the initial large percentage of near-zero activations, and then try to maintain a good distribution of activations throughout training. . Sergey Ioffe and Christian Szegedy presented a solution to this problem in the 2015 paper &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&quot;. In the abstract, they describe just the problem that we&#39;ve seen: . Important: Training Deep Neural Networks is complicated by the fact that the distribution of each layer&#8217;s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization... We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Their solution, they say is: . Important: Making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. The paper caused great excitement as soon as it was released, because it included the chart in &lt;&gt;, which clearly demonstrated that batch normalization could train a model that was even more accurate than the current state of the art (the Inception architecture) and around 5x faster.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Impact of batch normalization (courtesy of Sergey Ioffe and Christian Szegedy) . Batch normalization (often just called batchnorm) works by taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the activations. However, this can cause problems because the network might want some activations to be really high in order to make accurate predictions. So they also added two learnable parameters (meaning they will be updated in the SGD step), usually called gamma and beta. After normalizing the activations to get some new activation vector y, a batchnorm layer returns gamma*y + beta. . That&#39;s why our activations can have any mean or variance, independent from the mean and standard deviation of the results of the previous layer. Those statistics are learned separately, making training easier on our model. The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training. . Let&#39;s add a batchnorm layer to conv: . def conv(ni, nf, ks=3, act=True): layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)] if act: layers.append(nn.ReLU()) layers.append(nn.BatchNorm2d(nf)) return nn.Sequential(*layers) . and fit our model: . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.139975 | 0.062125 | 0.983400 | 00:13 | . That&#39;s a great result! Let&#39;s take a look at color_dim: . learn.activation_stats.color_dim(-4) . This is just what we hope to see: a smooth development of activations, with no &quot;crashes.&quot; Batchnorm has really delivered on its promise here! In fact, batchnorm has been so successful that we see it (or something very similar) in nearly all modern neural networks. . An interesting observation about models containing batch normalization layers is that they tend to generalize better than models that don&#39;t contain them. Although we haven&#39;t as yet seen a rigorous analysis of what&#39;s going on here, most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and standard deviation than other mini-batches. Therefore, the activations will be normalized by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust to these variations. In general, adding additional randomization to the training process often helps. . Since things are going so well, let&#39;s train for a few more epochs and see how it goes. In fact, let&#39;s increase the learning rate, since the abstract of the batchnorm paper claimed we should be able to &quot;train at much higher learning rates&quot;: . learn = fit(5, lr=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.190594 | 0.121982 | 0.963600 | 00:13 | . 1 | 0.082335 | 0.078569 | 0.976400 | 00:14 | . 2 | 0.053771 | 0.060812 | 0.980500 | 00:13 | . 3 | 0.030423 | 0.029766 | 0.989200 | 00:13 | . 4 | 0.016372 | 0.024753 | 0.991800 | 00:13 | . At this point, I think it&#39;s fair to say we know how to recognize digits! It&#39;s time to move on to something harder... . Conclusions . We&#39;ve seen that convolutions are just a type of matrix multiplication, with two constraints on the weight matrix: some elements are always zero, and some elements are tied (forced to always have the same value). In &lt;&gt; we saw the eight requirements from the 1986 book Parallel Distributed Processing; one of them was &quot;A pattern of connectivity among units.&quot; That&#39;s exactly what these constraints do: they enforce a certain pattern of connectivity.&lt;/p&gt; These constraints allow us to use far fewer parameters in our model, without sacrificing the ability to represent complex visual features. That means we can train deeper models faster, with less overfitting. Although the universal approximation theorem shows that it should be possible to represent anything in a fully connected network in one hidden layer, we&#39;ve seen now that in practice we can train much better models by being thoughtful about network architecture. . Convolutions are by far the most common pattern of connectivity we see in neural nets (along with regular linear layers, which we refer to as fully connected), but it&#39;s likely that many more will be discovered. . We&#39;ve also seen how to interpret the activations of layers in the network to see whether training is going well or not, and how batchnorm helps regularize the training and makes it smoother. In the next chapter, we will use both of those layers to build the most popular architecture in computer vision: a residual network. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Questionnaire . What is a &quot;feature&quot;? | Write out the convolutional kernel matrix for a top edge detector. | Write out the mathematical operation applied by a 3×3 kernel to a single pixel in an image. | What is the value of a convolutional kernel apply to a 3×3 matrix of zeros? | What is &quot;padding&quot;? | What is &quot;stride&quot;? | Create a nested list comprehension to complete any task that you choose. | What are the shapes of the input and weight parameters to PyTorch&#39;s 2D convolution? | What is a &quot;channel&quot;? | What is the relationship between a convolution and a matrix multiplication? | What is a &quot;convolutional neural network&quot;? | What is the benefit of refactoring parts of your neural network definition? | What is Flatten? Where does it need to be included in the MNIST CNN? Why? | What does &quot;NCHW&quot; mean? | Why does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications? | What is a &quot;receptive field&quot;? | What is the size of the receptive field of an activation after two stride 2 convolutions? Why? | Run conv-example.xlsx yourself and experiment with trace precedents. | Have a look at Jeremy or Sylvain&#39;s list of recent Twitter &quot;like&quot;s, and see if you find any interesting resources or ideas there. | How is a color image represented as a tensor? | How does a convolution work with a color input? | What method can we use to see that data in DataLoaders? | Why do we double the number of filters after each stride-2 conv? | Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)? | What information does ActivationStats save for each layer? | How can we access a learner&#39;s callback after training? | What are the three statistics plotted by plot_layer_stats? What does the x-axis represent? | Why are activations near zero problematic? | What are the upsides and downsides of training with a larger batch size? | Why should we avoid using a high learning rate at the start of training? | What is 1cycle training? | What are the benefits of training with a high learning rate? | Why do we want to use a low learning rate at the end of training? | What is &quot;cyclical momentum&quot;? | What callback tracks hyperparameter values during training (along with other information)? | What does one column of pixels in the color_dim plot represent? | What does &quot;bad training&quot; look like in color_dim? Why? | What trainable parameters does a batch normalization layer contain? | What statistics are used to normalize in batch normalization during training? How about during validation? | Why do models with batch normalization layers generalize better? | Further Research . What features other than edge detectors have been used in computer vision (especially before deep learning became popular)? | There are other normalization layers available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed, and how they differ from batch normalization. | Try moving the activation function after the batch normalization layer in conv. Does it make a difference? See what you can find out about what order is recommended, and why. | &lt;/div&gt; . . . . .",
            "url": "https://niyazikemer.com/fastbook/2021/09/04/chapter-13.html",
            "relUrl": "/fastbook/2021/09/04/chapter-13.html",
            "date": " • Sep 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Chapter 8 - Collaborative Filtering Deep Dive",
            "content": "This my dougther at the IKEA very close to our home. . import fastbook fastbook.setup_book() from fastbook import * . %config Completer.use_jedi = False . Collaborative filtering modules: . Exploring the data . from fastai.collab import * from fastai.tabular.all import* . Downloading and extracting data from the URL list . path = untar_data(URLs.ML_100k) . Giving columns names and readind first five rows. . ratings = pd.read_csv(path/&#39;u.data&#39;,delimiter = &#39; t&#39;, header= None, engine=&#39;python&#39;,names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . How to recommend movies. Assume the movie has three properties, scince fiction(ness), action, old(ness). . Last skywalker is a sci-fi, and action and not old. . How model learn about our preferences . last_skywalker = np.array([0.98,0.9,-0.9]) . And a user who likes sci-fi and action movies and not so old movies would like this. . user1= np.array([.9,.8,-.6]) . If we multiply these two vectors and sum it. We get: . (user1*last_skywalker).sum() . 2.1420000000000003 . this our matching score, it is a positive value that shows there is a match between the movie and the user1 . casablanka= np.array([-.99,-.33,.8]) . (user1*casablanka).sum() . -1.635 . this is low at this time. There is no match. . Latent Factors . We can pick arbitrary number of parameters for the array. Above, we use three. That could be much more of them. We call them Latent Factors. We start training with random parameters and learn from the ratings given by users. . How to create Dataloaders . . Note: It is not easy to use data as it was. For this dataset, movie id and movie title are not on the same table. . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, engine= &#39;python&#39;,header=None,encoding=&#39;latin1&#39;, usecols=(0,1),names=(&#39;movie&#39;,&#39;title&#39;)) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . Let&#39;s bring ratings and movies together. (movie id will be the key parameter) . ratings=ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . For Dataloaders, we use CollabDataLoaders this Dataloader use first column for the user and second one for the item, in our situation we should change the default one because our item will be title. . dls=CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;,bs=64) dls.show_batch() . user title rating . 0 581 | Brassed Off (1996) | 3 | . 1 864 | Jaws (1975) | 4 | . 2 873 | Contact (1997) | 3 | . 3 58 | Wings of Desire (1987) | 3 | . 4 497 | Hard Target (1993) | 2 | . 5 892 | Jungle Book, The (1994) | 4 | . 6 43 | Santa Clause, The (1994) | 3 | . 7 751 | Strictly Ballroom (1992) | 4 | . 8 894 | Mighty Aphrodite (1995) | 4 | . 9 390 | Spitfire Grill, The (1996) | 5 | . dls.classes[&#39;user&#39;][:15] . (#15) [&#39;#na#&#39;,1,2,3,4,5,6,7,8,9...] . dls.classes[&#39;title&#39;][:15] . (#15) [&#39;#na#&#39;,&#34;&#39;Til There Was You (1997)&#34;,&#39;1-900 (1994)&#39;,&#39;101 Dalmatians (1996)&#39;,&#39;12 Angry Men (1957)&#39;,&#39;187 (1997)&#39;,&#39;2 Days in the Valley (1996)&#39;,&#39;20,000 Leagues Under the Sea (1954)&#39;,&#39;2001: A Space Odyssey (1968)&#39;,&#39;3 Ninjas: High Noon At Mega Mountain (1998)&#39;...] . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors = 5 user_factors = torch.randn(n_users,n_factors) movie_factors = torch.randn(n_movies, n_factors) . More PyTorch Less Python . . Tip: This is how one_hot works. . one_hot(0,5) . tensor([1, 0, 0, 0, 0], dtype=torch.uint8) . Let&#39;s create one_hot such that third index equal to one and the rest of them 0(lenght of number of users). one_hot_3 = one_hot(3,n_users).float() one_hot_3[:10] . tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]) . and multiply by users_factors(matrix multiplication) . user_factors.t() @ one_hot_3 . tensor([ 0.4286, 0.8374, -0.5413, -1.6935, 0.1618]) . This might look a bit daunting but it is not. Basically we want utilize pytorch more and python less. PyTorch very good at matrix multiplication, python is not. With this matrix multiplication we can access every index of the latent factor tensor in one move. Otherwise we would have use regular python loop and index which is very very slow. . This is Python version: . user_factors[3] . tensor([ 0.4286, 0.8374, -0.5413, -1.6935, 0.1618]) . This is same. Great. . Collaborative Filtering from Scratch . At this point there is a section regarding OOP if you want to learn OOP the check the original book page 260 (3rd release) or the course notebook . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) . . Important: This forward method is a bit confusing but I guess what happens there is, x is merged df (it became part of the dls) from above so first column is user id and the second is movie id. check this part: python ratings=ratings.merge(movies) ratings.head() . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . x[0] . tensor([804, 763]) . first one is user id and the second is movie. . y[0] . tensor([5], dtype=torch.int8) . must be the rating. . Let&#39;s train . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.382019 | 1.291539 | 00:06 | . 1 | 1.064109 | 1.072716 | 00:06 | . 2 | 0.977546 | 0.980324 | 00:06 | . 3 | 0.869058 | 0.885319 | 00:06 | . 4 | 0.803102 | 0.871484 | 00:07 | . not bad but we can force our model to make predictions into range 0-5 . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . The dls has values in this range as dependent variables (ratings) and there is a special method in the fastai(I assume) for that. . doc(sigmoid_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.991997 | 0.972494 | 00:08 | . 1 | 0.856079 | 0.889023 | 00:08 | . 2 | 0.677160 | 0.858434 | 00:07 | . 3 | 0.455940 | 0.864097 | 00:07 | . 4 | 0.371842 | 0.868755 | 00:07 | . A little bit better results. . Bias . Sometimes a user could give low (or high) ratings based on his/her subjective preference even the others thinks that is a very good movie. Let&#39;s add a net parameter for that is bias. Bias effects all other parameters in negative or positive way. . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.950526 | 0.923924 | 00:08 | . 1 | 0.811043 | 0.851933 | 00:08 | . 2 | 0.609098 | 0.852216 | 00:08 | . 3 | 0.400987 | 0.877794 | 00:08 | . 4 | 0.289632 | 0.884916 | 00:08 | . And the training loss goes down faster and faster, but valid loss not so. . Weight Decay (or L2 regularization) . from the book: . Weight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.** . Why would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is (&lt;&gt;).&lt;/p&gt; So, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting. . Limiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters): . loss_with_wd = loss + wd * (parameters**2).sum() . In practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing: . parameters.grad += wd * 2 * parameters . In practice, since wd is a parameter that we choose, we can just make it twice as big, so we don&#39;t even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; x = np.linspace(-2,2,100) a_s = [1,2,5,10,50] ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(8,6)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f&#39;a={a}&#39;) ax.set_ylim([0,5]) ax.legend(); . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.977155 | 0.931246 | 00:08 | . 1 | 0.870503 | 0.858914 | 00:07 | . 2 | 0.746876 | 0.823343 | 00:07 | . 3 | 0.573917 | 0.810015 | 00:07 | . 4 | 0.480767 | 0.810702 | 00:07 | . Not so good traing loss but at this time validation loss is far better. . Creating Our Own Embedding Module . class T(Module): def __init__(self): self.a = torch.ones(3) L(T().parameters()) . (#0) [] . There is no pararameters, by its definition parameters must be trainable. . type(torch.ones(3)[0]) . torch.Tensor . . Note: this is tensor not parameter.So it is not trainable. No gradiend tracking. . from the book: . To tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesn&#39;t actually add any functionality (other than automatically calling requires_grad_ for us). It&#39;s only used as a &quot;marker&quot; to show what to include in parameters: . class T(Module): def __init__(self): self.a = nn.Parameter(torch.ones(3)) L(T().parameters()) . (#1) [Parameter containing: tensor([1., 1., 1.], requires_grad=True)] . and . class T(Module): def __init__(self): self.a = nn.Linear(1, 3, bias=False) t = T() L(t.parameters()) . (#1) [Parameter containing: tensor([[-0.0643], [-0.8105], [ 0.1346]], requires_grad=True)] . type(t.a.weight) . torch.nn.parameter.Parameter . . Note: This is a parameter . type(t.a.weight.data) . torch.Tensor . . Note: This is not. . Custom Embedding without using Pytorch Embedding . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) . doc(create_params) . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.925678 | 0.936475 | 00:08 | . 1 | 0.820004 | 0.864908 | 00:08 | . 2 | 0.718156 | 0.818959 | 00:08 | . 3 | 0.589835 | 0.812693 | 00:08 | . 4 | 0.462965 | 0.813873 | 00:08 | . Very similiar results. . Interpreting Embeddings and Biases . Lowest biases in the model. . movie_bias=learn.model.movie_bias.squeeze() idxs=movie_bias.argsort()[0:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Crow: City of Angels, The (1996)&#39;, &#39;Jury Duty (1995)&#39;, &#39;Mortal Kombat: Annihilation (1997)&#39;, &#39;Cable Guy, The (1996)&#39;] . from the book: . Think about what this means. What it&#39;s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don&#39;t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias: . idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Titanic (1997)&#39;, &#34;Schindler&#39;s List (1993)&#34;, &#39;Star Wars (1977)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Rear Window (1954)&#39;] . from the book: . So, for instance, even if you don&#39;t normally enjoy detective movies, you might enjoy LA Confidential! . It is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course Computational Linear Algebra for Coders. &lt;&gt; shows what our movies look like based on two of the strongest PCA components.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . Lets try changing X axis. . g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = list(range(50)) X = fac1[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . Very interesting to study changes. . Using fastai.collab&#182; . Same thing with fastai collab_learner . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.940206 | 0.939116 | 00:07 | . 1 | 0.886674 | 0.867349 | 00:07 | . 2 | 0.750853 | 0.824058 | 00:07 | . 3 | 0.610644 | 0.811104 | 00:07 | . 4 | 0.497868 | 0.812237 | 00:07 | . learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1665, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1665, 1) ) . movie_bias = learn.model.i_bias.weight.squeeze() idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#34;Schindler&#39;s List (1993)&#34;, &#39;Titanic (1997)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Rear Window (1954)&#39;, &#39;Star Wars (1977)&#39;] . similar results. . Embedding Distance . Basically it means if two movies has similar latent factors.(embedding vector) This is the movie very similar latent factors with Silence of the lambs. . movie_factors = learn.model.i_weight.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Silence of the Lambs, The (1991)&#39;] distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) idx = distances.argsort(descending=True)[1] dls.classes[&#39;title&#39;][idx] . &#39;Body Snatcher, The (1945)&#39; . Bootstrapping a Collaborative Filtering Model . read the all section from the original book at page 270 (3rd release) or the course notebook . Deep Learning for Collaborative Filtering . First fastai could make a recommendation for right embedding sizes(latent factors). . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) . learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.959750 | 0.937144 | 00:09 | . 1 | 0.919930 | 0.894244 | 00:08 | . 2 | 0.857974 | 0.870025 | 00:08 | . 3 | 0.814399 | 0.854047 | 00:08 | . 4 | 0.763636 | 0.860031 | 00:08 | . with one step. . above is possibble(again) with collab_learner with one step. just use use_nn=True. . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.989683 | 0.959795 | 00:10 | . 1 | 0.902582 | 0.904747 | 00:10 | . 2 | 0.864139 | 0.879289 | 00:10 | . 3 | 0.824376 | 0.847727 | 00:10 | . 4 | 0.790679 | 0.850178 | 00:10 | . from the book: . Although the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That&#39;s exactly what TabularModel does. In fact, we&#39;ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, we&#39;d better spend some time learning about TabularModel, and how to use it to get great results! We&#39;ll do that in the next chapter. . &lt;/div&gt; .",
            "url": "https://niyazikemer.com/fastbook/2021/09/01/chapter-8.html",
            "relUrl": "/fastbook/2021/09/01/chapter-8.html",
            "date": " • Sep 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Chapter 7 - Training a State-of-the-Art Model",
            "content": "This my favorite Turkish coffe cup, designed by German-Turkish artist Taner Ceylan, check his works at here . import fastbook fastbook.setup_book() %config Completer.use_jedi = False . Imagenette . Imagenette is a subset of ImageNet that contains 10 classes from the full ImageNet that looked very different from one another. Considering the size of ImageNet, it is very costly and time consuming to create a prototype for your project. Smaller datasets lets you make much more experiments, and could provide insight for your projects direction. . from fastai.vision.all import * path = untar_data(URLs.IMAGENETTE) . dblock = DataBlock(blocks = (ImageBlock(),CategoryBlock()), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = dblock.dataloaders(path,bs=64) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . model=xresnet50(n_out=dls.c) learn=Learner(dls,model,loss_func=CrossEntropyLossFlat(),metrics=accuracy) learn.fit_one_cycle(5,3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.581075 | 3.990604 | 0.335325 | 01:11 | . 1 | 1.188324 | 2.071529 | 0.488798 | 01:12 | . 2 | 0.967764 | 1.166690 | 0.639656 | 01:12 | . 3 | 0.723403 | 0.728145 | 0.770724 | 01:12 | . 4 | 0.571699 | 0.579214 | 0.828603 | 01:12 | . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . Normalization . Normalized data helps better results. Normalization is your data has a mean of 0 and standart deviation of 1. But our data encoded with numbers between 0 and 255 or sometimes 0-1. Lets check the data in the Imaginette: . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.4661, 0.4575, 0.4309], device=&#39;cuda:0&#39;), TensorImage([0.2791, 0.2752, 0.2898], device=&#39;cuda:0&#39;)) . Our data is around 0.5 mean and 0.3 deviation. So it is not in desirable range.With fastai it is possible to normalize our data by adding Normalize transform. . def get_dls(bs, size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . dls = get_dls(64, 224) . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([-0.2460, -0.1802, -0.0632], device=&#39;cuda:0&#39;), TensorImage([1.2249, 1.1904, 1.2784], device=&#39;cuda:0&#39;)) . Now it is better. Let&#39;s check it if it helped the training process. Same code again for the training. . model = xresnet50(n_out=dls.c) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.612493 | 2.099523 | 0.436146 | 01:12 | . 1 | 1.253520 | 1.564609 | 0.538462 | 01:12 | . 2 | 0.957898 | 1.758915 | 0.567961 | 01:14 | . 3 | 0.760550 | 0.672671 | 0.788648 | 01:14 | . 4 | 0.613525 | 0.580995 | 0.819268 | 01:13 | . a little bit better but Normalization is much more important when we use pretrained model. Normalizing our data with the original data statistic helps better transfer learning results. . Progressive Resizing . from the book: . Spending most of the epochs training with small images, helps training complete much faster. Completing training using large images makes the final accuracy much higher. We call this approach progressive resizing. . This my check on using progressive resizing . import time start_time = time.time() dls = get_dls(128, 128) learn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(4, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.628276 | 3.793727 | 0.295370 | 00:35 | . 1 | 1.250497 | 1.006853 | 0.675878 | 00:36 | . 2 | 0.945165 | 0.896517 | 0.711352 | 00:35 | . 3 | 0.750154 | 0.655099 | 0.798730 | 00:35 | . learn.dls = get_dls(64, 224) learn.fine_tune(6, 3e-3) print(&quot; %s seconds &quot; % (time.time() - start_time)) . epoch train_loss valid_loss accuracy time . 0 | 1.072026 | 1.799888 | 0.481703 | 01:11 | . epoch train_loss valid_loss accuracy time . 0 | 0.740281 | 0.882515 | 0.753174 | 01:11 | . 1 | 0.772105 | 0.909184 | 0.714339 | 01:11 | . 2 | 0.671060 | 0.985478 | 0.727035 | 01:11 | . 3 | 0.588883 | 0.552914 | 0.830471 | 01:11 | . 4 | 0.464459 | 0.420264 | 0.870052 | 01:13 | . 5 | 0.404156 | 0.390893 | 0.877147 | 01:12 | . 649.6742904186249 seconds . import time start_time = time.time() dls = get_dls(32, 224) learn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(8, 3e-3) print(&quot; %s seconds &quot; % (time.time() - start_time)) . epoch train_loss valid_loss accuracy time . 0 | 1.662178 | 1.874999 | 0.467886 | 01:22 | . 1 | 1.307072 | 1.275218 | 0.576176 | 01:23 | . 2 | 1.070854 | 1.173411 | 0.638536 | 01:23 | . 3 | 0.842672 | 0.831104 | 0.728902 | 01:23 | . 4 | 0.699521 | 0.746880 | 0.774832 | 01:24 | . 5 | 0.579603 | 0.524914 | 0.828603 | 01:23 | . 6 | 0.457707 | 0.423468 | 0.868559 | 01:24 | . 7 | 0.401849 | 0.415911 | 0.872293 | 01:23 | . 670.1757352352142 seconds . I&#39;ve changed some hyperparameters like number of epochs and learning rate. It is faster and better result most of the time(not in every situation), nice. . Test Time Augmentation . Random cropping sometimes leads suprising problems.Especially if it used with multicategory images, for example the objects in the image that close to edges could be ignored totaly. There are some workarounds to solve this problem (squish or stretch them)but most of them couse other kind of problems that could hurt the results. Only downside is validation time would be slower. . . Warning: How is it possible? Since we do not use validation loss for backpropagation how come it improves our results. . preds,targs = learn.tta() accuracy(preds, targs).item() . . 0.8760268688201904 . from the book: . jargon: test time augmentation (TTA): During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image. . Mixup . Especially used when we don&#39;t have enough data and do not have pretrained model that was trained on similar to our dataset. . from the book: Mixup works as follows, for each image: . Select another image from your dataset at random. | Pick a weight at random. | Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable. | Take a weighted average (with the same weight) of this image&#39;s labels with your image&#39;s labels; this will be your dependent variable. | The paper explains: &quot;While data augmentation consistently leads to improved generalization, the procedure is dataset-dependent, and thus requires the use of expert knowledge.&quot; For instance, it&#39;s common to flip images as part of data augmentation, but should you flip only horizontally, or also vertically? The answer is that it depends on your dataset. In addition, if flipping (for instance) doesn&#39;t provide enough data augmentation for you, you can&#39;t &quot;flip more.&quot; It&#39;s helpful to have data augmentation techniques where you can &quot;dial up&quot; or &quot;dial down&quot; the amount of change, to see what works best for you. . shows what it looks like when we take a linear combination of images, as done in Mixup. . I&#39;ve replaced these rows like above. It seems there is no get_image_files_sorted method in the fastai. . church = PILImage.create(get_image_files_sorted(path/&#39;train&#39;/&#39;n03028079&#39;)[0]) gas = PILImage.create(get_image_files_sorted(path/&#39;train&#39;/&#39;n03425413&#39;)[0]) . Label Smoothing . . Warning: check the original notebook for this part. Only thing I can say is, it used for making the model less confident for the classification to overcome overfitting. .",
            "url": "https://niyazikemer.com/fastbook/2021/08/15/chapter-7.html",
            "relUrl": "/fastbook/2021/08/15/chapter-7.html",
            "date": " • Aug 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Chapter 6 - Other Computer Vision Problems",
            "content": "My Deep Learning for Coders with fastai &amp; PyTorch has arrived. It is very good for taking notes on it directly. (Some chapters are slightly different from the notebook version) . #!pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastbook import * . Multi-Label Classification . I think main theme of this lesson is &quot;Binary Cross Entropy&quot;. It is important when a photo have more than one category or when there is no category to find. Think about the Bear classifier. It classifies between two. But if there is an another bear breed in the picture it just tries to pick either black or grizzlie anyway which is not good, or if there is one grizzlie and one rabbit, probably it would be confused between these labels, at least its confidence level would be lower. . The Dataset . from fastai.vision.all import * path = untar_data(URLs.PASCAL_2007) . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . . Note: pd at the beginning is pandas it is library for creating data frames df from csv files. Data frame is a table contains columns and rows. there are some rows that contains more than one labels. Check row no 2. . Pandas and DataFrames . df.iloc[:,0] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . . Note: Very easy to navigate in a dataframe. . df.iloc[0,:] # Trailing :s are always optional (in numpy, pytorch, pandas, etc.), # so this is equivalent: df.iloc[0] . fname 000005.jpg labels chair is_valid True Name: 0, dtype: object . df[&#39;fname&#39;] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . . Note: it is possible to use column names to select a column in the dataframe. . tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) tmp_df . a b . 0 1 | 3 | . 1 2 | 4 | . tmp_df[&#39;c&#39;] = tmp_df[&#39;a&#39;]+tmp_df[&#39;b&#39;] tmp_df . a b c . 0 1 | 3 | 4 | . 1 2 | 4 | 6 | . . Note: It is also possible to create new column. . . Note: From the book: . Pandas is a fast and flexible library, and an important part of every data scientist’s Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it takes a while to get familiar with it. If you haven’t used Pandas before, we’d suggest going through a tutorial; we are particularly fond of the book Python for Data Analysis by Wes McKinney, the creator of Pandas (O&#39;Reilly). It also covers other important libraries like matplotlib and numpy. We will try to briefly describe Pandas functionality we use as we come across it, but will not go into the level of detail of McKinney’s book. . Constructing a DataBlock . Dataset and Dataloader . Dataset: Anything in which we can index to it and you can take the length to it. | DataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables | . a = list (enumerate(string.ascii_lowercase)) a[0],len(a) . ((0, &#39;a&#39;), 26) . . Note: above index and length. . dl_a = DataLoader(a, batch_size=8, shuffle=True) b = first(dl_a) b . (tensor([ 6, 11, 13, 0, 8, 22, 10, 3]), (&#39;g&#39;, &#39;l&#39;, &#39;n&#39;, &#39;a&#39;, &#39;i&#39;, &#39;w&#39;, &#39;k&#39;, &#39;d&#39;)) . . Note: batch_size=8 for mini-batch size. first just takes the first batch. . list(zip(b[0],b[1])) . [(tensor(6), &#39;g&#39;), (tensor(11), &#39;l&#39;), (tensor(13), &#39;n&#39;), (tensor(0), &#39;a&#39;), (tensor(8), &#39;i&#39;), (tensor(22), &#39;w&#39;), (tensor(10), &#39;k&#39;), (tensor(3), &#39;d&#39;)] . . Note: This is how you can see which independent and dependent variables are correspond each other. . list(zip(*b)) . [(tensor(6), &#39;g&#39;), (tensor(11), &#39;l&#39;), (tensor(13), &#39;n&#39;), (tensor(0), &#39;a&#39;), (tensor(8), &#39;i&#39;), (tensor(22), &#39;w&#39;), (tensor(10), &#39;k&#39;), (tensor(3), &#39;d&#39;)] . . Note: Short cut for zipping.*try to understand) It used for transposing (JH said in the lesson video, check it how) . Dataset(s) and Dataloader(s) . Datasets: An object that contains a training Dataset and a validation Dataset | DataLoaders: An object that contains a training DataLoader and a validation DataLoader | . a = list (string.ascii_lowercase) a[0],len(a) . (&#39;a&#39;, 26) . similar dataset as previous one. but there is no enumeration. . dss = Datasets(a) dss[0] . (&#39;a&#39;,) . For creating our dependent and independent variable we can use functions. e.g.: . def f1 (o): return o+&#39;a&#39; def f2 (o): return o+&#39;b&#39; . dss = Datasets(a,[[f1]]) dss[0] . (&#39;aa&#39;,) . dss = Datasets(a,[[f1,f2]]) dss[0] . (&#39;aab&#39;,) . . Note: that means if we have &#8217;a&#8217; in the inital dataset, our independent value should be &#8217;aa&#8217; and our dependent should be &#8217;ab&#8217;. But it is not at the moment. [[f1,f2]]) is a list of lists and if we change the shape of the input arguments a bit: . dss = Datasets(a,[[f1],[f2]]) dss[0] . (&#39;aa&#39;, &#39;ab&#39;) . . Note: Now we are good to go. Now we can create our Dataloaders from our Datasets. . dls = DataLoaders.from_dsets(dss, batch_size=4) . first(dls.train) . ((&#39;va&#39;, &#39;ra&#39;, &#39;ea&#39;, &#39;ua&#39;), (&#39;vb&#39;, &#39;rb&#39;, &#39;eb&#39;, &#39;ub&#39;)) . . Note: Our dataloaders is ready. This is how we create dataloaders from scratch. . What is DataBlock ? . . Note: There is much more easier way to create our datasets. . dblock = DataBlock() . . Note: An empty DataBlock. . dsets = dblock.datasets(df) . . Note: From the book: . We can create a Datasets object from this. The only thing needed is a source—in this case, our DataFrame df . len(dsets.train),len(dsets.valid) . (4009, 1002) . Our training and validation sets are ready. How? First: if we didn&#39;t give any argument for splitting then the split is random and the split ratio is %20. . x,y = dsets.train[0] x,y . (fname 001293.jpg labels dog is_valid True Name: 646, dtype: object, fname 001293.jpg labels dog is_valid True Name: 646, dtype: object) . This is first row of the batch repeated twice. (this is how default value works) . x[&#39;fname&#39;] . &#39;001293.jpg&#39; . . Note: However we need file name (fname) as a independent and labels as dependent variables. . dblock = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[0] . (&#39;006610.jpg&#39;, &#39;diningtable bottle person&#39;) . . Note: like this. . Same thing with functions without lambda functions. Most of the time it is much more relevant because Lambda causes problems if you try to serialize the model. . def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;] dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (&#39;006828.jpg&#39;, &#39;bottle&#39;) . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (Path(&#39;/home/niyazi/.fastai/data/pascal_2007/train/005042.jpg&#39;), [&#39;bicycle&#39;, &#39;person&#39;]) . . Note: all are ok again. Datasets object is ready, the shape is right. . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=375x500, TensorMultiCategory([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . . Note: blocks types are important, in previous lessons we used different type of blocks. Based on selected type Datablock gains additional capabilities. In the current one ImageBlock help us to see the information as image. MultiCategoryBlock encodes labels as a tensor that every index correspond to a object label. (onehot encoding). Only one thing I do not understand about it that how fastai understands number of categories.(total 20 now) . idxs = torch.where(dsets.train[0][1]==1.)[0] print(idxs) dsets.train.vocab[idxs] . TensorMultiCategory([4]) . (#1) [&#39;bottle&#39;] . . Note: Example above there are two categories. (it changes every run) . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . . Note: ~ is a bitwise operation that reverses bits. . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(df) . dls.show_batch(nrows=1, ncols=3) . What is Binary Cross-Entropy ? . learn = cnn_learner(dls, resnet18) . x,y = to_cpu(dls.train.one_batch()) activs = learn.model(x) activs.shape . torch.Size([64, 20]) . . Note: model refers to resnet at this statement and activs are activation from the last layer of the resnet18 for one batch. Jeremy says &#8217;learn.model(x)&#8217; is plain pytorch. (didn&#8217;t know it) . activs[0] . tensor([ 1.1090, -1.4315, 2.8930, 0.5827, 3.0797, 2.5147, -1.3310, 1.7237, -0.2547, 0.3985, -0.2740, -0.1811, -1.5258, -1.0918, -1.7862, 0.3597, -0.4354, -0.1203, 2.2807, -0.3097], grad_fn=&lt;SelectBackward&gt;) . . Note: Values are not between 0 and 1. . def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() . . Note: a couple of things going on here: - Sigmoid brings everything between zero and one. . Log just adjust results such a way based on their relative confidence level. (Check the section on Chapter -5) | Broadcasting. We&#39;ll get the results for every item. | . loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . TensorMultiCategory(1.0493, grad_fn=&lt;AliasBackward&gt;) . . Important: Although there is a pytorch equivalent for our binary_cross_entropy ( F.binary_cross_entropy and nn.BCELoss) they don&#8217;t include sigmoid. So instead we use F.binary_cross_entropy_with_logits or nn.BCEWithLogitsLoss . . . Note: Direct from the book: . We don&#39;t actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default. . One change compared to the last chapter is the metric we use: because this is a multilabel problem, we can&#39;t use the accuracy function. Why is that? Well, accuracy was comparing our outputs to our targets like so: . def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred = inp.argmax(dim=axis) return (pred == targ).float().mean() . The class predicted was the one with the highest activation (this is what argmax does). Here it doesn&#39;t work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a threshold. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0: . def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . . Note: We need to pass our accuracy function to the learner for getting accuracy. See learn statement below. The only problem is our default value is 0.5. When we need an another value, we need to use Python partial functionality. See usage below or check original file check original file and video. . What is partial function ? . it is used when there is a need to change default values. . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) . (&#39;Hello Jeremy.&#39;, &#39;Ahoy! Jeremy.&#39;) . f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) . (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.936221 | 0.694192 | 0.238267 | 00:11 | . 1 | 0.821117 | 0.555462 | 0.291215 | 00:11 | . 2 | 0.600436 | 0.203543 | 0.820837 | 00:10 | . 3 | 0.357310 | 0.127290 | 0.935956 | 00:10 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.133780 | 0.117657 | 0.943227 | 00:12 | . 1 | 0.117373 | 0.106338 | 0.949582 | 00:12 | . 2 | 0.097523 | 0.102176 | 0.954183 | 00:12 | . How to find right threshold ? . learn.metrics = partial(accuracy_multi, thresh=0.1) learn.validate() . (#2) [0.10217633843421936,0.9333268404006958] . . Note: Low threshold (selects even on low confidence) . learn.metrics = partial(accuracy_multi, thresh=0.99) learn.validate() . (#2) [0.10217633843421936,0.9434263110160828] . . Note: High threshold (selects only on high confidence) . preds,targs = learn.get_preds() . accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) . TensorBase(0.9558) . . Note: This a better way to pick the right threshold value.Testing for a range of values. . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . 0.5 looks best. . Image Regression . . Note: Classification is used for finding right classes and Regression for continuous values, e.g. house prices or a coordinates of something or length etc... . Assemble the Data . path = untar_data(URLs.BIWI_HEAD_POSE) . . Note: In this exaple we will find center point of a heads. . Path.BASE_PATH = path . path.ls().sorted() . (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...] . (path/&#39;01&#39;).ls().sorted() . (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...] . . Note: From the book: . Inside the subdirectories, we have different frames, each of them come with an image (_rgb.jpg) and a pose file (_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file: | . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) . Path(&#39;03/frame_00668_pose.txt&#39;) . img2pose creates a path based for coordinate file based on image name. . im = PILImage.create(img_files[0]) im.shape . (480, 640) . im.to_thumb(160) . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . . Note: From the book: - The Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren&#39;t important for our purposes, so we&#39;ll just show the function we use to extract the head center point: . get_ctr(img_files[0]) . tensor([447.7369, 283.9802]) . biwi = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) . . Note: Most important thing about this DataBlock is splitter, basically we only use person no 13 see explanation(of course lots of pics of the person no:13), if we&#8217;d split randomly then there would be a very high chance for same person to be in the both training and validations sets. (there are lots of pictures of same person in this dataset) Also see dependent variable is continious value which is PointBlock as coordinates. There is also a normalization process as batch_tmfs. . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . xb,yb = dls.one_batch() xb.shape,yb.shape . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) is important. . 64 : batch size | 3 : RGB | 240,320: image size | 1,2 : one row with two values (one point with two coordinates) | . yb[0] . TensorPoint([[-0.1250, 0.0771]], device=&#39;cuda:0&#39;) . Dependent variable. . Training the Model . learn = cnn_learner(dls, resnet18, y_range=(-1,1)) . y_range=(-1,1) is important we tell fast ai that we need results in this range.(for coordinates) y_range is implemented in fastai using sigmoid_range, which is defined as: . def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo . plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) . dls.loss_func . FlattenedLoss of MSELoss() . Default value . learn.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/callback/schedule.py:270: UserWarning: color is redundantly defined by the &#39;color&#39; keyword argument and the fmt string &#34;ro&#34; (-&gt; color=&#39;r&#39;). The keyword argument will take precedence. ax.plot(val, idx, &#39;ro&#39;, label=nm, c=color) . SuggestedLRs(valley=tensor(0.0012)) . lr = 1e-2 learn.fine_tune(3, lr) . epoch train_loss valid_loss time . 0 | 0.049803 | 0.002713 | 00:46 | . epoch train_loss valid_loss time . 0 | 0.008684 | 0.002087 | 00:56 | . 1 | 0.003187 | 0.000621 | 00:56 | . 2 | 0.001467 | 0.000064 | 00:57 | . math.sqrt(0.0001) . 0.01 . learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . Questionnaire . How could multi-label classification improve the usability of the bear classifier? | How do we encode the dependent variable in a multi-label classification problem? | How do you access the rows and columns of a DataFrame as if it was a matrix? | How do you get a column by name from a DataFrame? | What is the difference between a Dataset and DataLoader? | What does a Datasets object normally contain? | What does a DataLoaders object normally contain? | What does lambda do in Python? | What are the methods to customize how the independent and dependent variables are created with the data block API? | Why is softmax not an appropriate output activation function when using a one hot encoded target? | Why is nll_loss not an appropriate loss function when using a one-hot-encoded target? | What is the difference between nn.BCELoss and nn.BCEWithLogitsLoss? | Why can&#39;t we use regular accuracy in a multi-label problem? | When is it okay to tune a hyperparameter on the validation set? | How is y_range implemented in fastai? (See if you can implement it yourself and test it without peeking!) | What is a regression problem? What loss function should you use for such a problem? | What do you need to do to make sure the fastai library applies the same data augmentation to your inputs images and your target point coordinates? | Further Research . Read a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book&#39;s website for recommended tutorials. | Retrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don&#39;t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification. |",
            "url": "https://niyazikemer.com/fastbook/2021/08/02/chapter-6.html",
            "relUrl": "/fastbook/2021/08/02/chapter-6.html",
            "date": " • Aug 2, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Chapter 5 - Image Classification",
            "content": ". . I&#39;m a Doctor Who fan and this is my cyberman coffee cup, as I remember got it from Manchester Science Museum. . . import fastbook fastbook.setup_book() %config Completer.use_jedi = False . from fastbook import * . [[chapter_pet_breeds]] . PLAYING WITH THE DATASET . from fastai.vision.all import * path = untar_data(URLs.PETS) . . Note: With untar we download the data. This data originally come from Oxford University Visual Geomety Group and our dataset is here: . path . Path(&#39;/home/niyazi/.fastai/data/oxford-iiit-pet&#39;) . . Note: This is the local download path for my computer. . Path.BASE_PATH = path . . Tip: This is a trick to get the relative path, check above and below . path . Path(&#39;.&#39;) . Now the path is looks different. . path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . . Note: #2 is number of item in the list. annotations represents target variables of this datasets but we do not use them at this time instead we create our own labels. . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/staffordshire_bull_terrier_90.jpg&#39;),Path(&#39;images/Russian_Blue_70.jpg&#39;),Path(&#39;images/japanese_chin_69.jpg&#39;),Path(&#39;images/Maine_Coon_266.jpg&#39;),Path(&#39;images/japanese_chin_200.jpg&#39;),Path(&#39;images/Siamese_57.jpg&#39;),Path(&#39;images/Persian_175.jpg&#39;),Path(&#39;images/havanese_81.jpg&#39;),Path(&#39;images/Birman_72.jpg&#39;),Path(&#39;images/leonberger_55.jpg&#39;)...] . fname = (path/&quot;images&quot;).ls()[0] . fname . Path(&#39;images/staffordshire_bull_terrier_90.jpg&#39;) . . Note: The first image in the path list. . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;staffordshire_bull_terrier&#39;] . . Note: Since we don&#8217;t use the annonations in the Dataset we need to find a way to get breeds form the filename. This is regex findall method, Check geeksforgeeks.org tutorial here . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . . Note: now find all names with RegexLabeller. The item_tmsf and batch_transfdrms may look a bit meaningless. Check below to find out why. . . PRESIZING . As a summary FastAi gives a chance to augment our images in a smarter way (presizing) such that provide much more detail and information for the training. First, we presize images with item_tfms then push them to GPU and use augmentation. . check the original document for the whole idea . #caption A comparison of fastai&#39;s data augmentation strategy (left) and the traditional approach (right). dblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_y=parent_label, item_tfms=Resize(460)) # Place an image in the &#39;images/grizzly.jpg&#39; subfolder where this notebook is located before running this dls1 = dblock1.dataloaders([(Path.cwd()/&#39;images&#39;/&#39;chapter-05&#39;/&#39;grizzly.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . dls.show_batch(nrows=3, ncols=3) . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /home/niyazi/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /home/niyazi/.fastai/data/oxford-iiit-pet/images/British_Shorthair_110.jpg applying PILBase.create gives PILImage mode=RGB size=500x333 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /home/niyazi/.fastai/data/oxford-iiit-pet/images/British_Shorthair_110.jpg applying partial gives British_Shorthair applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(4) Final sample: (PILImage mode=RGB size=500x333, TensorCategory(4)) Collecting items from /home/niyazi/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} Building one batch Applying item_tfms to the first sample: Pipeline: ToTensor starting from (PILImage mode=RGB size=500x333, TensorCategory(4)) applying ToTensor gives (TensorImage of size 3x333x500, TensorCategory(4)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Error! It&#39;s not possible to collate your items in a batch Could not collate the 0-th members of your tuples because got the following shapes torch.Size([3, 333, 500]),torch.Size([3, 500, 396]),torch.Size([3, 375, 500]),torch.Size([3, 500, 281]) . RuntimeError Traceback (most recent call last) &lt;ipython-input-15-ead0dd2a047d&gt; in &lt;module&gt; 3 splitter=RandomSplitter(seed=42), 4 get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) -&gt; 5 pets1.summary(path/&#34;images&#34;) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs) 188 why = _find_fail_collate(s) 189 print(&#34;Make sure all parts of your samples are tensors of the same size&#34; if why is None else why) --&gt; 190 raise e 191 192 if len([f for f in dls.train.after_batch.fs if f.name != &#39;noop&#39;])!=0: ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs) 182 print(&#34; nCollating items in a batch&#34;) 183 try: --&gt; 184 b = dls.train.create_batch(s) 185 b = retain_types(b, s[0] if is_listy(s) else s) 186 except Exception as e: ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in create_batch(self, b) 141 elif s is None: return next(self.it) 142 else: raise IndexError(&#34;Cannot index an iterable dataset numerically - must use `None`.&#34;) --&gt; 143 def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b) 144 def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b) 145 def to(self, device): self.device = device ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in fa_collate(t) 48 b = t[0] 49 return (default_collate(t) if isinstance(b, _collate_types) &gt; 50 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 51 else default_collate(t)) 52 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in &lt;listcomp&gt;(.0) 48 b = t[0] 49 return (default_collate(t) if isinstance(b, _collate_types) &gt; 50 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 51 else default_collate(t)) 52 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in fa_collate(t) 47 &#34;A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s&#34; 48 b = t[0] &gt; 49 return (default_collate(t) if isinstance(b, _collate_types) 50 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 51 else default_collate(t)) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py in default_collate(batch) 54 storage = elem.storage()._new_shared(numel) 55 out = elem.new(storage) &gt; 56 return torch.stack(batch, 0, out=out) 57 elif elem_type.__module__ == &#39;numpy&#39; and elem_type.__name__ != &#39;str_&#39; 58 and elem_type.__name__ != &#39;string_&#39;: ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs) 338 convert=False 339 if _torch_handled(args, self._opt, func): convert,types = type(self),(torch.Tensor,) --&gt; 340 res = super().__torch_function__(func, types, args=args, kwargs=kwargs) 341 if convert: res = convert(res) 342 if isinstance(res, TensorBase): res.set_meta(self, as_copy=True) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py in __torch_function__(cls, func, types, args, kwargs) 1021 1022 with _C.DisableTorchFunction(): -&gt; 1023 ret = func(*args, **kwargs) 1024 return _convert(ret, cls) 1025 RuntimeError: stack expects each tensor to be equal size, but got [3, 333, 500] at entry 0 and [3, 500, 396] at entry 1 . . Note: It is alway good to get a quick summary. pets1.summary(path/&quot;images&quot;) Check the summary above, it has lots of details. It is natural to get an error in this example because we are trying the put diffent size images into the same DataBlock. . . BASELINE MODEL . For every project, just start with a Baseline. Baseline is a good point to think about the project/domain/problem at the same time, then start improve and make experiments about architecture, hyperparameters etc. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss error_rate time . 0 | 1.513288 | 0.355303 | 0.110284 | 00:22 | . epoch train_loss valid_loss error_rate time . 0 | 0.518711 | 0.313168 | 0.106225 | 00:27 | . 1 | 0.325613 | 0.261644 | 0.089310 | 00:27 | . . Note: A basic run is helpful as baseline for the beginning. . Defaults for the baseline . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . learn.lr . 0.001 . . Tip: Very easy to see default arguments for the learner. Above loss function loss_func and learning rate lr. . One Batch Run . first(dls.train) . (TensorImage([[[[ 7.7591e-02, -1.3409e-01, 1.4352e-01, ..., -8.8188e-01, -8.0163e-01, -1.4735e-01], [ 1.9115e-03, 4.8835e-01, 4.3845e-01, ..., -1.3028e+00, -1.4314e+00, -1.2478e+00], [-1.2349e-01, 7.3246e-02, -9.2777e-02, ..., -7.9699e-01, -1.1984e+00, -9.0709e-02], ..., [-1.4486e+00, -9.5970e-01, 8.6840e-02, ..., -1.1097e+00, -3.3829e-01, 8.2527e-02], [-1.4246e+00, -8.2784e-01, 8.7511e-02, ..., -9.5360e-01, -1.0563e-01, -5.1489e-01], [-1.3575e+00, -7.6923e-01, 1.0015e-01, ..., -1.0628e+00, 4.3092e-02, -6.2399e-01]], [[ 2.5566e-01, 7.5052e-02, 2.0962e-01, ..., -9.7342e-01, -8.9785e-01, -1.5707e-01], [ 8.3578e-02, 6.1146e-01, 5.1947e-01, ..., -1.3980e+00, -1.5514e+00, -1.3726e+00], [-1.2059e-02, 1.2505e-01, -2.9267e-03, ..., -9.0869e-01, -1.3052e+00, -2.3089e-01], ..., [-1.4979e+00, -1.1395e+00, -2.8139e-01, ..., -1.3591e+00, -4.8733e-01, -2.1415e-01], [-1.4548e+00, -9.8541e-01, -2.7210e-01, ..., -1.1278e+00, -3.0796e-01, -8.4852e-01], [-1.3689e+00, -9.2548e-01, -2.6808e-01, ..., -1.2366e+00, -6.3006e-02, -1.0183e+00]], [[-1.1168e+00, -1.2721e+00, -1.0968e+00, ..., -1.1363e+00, -9.8121e-01, -3.4084e-01], [-1.0031e+00, -6.8494e-01, -8.5066e-01, ..., -1.5088e+00, -1.6080e+00, -1.4639e+00], [-1.1476e+00, -1.0927e+00, -1.3264e+00, ..., -1.0406e+00, -1.3088e+00, -3.4494e-01], ..., [-1.4021e+00, -9.7390e-01, -4.7906e-01, ..., -1.4878e+00, -5.0896e-01, -3.1871e-01], [-1.3213e+00, -8.4023e-01, -5.3294e-01, ..., -1.3262e+00, -5.3787e-01, -1.0765e+00], [-1.1781e+00, -8.0876e-01, -5.8936e-01, ..., -1.3399e+00, -4.2362e-01, -1.1124e+00]]], [[[ 1.9623e+00, 2.0361e+00, 1.9064e+00, ..., 2.2392e+00, 2.2249e+00, 2.2211e+00], [ 2.0734e+00, 2.0294e+00, 2.1349e+00, ..., 2.2461e+00, 2.2249e+00, 2.2376e+00], [ 2.0202e+00, 1.9569e+00, 1.8405e+00, ..., 2.2373e+00, 2.2353e+00, 2.2223e+00], ..., [ 3.5436e-01, 2.5449e-01, 5.5067e-01, ..., 1.0332e+00, 1.0161e+00, 9.8812e-01], [ 3.5005e-01, 1.6332e-01, 3.8754e-01, ..., 9.7724e-01, 9.6458e-01, 1.0630e+00], [ 3.4791e-01, 8.2361e-02, 2.2118e-01, ..., 8.6120e-01, 1.0850e+00, 1.1228e+00]], [[ 2.1661e+00, 2.2408e+00, 2.0968e+00, ..., 1.6825e+00, 1.6601e+00, 1.6328e+00], [ 2.2785e+00, 2.2232e+00, 2.3262e+00, ..., 1.6905e+00, 1.6537e+00, 1.6441e+00], [ 2.2230e+00, 2.1458e+00, 2.0269e+00, ..., 1.6834e+00, 1.6742e+00, 1.6310e+00], ..., [ 8.0271e-01, 7.3445e-01, 1.0265e+00, ..., 8.7042e-01, 8.4557e-01, 8.8788e-01], [ 8.1180e-01, 6.5414e-01, 8.8815e-01, ..., 8.1193e-01, 7.7473e-01, 9.1055e-01], [ 8.1687e-01, 5.4252e-01, 6.8486e-01, ..., 6.5186e-01, 9.1647e-01, 9.5377e-01]], [[ 2.3636e+00, 2.4168e+00, 2.2347e+00, ..., 1.7282e+00, 1.6718e+00, 1.6554e+00], [ 2.4760e+00, 2.3875e+00, 2.4613e+00, ..., 1.7363e+00, 1.6699e+00, 1.6726e+00], [ 2.4182e+00, 2.2941e+00, 2.1473e+00, ..., 1.7294e+00, 1.6948e+00, 1.6592e+00], ..., [ 1.4156e+00, 1.3690e+00, 1.6562e+00, ..., 1.2044e+00, 1.2088e+00, 1.2487e+00], [ 1.4260e+00, 1.3050e+00, 1.5455e+00, ..., 1.0673e+00, 1.0365e+00, 1.1483e+00], [ 1.4369e+00, 1.2059e+00, 1.3302e+00, ..., 7.4460e-01, 9.8735e-01, 9.8728e-01]]], [[[ 7.9667e-01, 6.5725e-01, 6.7499e-01, ..., 2.2489e+00, 2.2489e+00, 2.2489e+00], [ 1.6647e+00, 1.8548e+00, 4.2411e-01, ..., 2.2489e+00, 2.2489e+00, 2.2489e+00], [ 2.0417e+00, 2.1499e+00, 1.9243e+00, ..., 2.2489e+00, 2.2489e+00, 2.2489e+00], ..., [-7.8885e-02, -8.4444e-02, -1.8854e-01, ..., -3.3191e-02, 1.6326e-01, -2.5189e-02], [-3.9591e-02, -3.7761e-02, -3.5708e-02, ..., 4.1777e-01, 3.0722e-01, -8.4517e-02], [-5.5125e-01, -3.7390e-01, -3.7190e-01, ..., 1.6706e-01, -3.8756e-02, -3.0213e-01]], [[ 3.3600e-01, 1.2690e-01, 8.4595e-02, ..., 2.4286e+00, 2.4286e+00, 2.4286e+00], [ 1.3936e+00, 1.5396e+00, -7.8121e-02, ..., 2.4286e+00, 2.4286e+00, 2.4286e+00], [ 1.7230e+00, 1.8204e+00, 1.5281e+00, ..., 2.4286e+00, 2.4286e+00, 2.4286e+00], ..., [-2.6621e-01, -3.4865e-01, -5.4389e-01, ..., 1.5566e-02, 3.6483e-01, 3.7018e-01], [-2.3416e-01, -2.9848e-01, -3.8383e-01, ..., 4.3211e-01, 5.4771e-01, 3.7147e-01], [-7.7599e-01, -6.7812e-01, -7.3404e-01, ..., 2.9308e-01, 2.0118e-01, 3.7493e-02]], [[-6.9486e-02, -3.3152e-01, -5.6258e-01, ..., 2.6400e+00, 2.6400e+00, 2.6400e+00], [ 9.0693e-01, 9.7337e-01, -5.6124e-01, ..., 2.6400e+00, 2.6400e+00, 2.6400e+00], [ 1.2463e+00, 1.1590e+00, 8.0907e-01, ..., 2.6400e+00, 2.6400e+00, 2.6400e+00], ..., [-3.1419e-01, -2.4941e-01, -4.5623e-01, ..., -6.5955e-01, -6.0038e-01, -8.8913e-01], [-2.6903e-01, -2.5050e-01, -3.9344e-01, ..., -3.7691e-01, -6.0662e-01, -9.9883e-01], [-6.3179e-01, -4.3123e-01, -5.1774e-01, ..., -7.1518e-01, -8.3215e-01, -9.5885e-01]]], ..., [[[ 2.6701e-03, 4.8764e-02, 1.3802e-01, ..., -3.5556e-01, -2.1186e-01, -6.3790e-02], [ 2.7203e-01, 2.9067e-01, 3.0956e-01, ..., -9.4003e-02, -5.8179e-02, -7.6002e-02], [ 3.5114e-01, 3.3277e-01, 3.2004e-01, ..., 2.0249e-02, -2.6842e-02, -4.4070e-02], ..., [ 1.9681e+00, 2.0169e+00, 2.0680e+00, ..., -2.0286e-01, 1.0193e-01, 3.1608e-01], [ 1.9411e+00, 2.0085e+00, 2.1026e+00, ..., -1.3970e-01, 1.2286e-01, 3.5735e-01], [ 1.8141e+00, 1.8327e+00, 1.9489e+00, ..., -1.0404e-01, 1.8111e-01, 3.2454e-01]], [[ 2.0398e-01, 2.9756e-01, 3.7903e-01, ..., -1.5909e-02, 4.7189e-02, 1.5181e-01], [ 5.4995e-01, 5.8114e-01, 6.0668e-01, ..., 2.2921e-02, 2.9592e-02, 1.2454e-01], [ 6.2037e-01, 6.1136e-01, 6.1487e-01, ..., 1.3991e-01, 7.2302e-02, 1.2691e-01], ..., [ 2.1161e+00, 2.1586e+00, 2.1592e+00, ..., 1.0077e-01, 4.3020e-01, 5.8235e-01], [ 2.0806e+00, 2.1535e+00, 2.2194e+00, ..., 1.1844e-01, 4.4620e-01, 5.9031e-01], [ 1.9537e+00, 1.9817e+00, 2.1045e+00, ..., 1.3738e-01, 4.2917e-01, 6.0165e-01]], [[-3.0177e-02, -3.0919e-02, 5.6294e-02, ..., 1.2119e-02, 2.9192e-01, 4.9523e-01], [ 1.4675e-01, 1.8120e-01, 2.0599e-01, ..., 6.5189e-02, 2.1124e-01, 4.7340e-01], [ 2.2902e-01, 2.3191e-01, 2.1012e-01, ..., 1.2057e-01, 1.4622e-01, 3.3338e-01], ..., [ 2.3455e+00, 2.3984e+00, 2.4285e+00, ..., -2.0567e-01, 8.8979e-02, 1.8777e-01], [ 2.3240e+00, 2.3670e+00, 2.4654e+00, ..., -1.8698e-01, 1.2802e-01, 2.0268e-01], [ 2.2811e+00, 2.2660e+00, 2.3926e+00, ..., -1.4246e-01, 1.2407e-01, 2.1404e-01]]], [[[ 2.1948e+00, 2.1682e+00, 2.1729e+00, ..., -8.0144e-02, -1.7157e-01, -2.2714e-01], [ 2.1642e+00, 2.1482e+00, 2.1615e+00, ..., -2.8867e-01, -3.3114e-01, -4.2198e-01], [ 2.1637e+00, 2.1500e+00, 2.1554e+00, ..., -5.3515e-01, -4.0755e-01, -3.9795e-01], ..., [ 1.0268e+00, 1.0389e+00, 1.0086e+00, ..., 2.1637e+00, 2.1637e+00, 2.1637e+00], [ 1.0284e+00, 1.0010e+00, 1.0453e+00, ..., 2.1637e+00, 2.1637e+00, 2.1637e+00], [ 1.0163e+00, 1.0296e+00, 1.0190e+00, ..., 2.1637e+00, 2.1637e+00, 2.1637e+00]], [[ 2.3155e+00, 2.2723e+00, 2.2853e+00, ..., 2.2712e-01, 2.1174e-01, 1.6661e-01], [ 2.2455e+00, 2.2432e+00, 2.2569e+00, ..., 7.0896e-02, 4.8820e-02, -4.3276e-02], [ 2.2668e+00, 2.2566e+00, 2.2656e+00, ..., -1.1109e-01, -5.4757e-02, -8.3705e-02], ..., [ 1.1080e+00, 1.0955e+00, 1.0469e+00, ..., 2.2754e+00, 2.2754e+00, 2.2754e+00], [ 1.1009e+00, 1.0698e+00, 1.0872e+00, ..., 2.2754e+00, 2.2754e+00, 2.2754e+00], [ 1.0910e+00, 1.1021e+00, 1.0618e+00, ..., 2.2754e+00, 2.2754e+00, 2.2754e+00]], [[ 2.5272e+00, 2.4830e+00, 2.4523e+00, ..., 8.5871e-01, 8.1094e-01, 7.8095e-01], [ 2.4560e+00, 2.4281e+00, 2.4305e+00, ..., 5.5803e-01, 4.9335e-01, 3.9940e-01], [ 2.4629e+00, 2.4525e+00, 2.4602e+00, ..., 1.7717e-01, 2.3084e-01, 2.2868e-01], ..., [ 1.1968e+00, 1.1985e+00, 1.1224e+00, ..., 2.4712e+00, 2.4712e+00, 2.4712e+00], [ 1.2228e+00, 1.1839e+00, 1.1597e+00, ..., 2.4712e+00, 2.4712e+00, 2.4712e+00], [ 1.2212e+00, 1.2520e+00, 1.1560e+00, ..., 2.4712e+00, 2.4712e+00, 2.4712e+00]]], [[[ 2.2489e+00, 2.2489e+00, 2.2489e+00, ..., 2.2312e+00, 2.2403e+00, 2.2489e+00], [ 2.2489e+00, 2.2489e+00, 2.2489e+00, ..., 2.2320e+00, 2.2485e+00, 2.2489e+00], [ 2.2489e+00, 2.2489e+00, 2.2489e+00, ..., 2.2291e+00, 2.2471e+00, 2.2489e+00], ..., [-1.7937e+00, -1.9148e+00, -1.9569e+00, ..., 7.9287e-01, 6.7453e-01, 7.8103e-01], [-1.6935e+00, -1.8518e+00, -1.8703e+00, ..., 4.8874e-01, 2.1611e-01, 1.1217e-01], [-1.6270e+00, -1.8958e+00, -1.8929e+00, ..., 8.0896e-01, 8.9964e-01, 1.0060e+00]], [[ 2.4286e+00, 2.4286e+00, 2.4286e+00, ..., 2.4109e+00, 2.4200e+00, 2.4286e+00], [ 2.4286e+00, 2.4286e+00, 2.4286e+00, ..., 2.4113e+00, 2.4282e+00, 2.4286e+00], [ 2.4286e+00, 2.4286e+00, 2.4286e+00, ..., 2.4083e+00, 2.4268e+00, 2.4286e+00], ..., [-1.4270e+00, -1.6502e+00, -1.6874e+00, ..., 7.1551e-01, 4.9632e-01, 6.5896e-01], [-1.2436e+00, -1.4754e+00, -1.4859e+00, ..., 2.8340e-01, -9.5874e-02, -1.2210e-01], [-1.1076e+00, -1.4404e+00, -1.4846e+00, ..., 6.4517e-01, 7.3422e-01, 8.5824e-01]], [[ 2.6400e+00, 2.6400e+00, 2.6400e+00, ..., 2.6223e+00, 2.6305e+00, 2.6147e+00], [ 2.6400e+00, 2.6400e+00, 2.6400e+00, ..., 2.6228e+00, 2.6396e+00, 2.6398e+00], [ 2.6400e+00, 2.6400e+00, 2.6400e+00, ..., 2.6198e+00, 2.6382e+00, 2.6400e+00], ..., [-1.0548e+00, -1.1392e+00, -1.2386e+00, ..., 6.2691e-01, 3.3431e-01, 4.8703e-01], [-8.5461e-01, -8.5948e-01, -9.4681e-01, ..., 2.0497e-01, -1.5078e-01, -2.6161e-01], [-9.2614e-01, -8.7474e-01, -8.2363e-01, ..., 6.1918e-01, 7.6773e-01, 8.3740e-01]]]], device=&#39;cuda:0&#39;), TensorCategory([25, 4, 27, 20, 12, 27, 31, 33, 14, 35, 16, 5, 22, 33, 3, 35, 3, 0, 32, 12, 1, 20, 18, 22, 15, 11, 13, 5, 35, 4, 22, 34, 15, 4, 3, 21, 5, 22, 27, 11, 15, 13, 14, 32, 13, 4, 7, 30, 9, 20, 7, 20, 9, 1, 6, 35, 23, 8, 14, 16, 18, 6, 2, 35], device=&#39;cuda:0&#39;)) . . Note: above and below is same . x,y = dls.one_batch() . . Understanding Labels . dls.vocab . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . dls.vocab[0] . &#39;Abyssinian&#39; . . Tip: vocab gives as all labels as text. . . What&#39;s inside the tensors? . y . TensorCategory([13, 35, 8, 36, 3, 10, 10, 14, 22, 1, 5, 5, 5, 0, 4, 7, 11, 33, 18, 25, 20, 3, 33, 0, 25, 15, 27, 9, 17, 25, 19, 26, 9, 0, 35, 5, 6, 1, 31, 14, 7, 9, 8, 27, 2, 7, 21, 13, 26, 17, 25, 30, 31, 5, 19, 17, 4, 12, 29, 8, 21, 33, 18, 9], device=&#39;cuda:0&#39;) . . Note: Targets as coded. . x . TensorImage([[[[-1.3790, -1.3778, -1.3984, ..., -0.1093, 0.1460, -0.0339], [-1.3243, -1.3580, -1.3804, ..., 0.0449, 0.0572, -0.0411], [-1.3337, -1.3652, -1.3996, ..., -0.0918, -0.1107, -0.0857], ..., [-0.4574, -0.3503, -0.3927, ..., -0.6010, -0.7011, -0.7119], [-0.3509, -0.1960, -0.2069, ..., -0.6884, -0.6634, -0.6341], [-0.3221, -0.3299, -0.3177, ..., -0.5625, -0.4453, -0.4082]], [[-1.6744, -1.6758, -1.6812, ..., -0.5800, -0.2989, -0.4613], [-1.5871, -1.6285, -1.6568, ..., -0.3977, -0.3745, -0.4682], [-1.5626, -1.6162, -1.6632, ..., -0.5169, -0.5306, -0.5112], ..., [-1.0813, -0.9612, -0.9992, ..., -1.1308, -1.2921, -1.3514], [-0.9441, -0.7857, -0.7948, ..., -1.2437, -1.2741, -1.3164], [-0.9350, -0.9371, -0.9192, ..., -1.1914, -1.1397, -1.1095]], [[-1.7511, -1.7434, -1.7629, ..., -0.6031, -0.3360, -0.5057], [-1.6791, -1.7414, -1.7652, ..., -0.4428, -0.4310, -0.5192], [-1.6424, -1.7149, -1.7630, ..., -0.5840, -0.6148, -0.5747], ..., [-1.6312, -1.4844, -1.5600, ..., -1.3854, -1.6552, -1.7876], [-1.4652, -1.2946, -1.3208, ..., -1.5283, -1.6561, -1.7149], [-1.4120, -1.4189, -1.4288, ..., -1.5720, -1.5691, -1.5458]]], [[[-1.1709, -1.0320, -0.3882, ..., -2.0315, -2.0706, -2.0406], [-0.7207, -1.2576, -0.8119, ..., -2.0559, -2.0684, -2.0728], [-0.2858, -0.7315, -1.1736, ..., -2.0433, -2.0766, -2.0962], ..., [-0.6050, -0.6222, -0.7002, ..., 0.0488, 0.0771, 0.0815], [-0.6429, -0.6763, -0.7053, ..., 0.1518, -0.0409, 0.1402], [-0.6518, -0.7125, -0.7378, ..., 0.1249, 0.0496, 0.1191]], [[-0.8777, -0.6010, 0.1436, ..., -1.6740, -1.8200, -1.7518], [-0.3933, -0.8562, -0.2790, ..., -1.7451, -1.8429, -1.8417], [ 0.0344, -0.4096, -0.7499, ..., -1.8033, -1.8966, -1.8918], ..., [-0.6012, -0.6491, -0.6763, ..., 0.2168, 0.2401, 0.2356], [-0.6647, -0.7301, -0.6992, ..., 0.3333, 0.1283, 0.3030], [-0.6832, -0.7672, -0.7454, ..., 0.2804, 0.2115, 0.2686]], [[-0.6769, -0.6395, 0.1047, ..., -1.7038, -1.7277, -1.7018], [-0.1480, -0.7290, -0.2868, ..., -1.7481, -1.7221, -1.7096], [ 0.2469, -0.1654, -0.6479, ..., -1.7517, -1.7447, -1.7343], ..., [-0.5935, -0.7393, -0.8030, ..., 0.4358, 0.4557, 0.3989], [-0.6801, -0.8271, -0.8223, ..., 0.5317, 0.3682, 0.4378], [-0.7225, -0.8686, -0.8416, ..., 0.4712, 0.3960, 0.3788]]], [[[ 0.4054, 0.4157, 0.4160, ..., 0.4003, 0.3259, 0.1861], [ 0.4487, 0.4643, 0.4645, ..., 0.3877, 0.2998, 0.1810], [ 0.4725, 0.4946, 0.4947, ..., 0.3752, 0.2639, 0.1651], ..., [-0.5970, -0.4684, -0.6175, ..., 1.5827, 1.6680, 1.6609], [-0.5992, -0.5694, -0.4757, ..., 1.2104, 1.3103, 1.3862], [-0.6427, -0.7463, -0.7177, ..., 0.8878, 0.8792, 1.0099]], [[ 0.9298, 0.9404, 0.9407, ..., 0.9246, 0.8562, 0.7873], [ 0.9741, 0.9901, 0.9902, ..., 0.9117, 0.8413, 0.7847], [ 0.9984, 1.0209, 1.0210, ..., 0.8998, 0.8208, 0.7683], ..., [-0.1210, 0.0165, -0.1429, ..., 1.7798, 1.8600, 1.8469], [-0.1235, -0.0915, 0.0087, ..., 1.4962, 1.5836, 1.6497], [-0.1703, -0.2819, -0.2510, ..., 1.2888, 1.2676, 1.3861]], [[ 1.4446, 1.4550, 1.4553, ..., 1.4395, 1.4119, 1.4488], [ 1.4881, 1.5037, 1.5038, ..., 1.4270, 1.4177, 1.4495], [ 1.5118, 1.5338, 1.5339, ..., 1.4189, 1.4155, 1.4334], ..., [ 0.3907, 0.5309, 0.3682, ..., 2.0347, 2.1100, 2.0935], [ 0.3881, 0.4208, 0.5230, ..., 1.8240, 1.9038, 1.9552], [ 0.3403, 0.2258, 0.2576, ..., 1.6937, 1.6618, 1.7696]]], ..., [[[-0.7000, -0.6986, -0.7306, ..., -1.6313, -1.7078, -1.6480], [-0.6932, -0.6908, -0.7237, ..., -1.5230, -1.6776, -1.6388], [-0.6817, -0.6618, -0.6940, ..., -1.2779, -1.5299, -1.5905], ..., [ 0.7912, 0.9871, 0.9476, ..., 0.7637, 0.8491, 0.8404], [ 0.6435, 0.6536, 0.5389, ..., 0.2268, 0.2840, 0.7647], [ 0.2871, 0.1747, 0.0099, ..., 0.1704, 0.2587, 0.7739]], [[-0.5360, -0.5255, -0.5556, ..., -1.7426, -1.8394, -1.8137], [-0.5545, -0.5179, -0.5420, ..., -1.6711, -1.7999, -1.8003], [-0.5629, -0.4964, -0.5208, ..., -1.4790, -1.7004, -1.7559], ..., [ 0.8302, 1.0338, 0.9972, ..., 0.5573, 0.6361, 0.5920], [ 0.6521, 0.6515, 0.5315, ..., -0.0175, 0.0396, 0.5232], [ 0.2739, 0.1468, -0.0415, ..., -0.0737, 0.0125, 0.5333]], [[-0.1682, -0.1622, -0.1925, ..., -1.7371, -1.7819, -1.7964], [-0.1791, -0.1565, -0.1929, ..., -1.7217, -1.7671, -1.7829], [-0.1814, -0.1405, -0.1879, ..., -1.6271, -1.7128, -1.7439], ..., [ 0.7712, 1.0543, 1.0645, ..., 0.3954, 0.4138, 0.3209], [ 0.5921, 0.6752, 0.5766, ..., -0.2356, -0.1997, 0.2561], [ 0.2282, 0.1419, -0.0471, ..., -0.3060, -0.2291, 0.2662]]], [[[-1.7039, -1.5743, -0.7309, ..., -1.4899, -1.5223, -1.7014], [-1.6543, -1.3360, -0.7863, ..., -1.4862, -1.4992, -1.6543], [-1.4747, -0.9363, -0.9740, ..., -1.4786, -1.7038, -1.7715], ..., [-1.0359, -0.9016, -0.9339, ..., 1.1125, 1.1213, 0.7437], [-0.9960, -1.1363, -1.0869, ..., 0.8008, 1.0108, 0.9147], [-1.1167, -1.2009, -1.1964, ..., 0.6893, 1.3224, 0.2577]], [[-1.5970, -1.4079, -0.4739, ..., -1.2143, -1.3011, -1.4974], [-1.5914, -1.1409, -0.5364, ..., -1.1936, -1.2596, -1.4214], [-1.4410, -0.7377, -0.7863, ..., -1.1843, -1.4621, -1.5348], ..., [-0.6342, -0.4864, -0.5238, ..., 1.4785, 1.5627, 1.1883], [-0.6105, -0.7573, -0.7006, ..., 1.0617, 1.3020, 1.2896], [-0.7374, -0.8229, -0.8292, ..., 0.9343, 1.6271, 0.5592]], [[-1.7608, -1.7676, -1.6709, ..., -1.7209, -1.7915, -1.7875], [-1.7147, -1.7388, -1.6955, ..., -1.7866, -1.7723, -1.7779], [-1.6931, -1.5459, -1.6110, ..., -1.7689, -1.7772, -1.7837], ..., [-1.5969, -1.4913, -1.5156, ..., -0.6912, -0.3672, -0.7122], [-1.5018, -1.6808, -1.6485, ..., -0.5699, -0.2657, -0.5272], [-1.5775, -1.6876, -1.6868, ..., -0.0425, 0.4928, -0.8241]]], [[[ 1.4877, 1.4668, 1.5048, ..., 2.0390, 2.0327, 2.0266], [ 1.5198, 1.4866, 1.5066, ..., 2.0343, 2.0305, 2.0289], [ 1.5237, 1.4689, 1.5328, ..., 2.0266, 2.0217, 2.0152], ..., [ 1.4687, 1.8343, 1.9416, ..., -1.3055, -1.2876, -1.3210], [ 1.8619, 1.9001, 1.8640, ..., -1.1670, -1.3076, -1.3698], [ 1.8915, 1.8637, 1.8954, ..., -1.1410, -1.3742, -1.3414]], [[ 1.8535, 1.8615, 1.8702, ..., 2.2140, 2.2075, 2.2014], [ 1.8948, 1.9005, 1.8962, ..., 2.2092, 2.2053, 2.2037], [ 1.8926, 1.8579, 1.8944, ..., 2.2014, 2.2056, 2.2128], ..., [ 1.8888, 2.1500, 2.2057, ..., -1.2930, -1.2397, -1.3169], [ 2.2001, 2.2026, 2.1672, ..., -1.2065, -1.2288, -1.3877], [ 2.1885, 2.1623, 2.2023, ..., -1.2037, -1.3027, -1.3650]], [[ 2.0973, 2.0843, 2.1081, ..., 2.4264, 2.4200, 2.4138], [ 2.1341, 2.1351, 2.1528, ..., 2.4216, 2.4177, 2.4161], [ 2.1146, 2.0894, 2.1310, ..., 2.4138, 2.4146, 2.4143], ..., [ 2.3444, 2.4653, 2.4575, ..., -1.1536, -1.0986, -1.1590], [ 2.5056, 2.5002, 2.4774, ..., -1.0442, -1.1040, -1.1783], [ 2.5015, 2.5031, 2.5432, ..., -1.0341, -1.1608, -1.1347]]]], device=&#39;cuda:0&#39;) . . Note: Our stacked image tensor. . . Predictions of the baseline model. . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([1.4670e-06, 1.2070e-06, 8.4748e-07, 1.6964e-07, 7.0972e-06, 9.3213e-07, 1.9146e-06, 4.0787e-07, 1.3208e-06, 1.8394e-06, 1.8446e-08, 2.0282e-05, 9.3669e-04, 9.9753e-01, 4.6090e-06, 4.6171e-05, 8.3924e-05, 4.4448e-04, 3.7151e-07, 7.7943e-07, 6.8438e-06, 7.1965e-07, 2.7995e-07, 1.9403e-06, 1.0657e-06, 7.8017e-07, 1.8254e-05, 5.4245e-06, 4.5678e-06, 8.7494e-07, 3.8811e-06, 1.2178e-06, 6.4576e-07, 1.8837e-05, 8.5143e-04, 1.4807e-06, 1.7899e-06]) . . Note: result for first item that adds up to one. There are 37 outputs for 37 image categories and the results are in percentage for probability of each category. . _ . TensorCategory([13, 35, 8, 36, 3, 10, 10, 14, 22, 1, 5, 5, 5, 0, 4, 7, 11, 33, 18, 25, 20, 3, 33, 0, 25, 15, 27, 9, 17, 25, 19, 26, 9, 0, 35, 5, 6, 1, 31, 14, 7, 9, 8, 27, 2, 7, 21, 13, 26, 17, 25, 30, 31, 5, 19, 17, 4, 12, 29, 8, 21, 33, 18, 9]) . . Note: Category codes . len(preds[0]),preds[0].sum() . (37, tensor(1.0000)) . Prediction for 37 categories that adds up to one. . . FUNCTION FOR CLASSIFIYING MORE THAN TWO CATEGORY . For classifiying more than two category, we need to employ a new function. It is not totally different than sigmoid, in fact it starts with a sigmoid function. . plot_function(torch.sigmoid, min=-4,max=4) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/RangeFactories.cpp:25.) x = torch.linspace(min,max) . . Note: This is how torch.sigmoid squishes values between 0 and 1. . torch.random.manual_seed(42); . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . . Note: These are random numbers represent binary results of a hypothetical network. First colums represent 3&#8217;s the and second is 7&#8217;s standart deviation of 2. It generally shows how confident the model about the predictions. . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . . Note: If we apply the sigmoid, the result become like this(above). Obviously they aren&#8217;t adds up to one. These are relative confidence over inputs. For example first row says: it&#8217;s a three. But what is the probability? It is not clear. . (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . . Note: If we take the difference between these relative confidence the results become like this above: Now we can say that for the first item, model is 0.6025 (%60.25) confident. . this part is a bit different in the lesson video. so check the video. 1:35:20 . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . . Note: torch.softmax does that in one step. Now results for each item adds up to one and identical. . . Log Likelihood . targ = tensor([0,1,0,1,1,0]) . this is our softmax activations: . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . . Note: Nice trick for getting confidence level for each item. . lets see everything in a table: . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . . Warning: I think the last label is wrong here. It must be the confidence instead. . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . . Warning: There is a caveat here. These are neg of our confidence level, not loss. . Pytorch way of doing the same here: . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . . Note: Anyway, numbers are still not right, that will be addresses in the Taking the Log section below. The reason is F.nll_loss (negative log likelihood loss) needs arguments such that log is already applied to make the calculation right.(loss) . . Taking the Log . . Note: Directly from the book: . . Important: Confusing Name, Beware: The nll in nll_loss stands for &quot;negative log likelihood,&quot; but it doesn&#8217;t actually take the log at all! It assumes you have already taken the log. PyTorch has a function called log_softmax that combines log and softmax in a fast and accurate way. nll_loss is designed to be used after log_softmax. . When we first take the softmax, and then the log likelihood of that, that combination is called cross-entropy loss. In PyTorch, this is available as nn.CrossEntropyLoss (which, in practice, actually does log_softmax and then nll_loss): . pytorch&#39;s crossEntropy: . loss_func = nn.CrossEntropyLoss() . loss_func(acts, targ) . tensor(1.8045) . or: . F.cross_entropy(acts, targ) . tensor(1.8045) . . Note: this is the mean of all losses . and this is all results without taking the mean: . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . . Note: Results above are cross entrophy loss for each image in the list (of course our current numbers are fake numbers) . . Manual calculation log_softmax + nll_loss . First log_softmax: . log_sm_acts = torch.log_softmax(acts, dim=1) log_sm_acts . tensor([[-5.0672e-01, -9.2248e-01], [-6.8903e-01, -6.9729e-01], [-2.0160e+00, -1.4293e-01], [-3.3658e-03, -5.6958e+00], [-5.1760e-01, -9.0621e-01], [-1.0048e+00, -4.5589e-01]]) . Then negative log likelihood: . F.nll_loss(log_sm_acts, targ, reduction=&#39;none&#39;) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . . Note: Results are identical . . REVISITING THE BASELINE MODEL (Model Interpretation) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 8), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 7), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 5)] . this is our baseline we can start improveing from this point. . . IMPROVING THE MODEL . Fine Tune . Fine tune the model with default arguments: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.588707 | 4.300000 | 0.445873 | 00:21 | . epoch train_loss valid_loss error_rate time . 0 | 3.385068 | 2.263443 | 0.510825 | 00:26 | . . Note: This is where we overshot. Our loss just increase over second epoch is there a better way to find a learning rate? . . Learning Rate Finder . learn = cnn_learner(dls, resnet34, metrics=error_rate) . suggested_lr= learn.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/callback/schedule.py:270: UserWarning: color is redundantly defined by the &#39;color&#39; keyword argument and the fmt string &#34;ro&#34; (-&gt; color=&#39;r&#39;). The keyword argument will take precedence. ax.plot(val, idx, &#39;ro&#39;, label=nm, c=color) . . Warning: There is a discrepancy between lesson and reading group notebooks. In the book we get two values from the function but in reading group, only one. I thing there was an update for this function that not reflected in the book. . suggested_lr . SuggestedLRs(valley=tensor(0.0008)) . print(f&quot;suggested: {suggested_lr.valley:.2e}&quot;) . suggested: 8.32e-04 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=8.32e-04) . epoch train_loss valid_loss error_rate time . 0 | 2.203637 | 0.456601 | 0.139378 | 00:21 | . epoch train_loss valid_loss error_rate time . 0 | 0.631289 | 0.287444 | 0.087280 | 00:26 | . 1 | 0.423191 | 0.263927 | 0.085250 | 00:26 | . At this time it decreases steadily . What&#39;s under the hood of fine_tune . When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things: . Trains the randomly added layers for one epoch, with all other layers frozen | Unfreezes all of the layers, and trains them all for the number of epochs requested | . Lets do it manually . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 8.32e-04) . epoch train_loss valid_loss error_rate time . 0 | 1.806578 | 0.363257 | 0.114344 | 00:21 | . 1 | 0.697060 | 0.258624 | 0.083221 | 00:22 | . 2 | 0.449906 | 0.254586 | 0.087957 | 00:21 | . learn.unfreeze() . Run the lr_find again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn&#39;t appropriate any more: . learn.lr_find() . SuggestedLRs(valley=tensor(0.0001)) . Train again with the new lr. . learn.fit_one_cycle(6, lr_max=0.0001) . epoch train_loss valid_loss error_rate time . 0 | 0.369805 | 0.265072 | 0.085250 | 00:26 | . 1 | 0.379721 | 0.352767 | 0.112314 | 00:26 | . 2 | 0.320787 | 0.257370 | 0.075778 | 00:26 | . 3 | 0.198347 | 0.217450 | 0.066306 | 00:27 | . 4 | 0.143628 | 0.217090 | 0.066306 | 00:26 | . 5 | 0.111457 | 0.216973 | 0.066306 | 00:27 | . So far so good but there is more way to go . . Discriminative Learning Rates . Basically we use variable learning rate for the model. Bigger rate for the later layers and smaller for early layers. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 8.32e-04)# first lr learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(0.00005,0.0005))#second lr with a range . epoch train_loss valid_loss error_rate time . 0 | 1.783345 | 0.370482 | 0.119080 | 00:22 | . 1 | 0.700986 | 0.293102 | 0.096076 | 00:22 | . 2 | 0.448751 | 0.262937 | 0.093369 | 00:22 | . epoch train_loss valid_loss error_rate time . 0 | 0.390943 | 0.245929 | 0.079838 | 00:28 | . 1 | 0.356807 | 0.281976 | 0.088633 | 00:27 | . 2 | 0.344888 | 0.417350 | 0.117727 | 00:27 | . 3 | 0.267143 | 0.284152 | 0.081867 | 00:27 | . 4 | 0.217775 | 0.330306 | 0.092693 | 00:28 | . 5 | 0.172308 | 0.310047 | 0.081191 | 00:27 | . 6 | 0.122903 | 0.299161 | 0.079161 | 00:27 | . 7 | 0.099924 | 0.262270 | 0.074425 | 00:27 | . 8 | 0.059424 | 0.278250 | 0.074425 | 00:27 | . 9 | 0.045987 | 0.253283 | 0.067659 | 00:27 | . 10 | 0.036630 | 0.251685 | 0.068336 | 00:27 | . 11 | 0.034524 | 0.254469 | 0.067659 | 00:27 | . It is better most of the times.(sometimes I don&#39;t get good results, need to arrange the slice values more carefully) . learn.recorder.plot_loss() . . Note: Directly from the book: . As you can see, the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does not mean that it is getting less accurate, necessarily. Take a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we&#39;ve given the computer to help us to optimize. . . Important: I need to think about it how loss increase and accuracy stil becoming better. . Deeper Architectures . In general, a bigger model has the ability to better capture the real underlying relationships in your data, and also to capture and memorize the specific details of your individual images. However, using a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an out-of-memory error. This happens when you try to fit too much inside your GPU and looks like: . Cuda runtime error: out of memory . You may have to restart your notebook when this happens. The way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your DataLoaders with bs=. . The other downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called tensor cores that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module). . You can&#39;t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let&#39;s try a ResNet-50 now with mixed precision: . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(12, freeze_epochs=3) . epoch train_loss valid_loss error_rate time . 0 | 1.209030 | 0.308840 | 0.097429 | 00:20 | . 1 | 0.562807 | 0.326714 | 0.100812 | 00:21 | . 2 | 0.396488 | 0.263611 | 0.089310 | 00:21 | . epoch train_loss valid_loss error_rate time . 0 | 0.255827 | 0.262954 | 0.080514 | 00:24 | . 1 | 0.215601 | 0.256829 | 0.072395 | 00:24 | . 2 | 0.238660 | 0.392900 | 0.099459 | 00:23 | . 3 | 0.246021 | 0.409503 | 0.107578 | 00:24 | . 4 | 0.196632 | 0.448040 | 0.106225 | 00:23 | . 5 | 0.137433 | 0.353745 | 0.091340 | 00:23 | . 6 | 0.108764 | 0.333932 | 0.085250 | 00:24 | . 7 | 0.078872 | 0.295772 | 0.081867 | 00:24 | . 8 | 0.055900 | 0.273311 | 0.073072 | 00:24 | . 9 | 0.040353 | 0.274645 | 0.070365 | 00:24 | . 10 | 0.020883 | 0.260611 | 0.070365 | 00:24 | . 11 | 0.021018 | 0.259633 | 0.066982 | 00:24 | . learn.recorder.plot_loss() . As above traing time is not changed much. .",
            "url": "https://niyazikemer.com/fastbook/2021/07/22/chapter-5.html",
            "relUrl": "/fastbook/2021/07/22/chapter-5.html",
            "date": " • Jul 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Chapter 4 - Under the Hood - Training a Digit  Classifier",
            "content": ". I found this little one in front of my window. Suffering from foot deformity and can&#39;t fly. Now fully recovered and back his/her family. . import fastbook fastbook.setup_book() # below is for disabling Jedi autocomplete that doesn&#39;t work well. %config Completer.use_jedi = False . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . EXPLORING THE DATASET . What untar does? . . Note: &#8217;untar_data&#8217; come from fastai library, it downloads the data and untar it if it didn&#8217;t already and returns the destination folder. . path = untar_data(URLs.MNIST_SAMPLE) . ??untar_data . . Tip: Check it with &#8217;??&#8217; . What is path ? . path . Path(&#39;.&#39;) . . Tip: what is inside the current folder? this where the jupyter notebook works. &#8217;!&#8217; at the beginning means the command works on the terminal. . What is !ls ? . . Note: ls works on the terminal. (-d for only listing directories) . !ls . 2020-02-20-test.ipynb ghtop_images my_icons 2021-07-16-chapter-4.ipynb images README.md . can be used like this too. . !ls /home/niyazi/.fastai/data/mnist_sample/train -d . /home/niyazi/.fastai/data/mnist_sample/train . also like this: . !ls /home/niyazi/.fastai/data/mnist_sample/train/3 -d . /home/niyazi/.fastai/data/mnist_sample/train/3 . What is tree ? . . Note: for seeing tree sturucture of the files and folders (-d argument for directories) . !tree /home/niyazi/.fastai/data/mnist_sample/ -d . /home/niyazi/.fastai/data/mnist_sample/ ├── train │   ├── 3 │   └── 7 └── valid ├── 3 └── 7 6 directories . Path.BASE_PATH = path . What is ls() ? . . Note: &#8217;ls&#8217; is method by fastai similiar the Python&#8217;s list fuction but more powerful. . path.ls() . (#3) [Path(&#39;labels.csv&#39;),Path(&#39;train&#39;),Path(&#39;valid&#39;)] . . Note: Check this usage: . (path/&#39;train&#39;) . Path(&#39;train&#39;) . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . . Note: there are two folders under training folder . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() . . Note: this code returns and ordered list of paths . What is PIL ? (Python Image Library) . im3_path = threes[1] im3 = Image.open(im3_path) type(im3) #im3 . PIL.PngImagePlugin.PngImageFile . NumPy array . The 4:10 indicates we requested the rows from index 4 (included) to 10 (not included) and the same for the columns. NumPy indexes from top to bottom and left to right, so this section is located in the top-left corner of the image. . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . . Note: this is how it looks some part of the image in the NumPy array . Pytorch tensor . Here&#39;s the same thing as a PyTorch tensor: . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . . Note: It is possible to convert it to a tansor as well. . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;OrRd&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . BASELINE: Pixel similarity . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . . Note: &#8217;sevens&#8217; are still list of paths. &#8217;o&#8217; is a path in the list, then with the list comprehension we use the path to read the image, then cast the image into tensor.(Same for threes). &#8217;seven_tensor&#8217; is a list of tensors . show_image(three_tensors[0]); . . Tip: Show image shows the first tensor as image . show_image(tensor(im3)) . &lt;AxesSubplot:&gt; . . Note: check this in more straight way (im3&gt;tensor&gt;image) . Training Set: Stacking Tensors . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . type(stacked_sevens) . torch.Tensor . type(stacked_sevens[0]) . torch.Tensor . type(seven_tensors) . list . . Note: now we turn our list into a tensor size of ([6131, 28, 28]) . len(stacked_threes.shape) . 3 . . Note: This is rank (lenght of the shape) . stacked_threes.ndim . 3 . . Note: This is more direct way to get it. (ndim) . Mean of threes and sevens our ideal 3 and 7. . mean3 = stacked_threes.mean(0) show_image(mean3); . . Note: This is the mean of the all tensors through first axis. &#8217;Ideal Three&#8217; . mean7 = stacked_sevens.mean(0) show_image(mean7); . a_3 = stacked_threes[1] show_image(a_3); . Distance between the ideal three and other threes . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . . Note: Then we need to calculate the distance between the &#8217;ideal&#8217; and ordinary three.Two methods for getting the distance L1 Norm and MSE second one is panelize bigger mistake more havil, L1 is uniform. It is obvious that a_3 is closer to the perfect 3 so our approach worked at this time. (Both in L1 and MSE) . Pytorch L1 and MSE fuctions . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . . Note: torch.nn.functional as F (for mse, manually take the sqrt) . . Important: (from notebook) If you don&#8217;t know what C is, don&#8217;t worry as you won&#8217;t need it at all. In a nutshell, it&#8217;s a low-level (low-level means more similar to the language that computers use internally) language that is very fast compared to Python. To take advantage of its speed while programming in Python, try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors. . Array and Tensor Examples . data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) . arr # numpy . array([[1, 2, 3], [4, 5, 6]]) . tns # pytorch . tensor([[1, 2, 3], [4, 5, 6]]) . Splitting, adding, multiplying tensors . tns[:,1] . tensor([2, 5]) . tns[1,1:3] . tensor([5, 6]) . tns+1 . tensor([[2, 3, 4], [5, 6, 7]]) . tns.type() . &#39;torch.LongTensor&#39; . tns*1.5 . tensor([[1.5000, 3.0000, 4.5000], [6.0000, 7.5000, 9.0000]]) . Validation set :Stacking Tensors . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Manual L1 distance function . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . This is broadcasting: . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1270, 0.1254, 0.1114, ..., 0.1494, 0.1097, 0.1365]), torch.Size([1010])) . . Note: I think this an example of not using loops which slows down the process (check above important tag). Although shapes of the tensors don&#8217;t match, out function still works. Pytorch fills the gaps. . here is another example. Shapes don&#39;t match. . tensor([1,2,3]) + tensor(1) . tensor([2, 3, 4]) . (valid_3_tens-mean3).shape . torch.Size([1010, 28, 28]) . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . here is an another broadcasting for all validation set: . is_3(valid_3_tens) . tensor([True, True, True, ..., True, True, True]) . Accuracy of our &#39;ideal&#39; 3 and 7 . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . STOCHASTIC GRADIENT DECENT (SGD) . Arthur Samues Machine Learning process: . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss (SGD) | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&#39;t want to wait any longer). | . #caption The gradient descent process #alt Graph showing the steps for Gradient Descent gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop GD example . def f(x): return x**2 . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . We need to decrease the loss . . How to calculate gradient: . Now our tensor xt is under investigation. Pytorch will keeps its eye on it. . xt = tensor(3.).requires_grad_() . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . Result is 9 but there is a grad function in the result. . . yt.backward() . backward calculates the derivative. . xt.grad . tensor(6.) . result is 6. . . now with a bigger tensor . xt = tensor([3.,4.,10.]).requires_grad_() xt . tensor([ 3., 4., 10.], requires_grad=True) . def f(x): return (x**2).sum() . yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . again we expect 2*xt: . yt.backward() . xt.grad . tensor([ 6., 8., 20.]) . End to end SGD example . time = torch.arange(0,20).float() time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed) . &lt;matplotlib.collections.PathCollection at 0x7f8f2edd27c0&gt; . Now we are trying to come up with some parameters for our quadratic fuction that predicts speed any given time. Our choice is quadratic but that could be something else too. with a quadratic function our problem would be much easier. . here is the function gets time and parameter as inputs and predicts a result: . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . this our loss function that calculate distance between prediction and target( actual mesurements) . def mse(preds, targets): return ((preds-targets)**2).mean().sqrt() . Step 1: here are initial random parameters: . params = torch.randn(3).requires_grad_() params . tensor([ 0.9569, 0.0048, -0.1506], requires_grad=True) . orig_params = params.clone() . Step 2: calculate predictions: . preds = f(time,params) . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . Step 3: Calculate the loss . loss = mse(preds,speed) loss . tensor(139.3082, grad_fn=&lt;SqrtBackward&gt;) . . The Question is how to improve these results: . Step 4: first we calculate the gradient: . Pytorch makes it easier we just call the backward() on the loss but it calculates gradient for the params &#39;a&#39; &#39;b&#39; and &#39;c&#39;._ . loss.backward() params.grad # this is the derivative of the initial values in other word our slope. . tensor([165.0324, 10.5991, 0.6615]) . params.grad * 1e-5 # scaler at the end is learning rate. . tensor([1.6503e-03, 1.0599e-04, 6.6150e-06]) . params # they are still same. . tensor([ 0.9569, 0.0048, -0.1506], requires_grad=True) . . Step 5: Step the weight. . we picked the learning rate 1e-5 very small step to avoid missing the lowest possible loss. . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . preds = f(time,params) mse(preds, speed) . tensor(139.0348, grad_fn=&lt;SqrtBackward&gt;) . lets create a function for all these steps . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: repeat the step: . for i in range(10): apply_step(params) . 139.03475952148438 138.76133728027344 138.4879150390625 138.2145538330078 137.94122314453125 137.6679229736328 137.39466857910156 137.12144470214844 136.84825134277344 136.5751190185547 . params = orig_params.detach().requires_grad_() . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . . MNIST . Loss Function our 3 and 7 recognizer. Currently we use metric not loss . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . train_x.size() . torch.Size([12396, 784]) . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . How tensor manipulated . temp_tensor = tensor (1) . temp_tensor . tensor(1) . type(temp_tensor) . torch.Tensor . is above tensor is wrong what&#39;s the difference? . . we have a tensor . temp_tensor = tensor([1]) . then we multiuplied the inside of . temp_tensor =tensor([1]*4) . temp_tensor . tensor([1, 1, 1, 1]) . temp_tensor.shape . torch.Size([4]) . temp_tensor.ndim . 1 . temp_tensor.size() . torch.Size([4]) . (temp_tensor).unsqueeze(1) . tensor([[1], [1], [1], [1]]) . . Warning: looked changed but why size is still unchanged why not [4,1] . temp_tensor.shape . torch.Size([4]) . temp_tensor.size() . torch.Size([4]) . How unsqueeze works? . . Warning: Whaaaaaaaaaaaaat? . (temp_tensor).unsqueeze(1) doesn&#39;t work but (temp_tensor*1).unsqueeze(1) you need to unsqueeze it when creating otherwise it doesnt work. I do not believe it. . temp_tensor = tensor([1]).unsqueeze(1) . temp_tensor.shape . torch.Size([1, 1]) . temp_tensor =tensor([1]*1).unsqueeze(1) . Dataset . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,x.ndim,y . (torch.Size([784]), 1, tensor([1])) . we create list of tuples, each tuple contains a image and a target . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . same for validation . . Weights . this is not clear on the videos but consider a layer NN of 728 inputs and 1 output. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . weights.shape . torch.Size([784, 1]) . bias = init_params(1) . . Note: The function weights*pixels won&#8217;t be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its intercept is 0). You might remember from high school math that the formula for a line is y=w*x+b; we still need the b. We&#8217;ll initialize it to a random number too: . bias . tensor([0.0959], requires_grad=True) . Again transposing the weight matrix is not clear but Tariq Rashed&#39;s book would be very beneficial at this point . (train_x[0]*weights.T).sum() + bias . tensor([-5.6867], grad_fn=&lt;AddBackward0&gt;) . for all dataset put this multiplication in a function . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[ -5.6867], [ -6.5451], [ -2.0241], ..., [-14.3286], [ 4.3505], [-12.6773]], grad_fn=&lt;AddBackward0&gt;) . Create a tensor with results based on their value (above 0.5 is 7 and below it is 3) . corrects = (preds&gt;0.5).float() == train_y corrects . tensor([[False], [False], [False], ..., [ True], [False], [ True]]) . . check it . corrects.float().mean().item() . 0.4636172950267792 . almost half of them is 3 and the other half is 7 (since weighs are totally random) . . Why we need a loss Function . Basically we need to have gradients for correcting our weighs, we need to know which direction we need to go . If you dont understand all of these, ckeck khan academy for gradient. . trgts = tensor([1,0,1]) prds = tensor([0.9, 0.4, 0.2]) . def mnist_loss(predictions, targets): return torch.where(targets==1, 1-predictions, predictions).mean() . torch.where(trgts==1, 1-prds, prds) . tensor([0.1000, 0.4000, 0.8000]) . mnist_loss(prds,trgts) . tensor(0.4333) . Sigmoid . We need this for squishing predictions between 0-1 . def sigmoid(x): return 1/(1+torch.exp(-x)) . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . update the fuction with the sigmoid thats all. . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . What are SGD and Mini-Batches . This explains most of it. . coll = range(15) dl = DataLoader(coll, batch_size=5, shuffle=True) list(dl) . [tensor([ 0, 2, 10, 13, 8]), tensor([11, 12, 4, 1, 5]), tensor([ 3, 14, 6, 9, 7])] . but this is only a list however we neeed a tuple consist of independent and dependent variable. . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . DataLoader . then put it into a Dataloader. . dl = DataLoader(ds, batch_size=6, shuffle=True) list(dl) . [(tensor([ 1, 23, 9, 8, 24, 2]), (&#39;b&#39;, &#39;x&#39;, &#39;j&#39;, &#39;i&#39;, &#39;y&#39;, &#39;c&#39;)), (tensor([14, 25, 13, 11, 19, 5]), (&#39;o&#39;, &#39;z&#39;, &#39;n&#39;, &#39;l&#39;, &#39;t&#39;, &#39;f&#39;)), (tensor([ 0, 10, 4, 7, 18, 12]), (&#39;a&#39;, &#39;k&#39;, &#39;e&#39;, &#39;h&#39;, &#39;s&#39;, &#39;m&#39;)), (tensor([ 6, 21, 15, 16, 22, 3]), (&#39;g&#39;, &#39;v&#39;, &#39;p&#39;, &#39;q&#39;, &#39;w&#39;, &#39;d&#39;)), (tensor([20, 17]), (&#39;u&#39;, &#39;r&#39;))] . now we have batches and tuples . . all together . It&#39;s time to implement the process we saw in &lt;&gt;. In code, our process will be implemented something like this for each epoch:&lt;/p&gt; for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; weights = init_params((28*28,1)) bias = init_params(1) . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . valid_dl = DataLoader(valid_dset, batch_size=256) . a small test . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . . predictions . preds = linear1(batch) preds . tensor([[ 8.0575], [14.3841], [-3.8017], [ 5.1179]], grad_fn=&lt;AddBackward0&gt;) . loss . loss = mnist_loss(preds, train_y[:4]) loss . tensor(0.2461, grad_fn=&lt;MeanBackward0&gt;) . gradients . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0010), tensor([-0.0069])) . for the step we need a optimizer . . put all into a function except the optimizer. . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0021), tensor([-0.0138])) . . Warning: if you do it twice results are change. . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0031), tensor([-0.0207])) . weights.grad.zero_() bias.grad.zero_(); . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . little conversion to our results, it&#39;s important because we need to understand that what our model says about the numbers(three or not three) . (preds&gt;0.0).float() == train_y[:4] . tensor([[ True], [ True], [False], [ True]]) . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . . this is training accuracy . batch_accuracy(linear1(batch), train_y[:4]) . tensor(0.7500) . this is for validation for all set . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5136 . Training . one epochs of training . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.7121 . then more . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8656 0.9203 0.9457 0.9549 0.9593 0.9623 0.9652 0.9666 0.9681 0.9705 0.9706 0.9711 0.972 0.973 0.9735 0.9735 0.974 0.9745 0.9755 0.9755 . Optimizer . Let&#39;s start creating our model with Pytorch instead of our &quot;linear1&quot; function. Pytorch also creates parameters like our init_params function. . linear_model = nn.Linear(28*28,1) . w,b = linear_model.parameters() . w.shape, b.shape . (torch.Size([1, 784]), torch.Size([1])) . Custom optimizer . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . opt = BasicOptim(linear_model.parameters(), lr) . new training fuction will be . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.4078 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.8193 0.8418 0.9136 0.9331 0.9477 0.9555 0.9629 0.9658 0.9673 0.9697 0.9717 0.9736 0.9751 0.9761 0.9761 0.9775 0.9775 0.9785 0.9785 . Fastai&#39;s SDG class . instead of using &quot;BasicOptim&quot; class we can use fastai&#39;s SGD class . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.7808 0.8623 0.9185 0.9365 0.9521 0.9575 0.9638 0.9658 0.9678 0.9707 0.9726 0.9741 0.9751 0.9761 0.9765 0.9775 0.978 0.9785 0.9785 . Just remove the &quot;train_model&quot; at this time and use fastai&#39;s &quot;Learner.fit&quot; Before using Learner first we need to pass our trainig and validation data into &quot;Dataloaders&quot; not &quot;dataloader&quot; . Fastai&#39;s Dataloaders . dls = DataLoaders(dl, valid_dl) . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . FastAi&#39;s Fit . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.637166 | 0.503575 | 0.495584 | 00:00 | . 1 | 0.562232 | 0.139727 | 0.900393 | 00:00 | . 2 | 0.204552 | 0.207935 | 0.806183 | 00:00 | . 3 | 0.088904 | 0.114767 | 0.904809 | 00:00 | . 4 | 0.046327 | 0.081602 | 0.930324 | 00:00 | . 5 | 0.029754 | 0.064530 | 0.944553 | 00:00 | . 6 | 0.022963 | 0.054135 | 0.954858 | 00:00 | . 7 | 0.019966 | 0.047293 | 0.961236 | 00:00 | . 8 | 0.018464 | 0.042515 | 0.965162 | 00:00 | . 9 | 0.017573 | 0.039011 | 0.966634 | 00:00 | . Adding a Nonlinearity . The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there&#39;s no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters. (From Fastbook) . Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for w1 and w2 and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the universal approximation theorem._ The three lines of code that we have here are known as layers. The first and third are known as linear layers, and the second line of code is known variously as a nonlinearity, or activation function.(From Fastbook) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.303284 | 0.398378 | 0.511776 | 00:00 | . 1 | 0.142384 | 0.221517 | 0.817959 | 00:00 | . 2 | 0.079702 | 0.112610 | 0.917076 | 00:00 | . 3 | 0.052855 | 0.076474 | 0.942100 | 00:00 | . 4 | 0.040301 | 0.059791 | 0.958783 | 00:00 | . 5 | 0.033824 | 0.050389 | 0.964181 | 00:00 | . 6 | 0.030075 | 0.044483 | 0.966143 | 00:00 | . 7 | 0.027629 | 0.040465 | 0.966634 | 00:00 | . 8 | 0.025865 | 0.037553 | 0.969578 | 00:00 | . 9 | 0.024499 | 0.035336 | 0.971541 | 00:00 | . 10 | 0.023391 | 0.033579 | 0.972522 | 00:00 | . 11 | 0.022467 | 0.032142 | 0.973994 | 00:00 | . 12 | 0.021679 | 0.030936 | 0.973994 | 00:00 | . 13 | 0.020997 | 0.029901 | 0.974975 | 00:00 | . 14 | 0.020398 | 0.028998 | 0.974975 | 00:00 | . 15 | 0.019869 | 0.028198 | 0.975957 | 00:00 | . 16 | 0.019395 | 0.027484 | 0.976448 | 00:00 | . 17 | 0.018966 | 0.026841 | 0.976938 | 00:00 | . 18 | 0.018577 | 0.026259 | 0.977429 | 00:00 | . 19 | 0.018220 | 0.025730 | 0.977429 | 00:00 | . 20 | 0.017892 | 0.025244 | 0.978410 | 00:00 | . 21 | 0.017588 | 0.024799 | 0.979882 | 00:00 | . 22 | 0.017306 | 0.024388 | 0.979882 | 00:00 | . 23 | 0.017042 | 0.024008 | 0.980373 | 00:00 | . 24 | 0.016794 | 0.023656 | 0.980864 | 00:00 | . 25 | 0.016561 | 0.023328 | 0.980864 | 00:00 | . 26 | 0.016341 | 0.023022 | 0.980864 | 00:00 | . 27 | 0.016133 | 0.022737 | 0.981845 | 00:00 | . 28 | 0.015935 | 0.022470 | 0.981845 | 00:00 | . 29 | 0.015746 | 0.022221 | 0.981845 | 00:00 | . 30 | 0.015566 | 0.021988 | 0.982336 | 00:00 | . 31 | 0.015395 | 0.021769 | 0.982336 | 00:00 | . 32 | 0.015231 | 0.021565 | 0.982826 | 00:00 | . 33 | 0.015076 | 0.021371 | 0.982826 | 00:00 | . 34 | 0.014925 | 0.021190 | 0.982826 | 00:00 | . 35 | 0.014782 | 0.021018 | 0.982826 | 00:00 | . 36 | 0.014643 | 0.020856 | 0.982826 | 00:00 | . 37 | 0.014510 | 0.020703 | 0.982826 | 00:00 | . 38 | 0.014382 | 0.020558 | 0.982826 | 00:00 | . 39 | 0.014258 | 0.020420 | 0.982826 | 00:00 | . recorder is a fast ai method . plt.plot(L(learn.recorder.values).itemgot(2)); . Last value . learn.recorder.values[-1][2] . 0.982826292514801 . GOING DEEPER . why deeper if it is two and a nonlinear between them is enough . We already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers. . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.089727 | 0.011755 | 0.997056 | 00:13 | . &lt;/div&gt; .",
            "url": "https://niyazikemer.com/fastbook/2021/07/16/chapter-4.html",
            "relUrl": "/fastbook/2021/07/16/chapter-4.html",
            "date": " • Jul 16, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://niyazikemer.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://niyazikemer.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://niyazikemer.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://niyazikemer.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}