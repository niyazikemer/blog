{
  
    
        "post0": {
            "title": "Convert image files into a tensor and back with FastAi, PIL, Torchvision and  Vanilla Pytorch",
            "content": "import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * # below is for disabling Jedi autocomplete that doesn&#39;t work well in my computer. #Please comment it out if it was not a case for you. %config Completer.use_jedi = False . . Note: This image created by NightCafe creator with one word text input: &quot;FastAi&quot; NightCafe is here . Created with NightCafe . This is where my data (image) is located ( a path to a folder) . . Note: Understanding &quot;Path&quot; class is an another topic. In our case we can think Path as a location in a computer. . path = Path(&#39;Data/my_data&#39;) path . Path(&#39;Data/my_data&#39;) . This is how I open a particular image in a folder: . . Note: PIL is the Python Imaging Library by Fredrik Lundh and Contributors. Most known fork is Pillow and if you want to look deeper please follow this link: Pillow Documents . Im = PILImage.create(path/&#39;1528.png&#39;) . Now we have a PILimage . type(Im) . fastai.vision.core.PILImage . . Tip: There is no need to install PIL if you have FastAi. It is included in the library. . This is how we see it. . Im.show() . &lt;AxesSubplot:&gt; . This is how a PILimage converted into a Pytorch tensor manually . . Note: Just pass the PIlimage to tensor method. . Im_tensor = tensor(Im) . Shape of the tensor is: . Im_tensor.shape . torch.Size([970, 1057, 3]) . RGB values at x , y (300,700) location: All values are the same. (255) . Im_tensor[300][700] . tensor([255, 255, 255], dtype=torch.uint8) . Change the order of the tensor . . Tip: Sometimes(almost everytime) it is important to keep the tensor dimension in a particular order. In this case it is &quot;C H W&quot; (channel x heigh x weigh). It is easy with &quot;permute&quot; method. . Im_tensor = Im_tensor.permute(2,0,1) . Now the shape is right. . Im_tensor.shape . torch.Size([3, 970, 1057]) . The result is 217 (type:tensor) . . Important: At the location of 800 and 99, the value of the &quot;red&quot; channel is 217. Please keep this is in mind. We will see this exact number in other methods too. . Im_tensor[0][800][99] . tensor(217, dtype=torch.uint8) . Now we will use Torchvision ToTensor class for the same process. . . Note: Torchvision library is part of the PyTorch project. It used for transforming images, contain pretrained models and datasets etc. Below we import transforms from the library. . import torchvision.transforms as T . torchvis_transform = T.ToTensor() . Img_torchvis = torchvis_transform(Im) . . Important: With Torchvision, we do not need to arrange the order of tensor dimension, It is already how is expected. &quot;C H W&quot; (channel x heigh x weigh). . Img_torchvis.shape . torch.Size([3, 970, 1057]) . The result is now 0.8510 (tensor) . . Note: The result is not 217 but 0.8510. Same location but different value. Why? . Img_torchvis[0][800][99] . tensor(0.8510) . . Note: type is same . type(Img_torchvis) . torch.Tensor . Finally, fast.ai ToTensor class . doc(ToTensor ) . . Note: There is a ToTensor Class in the FastAi library but I believe the behaviour is slightly different than the previous examples. . transform = ToTensor() . Img_tensor = transform(Im) . Img_tensor.shape . torch.Size([3, 970, 1057]) . . Note: Shape of the tensor is right but value is 217 but not 0.8510 . The result is 217 again. . Img_tensor[0][800][99] . TensorImage(217, dtype=torch.uint8) . . Note: In fact result is same just need to be clamped between 1 and 0. . Img_tensor=Img_tensor/255 . Now 0.8510 . Img_tensor[0][800][99] . TensorImage(0.8510) . Im_tensor[0][800][99] . tensor(217, dtype=torch.uint8) . Transform a tensor (possibly a prediction) back into a PILimage: . . Note: We use torchvision library to convert the tensor into image. . transform_to_im = T.ToPILImage() . image_from_tensor= transform_to_im(Im_tensor) . . Note: It is useful when Debugging your code during develelopment, feeding a model with real data. . image_from_tensor .",
            "url": "https://niyazikemer.com/fastbook/data/fastai/2022/04/27/Image_exploration.html",
            "relUrl": "/fastbook/data/fastai/2022/04/27/Image_exploration.html",
            "date": " • Apr 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Tiny bug world with neuro evolution (2018)",
            "content": "Github Link is here . .",
            "url": "https://niyazikemer.com/coding/deeplearning/2022/01/15/myFirstNN.html",
            "relUrl": "/coding/deeplearning/2022/01/15/myFirstNN.html",
            "date": " • Jan 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Resnet - Implementation from scratch",
            "content": "Unrelated! . What is Resnet? . Video 1 - What is Resnet? &amp; Implementation of the Basic Block - Resnet From Scratch . . from fastbook: Resnet: chapter-14 . Note: In this chapter, we will build on top of the CNNs introduced in the previous chapter and explain to you the ResNet (residual network) architecture. It was introduced in 2015 by Kaiming He et al. in the article &quot;Deep Residual Learning for Image Recognition&quot; and is by far the most used model architecture nowadays. More recent developments in image models almost always use the same trick of residual connections, and most of the time, they are just a tweak of the original ResNet. . Training of networks of different depth (courtesy of Kaiming He et al.) . . Note: In 2015, the authors of the ResNet paper noticed something that they found curious. Even after using batchnorm, they saw that a network using more layers was doing less well than a network using fewer layers—and there were no other differences between the models. Most interestingly, the difference was observed not only in the validation set, but also in the training set; so, it wasn&#8217;t just a generalization issue, but a training issue. As the paper explains: . . How Resnet works. . from fastbook: Resnet: chapter-14 . Note: Again, this is rather inaccessible prose—so let&#8217;s try to restate it in plain English! If the outcome of a given layer is x, when using a ResNet block that returns y = x+block(x) we&#8217;re not asking the block to predict y, we are asking it to predict the difference between y and x. So the job of those blocks isn&#8217;t to predict certain features, but to minimize the error between x and the desired y. A ResNet is, therefore, good at learning about slight differences between doing nothing and passing though a block of two convolutional layers (with trainable weights). This is how these models got their name: they&#8217;re predicting residuals (reminder: &quot;residual&quot; is prediction minus target). . . What does that mean? . A simple ResNet block (courtesy of Kaiming He et al.) . Resnet Stages, Basic Blocks and Layers. . ResNet Architecture (courtesy of Kaiming He et al.) . What does Basic Block look like? . A simple ResNet block (courtesy of Kaiming He et al.) . . create a basic block/test the block with random tensor with random channel numbers/check downsample . Pytorch Resnet Implementation . import torch import torch.nn as nn import torchvision.models as models . models.resnet34() . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True) ) . Resnet From Scratch . Start with basic blocks: . class BasicBlock(nn.Module): def __init__(self,in_chs, out_chs): super().__init__() if in_chs==out_chs: self.stride=1 else: self.stride=2 self.conv1 = nn.Conv2d(in_chs,out_chs,kernel_size=3, padding=1,stride=self.stride,bias=False) self.bn1 = nn.BatchNorm2d(out_chs) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(out_chs,out_chs,kernel_size=3, padding=1,stride=1,bias=False) self.bn2 = nn.BatchNorm2d(out_chs) if in_chs==out_chs: self.downsample=None else: self.downsample= nn.Sequential(#nn.AvgPool2d(2,2), nn.Conv2d(in_chs,out_chs, kernel_size=1,stride=2,bias=False), nn.BatchNorm2d(out_chs)) def forward(self,x): skip_conn=x x=self.conv1(x) x=self.bn1(x) x=self.relu(x) x=self.conv2(x) x=self.bn2(x) if self.downsample: skip_conn=self.downsample(skip_conn) x+=skip_conn x=self.relu(x) return x . . Note: Turns out AvgPool experiment didn&#8217;t work. I&#8217;d thought that maybe getting an avarage of channels before conv2d downsampling could improve the result since kernel size 1 and stride 2 couse some information loss. (I believe :-)) . x=torch.randn(1,64,112,112) basic_block=BasicBlock(64,128) basic_block(x).shape . torch.Size([1, 128, 56, 56]) . . Note: This is what I expect. Deccrease the image size by half and double the number of channels. . #input = torch.randn(1, 64, 128, 128) #output = m(input) #output.shape . . Note: &#8217;repeat&#8217; is repeatitation of Basic blocks in corresponding stages. . repeat=[3,4,6,3] channels=[64,128,256,512] . Visualize channels and stages. . in_chans=64 for sta,(rep,out_chans) in enumerate(zip(repeat,channels)): for n in range(rep): print(sta,in_chans,out_chans) in_chans=out_chans . 0 64 64 0 64 64 0 64 64 1 64 128 1 128 128 1 128 128 1 128 128 2 128 256 2 256 256 2 256 256 2 256 256 2 256 256 2 256 256 3 256 512 3 512 512 3 512 512 . Create Basic Block Stages . def make_block(basic_b=BasicBlock,repeat=[3,4,6,3],channels=[64,128,256,512]): in_chans=channels[0] stages=[] for sta,(rep,out_chans) in enumerate(zip(repeat,channels)): blocks=[] for n in range(rep): blocks.append(basic_b(in_chans,out_chans)) #print(sta,in_chans,out_chans) in_chans=out_chans stages.append((f&#39;conv{sta+2}_x&#39;,nn.Sequential(*blocks))) #print(stages) return stages . Complete the Resnet implementation . class ResneTTe34(nn.Module): def __init__(self,num_classes): super().__init__() #stem self.conv1=nn.Conv2d(3,64, kernel_size=7, stride=2,padding=3,bias=False) self.bn1=nn.BatchNorm2d(64) self.relu=nn.ReLU() self.max_pool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1) # res-stages self.stage_modules= make_block() for stage in self.stage_modules: self.add_module(*stage) self.avg_pool=nn.AdaptiveAvgPool2d(output_size=(1,1)) self.fc=nn.Linear(512,num_classes,bias=True) #self.softmax=nn.Softmax(dim=1) def forward(self,x): x=self.conv1(x) x=self.bn1(x) x=self.relu(x) x=self.max_pool(x) x=self.conv2_x(x) x=self.conv3_x(x) x=self.conv4_x(x) x=self.conv5_x(x) x=self.avg_pool(x) x=torch.flatten(x,1) x=self.fc(x) #x=self.softmax(x) return x . . Note: I don&#8217;t why but softmax didn&#8217;t work well. . x=torch.randn(1,3,224,224) my_resnette=ResneTTe34(10) my_resnette(x).shape . torch.Size([1, 10]) . after conv layers: torch.Size([1, 512, 7, 7]) | after avg_pool: torch.Size([1, 512, 1, 1]) | after flatten: torch.Size([1, 512]) | after fc: torch.Size([1, 10]) | . Training with Fast AI . Video - 2 - Resnet Class Implementation - Resnet From Scratch . . import fastbook fastbook.setup_book() from fastai.vision.all import * . Dataset: IMAGENETTE_160 . A subset of 10 easily classified classes from Imagenet: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute. . fast.ai imagenette dataset . path = untar_data(URLs.IMAGENETTE_160) data_block=DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(160), batch_tfms=[*aug_transforms(min_scale=0.5, size=160), Normalize.from_stats(*imagenet_stats)], ) dls = data_block.dataloaders(path, bs=512) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . dls.c . 10 . . Note: dls.c Number of classes in the dataloaders. . dls.show_batch(max_n=12) . New Resnet instance: . rn=ResneTTe34(10) rn . ResneTTe34( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (conv2_x): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (conv3_x): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (conv4_x): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (conv5_x): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU() (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=10, bias=True) ) . Create a learner. . . Note: Detailed information about fastai learner class: Documentation . learn = Learner(dls, rn, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . . Note: Learn more about fastai learning rate finder: here . learn.lr_find() . SuggestedLRs(valley=0.00010964782268274575) . The Learning Rate Finder . Cyclical Learning Rates for Training Neural Networks . Training: . learn.fit_one_cycle(20, 0.000109) . epoch train_loss valid_loss accuracy time . 0 | 2.380851 | 2.387236 | 0.103439 | 00:10 | . 1 | 2.212096 | 2.009294 | 0.277962 | 00:11 | . 2 | 1.998115 | 1.962252 | 0.363312 | 00:11 | . 3 | 1.782758 | 1.822328 | 0.417580 | 00:11 | . 4 | 1.590943 | 1.815316 | 0.444586 | 00:11 | . 5 | 1.441378 | 1.804929 | 0.473121 | 00:11 | . 6 | 1.309173 | 1.451521 | 0.548025 | 00:11 | . 7 | 1.202027 | 1.469768 | 0.544204 | 00:11 | . 8 | 1.106232 | 1.137969 | 0.628790 | 00:10 | . 9 | 1.023413 | 1.444246 | 0.564331 | 00:11 | . 10 | 0.944548 | 1.267145 | 0.609936 | 00:11 | . 11 | 0.878883 | 1.263278 | 0.608662 | 00:10 | . 12 | 0.824138 | 1.046869 | 0.673631 | 00:11 | . 13 | 0.780820 | 0.930782 | 0.704713 | 00:11 | . 14 | 0.738063 | 0.935515 | 0.697070 | 00:10 | . 15 | 0.693144 | 0.887937 | 0.720510 | 00:11 | . 16 | 0.660048 | 1.019186 | 0.687643 | 00:10 | . 17 | 0.623636 | 0.907101 | 0.716688 | 00:10 | . 18 | 0.594946 | 0.891949 | 0.715924 | 00:11 | . 19 | 0.583335 | 0.914399 | 0.711083 | 00:11 | . Some Results: . Original Resnet34 lrfind graph was smoother and I am investigating now . | My Implementation baseline accuracy: %62, PyTorch resnet implementation:74 . | after setting linear chanel bias=True: %64 . | after setting conv layers bias=False: %65 . | after softmax removed: %72 . | training 50 epochs IMAGENETTE_160 :%78 . | training 20 epochs with bigger images IMAGENETTE_320 :%82 . | . Video - 3 - Training &#39;My Resnet&#39; - Resnet From Scratch . . Pytorch&#39;s Resnet34 implementation for Benchmark . resnet=models.resnet34() . learn_resnet = Learner(dls, resnet, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . learn_resnet.lr_find() . SuggestedLRs(valley=0.0004786300996784121) . Looks smoother. . learn_resnet.fit_one_cycle(20, 0.000478) . epoch train_loss valid_loss accuracy time . 0 | 6.478988 | 6.033509 | 0.117197 | 00:12 | . 1 | 5.071272 | 4.286289 | 0.204331 | 00:11 | . 2 | 3.680538 | 2.191735 | 0.353376 | 00:11 | . 3 | 2.801381 | 3.466609 | 0.344204 | 00:12 | . 4 | 2.248471 | 1.444590 | 0.537580 | 00:12 | . 5 | 1.869702 | 2.074517 | 0.430064 | 00:12 | . 6 | 1.602450 | 1.283586 | 0.594140 | 00:12 | . 7 | 1.397508 | 1.226493 | 0.589299 | 00:12 | . 8 | 1.226750 | 0.929667 | 0.713885 | 00:12 | . 9 | 1.112665 | 1.189254 | 0.607898 | 00:12 | . 10 | 1.005993 | 1.106539 | 0.647643 | 00:12 | . 11 | 0.917422 | 1.082780 | 0.669045 | 00:12 | . 12 | 0.841141 | 1.346959 | 0.602803 | 00:12 | . 13 | 0.777760 | 0.885834 | 0.729682 | 00:12 | . 14 | 0.713249 | 1.043039 | 0.678981 | 00:12 | . 15 | 0.660397 | 1.161693 | 0.661656 | 00:12 | . 16 | 0.619790 | 0.947324 | 0.710318 | 00:12 | . 17 | 0.576243 | 0.822031 | 0.744204 | 00:12 | . 18 | 0.549599 | 0.861511 | 0.734522 | 00:12 | . 19 | 0.519339 | 0.836370 | 0.741656 | 00:11 | . A little better result. . A test for 50 epochs. (%5 better) . rn_higher_epoch=ResneTTe34(10) learn_higher_epoch = Learner(dls, rn_higher_epoch, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . learn_higher_epoch.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . SuggestedLRs(valley=0.00013182566908653826) . learn_higher_epoch.fit_one_cycle(50, 0.000131) . epoch train_loss valid_loss accuracy time . 0 | 2.377973 | 2.386434 | 0.125860 | 00:11 | . 1 | 2.262577 | 2.187544 | 0.198981 | 00:11 | . 2 | 2.131813 | 2.076762 | 0.288153 | 00:11 | . 3 | 1.982456 | 1.810484 | 0.386497 | 00:11 | . 4 | 1.822840 | 2.068272 | 0.374777 | 00:11 | . 5 | 1.665031 | 1.880948 | 0.445096 | 00:11 | . 6 | 1.527851 | 1.564145 | 0.496306 | 00:11 | . 7 | 1.407204 | 1.601933 | 0.506242 | 00:11 | . 8 | 1.302459 | 1.382655 | 0.555414 | 00:11 | . 9 | 1.205924 | 1.412100 | 0.577580 | 00:11 | . 10 | 1.120911 | 1.306462 | 0.595669 | 00:11 | . 11 | 1.056995 | 1.286385 | 0.591338 | 00:11 | . 12 | 0.991353 | 1.091827 | 0.653503 | 00:11 | . 13 | 0.939019 | 0.991272 | 0.692229 | 00:11 | . 14 | 0.889291 | 1.482610 | 0.576560 | 00:11 | . 15 | 0.843552 | 1.134017 | 0.648662 | 00:12 | . 16 | 0.798295 | 1.556873 | 0.567898 | 00:11 | . 17 | 0.750999 | 1.271683 | 0.629299 | 00:11 | . 18 | 0.714964 | 1.453092 | 0.597452 | 00:12 | . 19 | 0.693228 | 1.105892 | 0.668025 | 00:11 | . 20 | 0.664791 | 1.161920 | 0.665733 | 00:11 | . 21 | 0.633090 | 1.229070 | 0.645605 | 00:11 | . 22 | 0.615386 | 0.935700 | 0.711847 | 00:11 | . 23 | 0.581805 | 1.142192 | 0.669554 | 00:11 | . 24 | 0.558872 | 1.138849 | 0.676688 | 00:12 | . 25 | 0.538580 | 0.860732 | 0.742166 | 00:12 | . 26 | 0.515516 | 0.977736 | 0.709554 | 00:12 | . 27 | 0.501689 | 1.064165 | 0.684076 | 00:12 | . 28 | 0.477749 | 0.935355 | 0.724586 | 00:12 | . 29 | 0.450755 | 0.950616 | 0.712611 | 00:12 | . 30 | 0.417243 | 1.084173 | 0.695032 | 00:11 | . 31 | 0.406682 | 1.021061 | 0.713885 | 00:11 | . 32 | 0.387076 | 1.004474 | 0.714140 | 00:11 | . 33 | 0.359906 | 0.797884 | 0.766115 | 00:11 | . 34 | 0.337195 | 0.814482 | 0.771465 | 00:11 | . 35 | 0.317810 | 0.904211 | 0.754395 | 00:11 | . 36 | 0.305618 | 0.925233 | 0.745732 | 00:11 | . 37 | 0.290424 | 0.813434 | 0.765096 | 00:11 | . 38 | 0.270168 | 0.856214 | 0.763312 | 00:12 | . 39 | 0.254928 | 0.915280 | 0.753631 | 00:11 | . 40 | 0.240487 | 0.881618 | 0.754395 | 00:11 | . 41 | 0.223493 | 0.808955 | 0.778344 | 00:11 | . 42 | 0.213208 | 0.911270 | 0.758726 | 00:12 | . 43 | 0.198452 | 0.811407 | 0.776306 | 00:11 | . 44 | 0.189459 | 0.895555 | 0.754904 | 00:11 | . 45 | 0.187922 | 0.868764 | 0.765350 | 00:11 | . 46 | 0.180785 | 0.811440 | 0.775541 | 00:11 | . 47 | 0.178707 | 0.826532 | 0.773503 | 00:11 | . 48 | 0.174185 | 0.817364 | 0.775796 | 00:11 | . 49 | 0.170915 | 0.805405 | 0.779873 | 00:11 | . Another test with bigger images. (IMAGENETTE_320) . path_320 = untar_data(URLs.IMAGENETTE_320) data_block_320=DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(320), batch_tfms=[*aug_transforms(min_scale=0.5, size=224), Normalize.from_stats(*imagenet_stats)], ) dls_320 = data_block_320.dataloaders(path_320, bs=256) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . rn_IM_320=ResneTTe34(10) learn__IM_320 = Learner(dls_320, rn_IM_320, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() . learn__IM_320.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . SuggestedLRs(valley=0.00010964782268274575) . learn__IM_320.fit_one_cycle(20, 0.0001096) . epoch train_loss valid_loss accuracy time . 0 | 2.245787 | 2.379868 | 0.198471 | 00:20 | . 1 | 1.967387 | 2.148099 | 0.340127 | 00:20 | . 2 | 1.657673 | 1.542151 | 0.509554 | 00:20 | . 3 | 1.432339 | 1.860907 | 0.455287 | 00:20 | . 4 | 1.241912 | 1.308743 | 0.576051 | 00:20 | . 5 | 1.107367 | 1.259022 | 0.604331 | 00:20 | . 6 | 1.004726 | 1.330731 | 0.598217 | 00:20 | . 7 | 0.916032 | 0.902091 | 0.714140 | 00:20 | . 8 | 0.846182 | 1.346130 | 0.601783 | 00:20 | . 9 | 0.766183 | 0.942265 | 0.716178 | 00:20 | . 10 | 0.710231 | 0.875285 | 0.710828 | 00:20 | . 11 | 0.655968 | 0.812030 | 0.745732 | 00:20 | . 12 | 0.600506 | 0.853020 | 0.732739 | 00:20 | . 13 | 0.553901 | 0.720496 | 0.776051 | 00:20 | . 14 | 0.515389 | 0.691851 | 0.782420 | 00:20 | . 15 | 0.475770 | 0.630577 | 0.800510 | 00:20 | . 16 | 0.432293 | 0.619373 | 0.805860 | 00:20 | . 17 | 0.403326 | 0.561097 | 0.823694 | 00:20 | . 18 | 0.388080 | 0.583485 | 0.814013 | 00:21 | . 19 | 0.378337 | 0.578927 | 0.812739 | 00:20 | . Resources: . Resnet paper by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1512.03385 , https://arxiv.org/pdf/1512.03385.pdf . W&amp;B Paper Reading Group: ResNets by Aman Arora . W&amp;B Fastbook Reading Group — 14. ResNet . Practical Deep Learning for Coders Book (fastbook) . https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb . Live Coding Session on ResNet by Aman Arora . [Classic] Deep Residual Learning for Image Recognition (Paper Explained) by Yannic Kilcher . Andrew Ng Resnet videos. . . .",
            "url": "https://niyazikemer.com/coding/2021/12/12/resnet-live.html",
            "relUrl": "/coding/2021/12/12/resnet-live.html",
            "date": " • Dec 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Chapter 12 - A Language Model from scratch",
            "content": ". https://msomuseum.com/en/north-american-p-51d-mustang-ferocious-frankie/ . [[chapter_nlp_dive]] A Language Model from Scratch . We&#39;re now ready to go deep... deep into deep learning! You already learned how to train a basic neural network, but how do you go from there to creating state-of-the-art models? In this part of the book we&#39;re going to uncover all of the mysteries, starting with language models. . You saw in &lt;&gt; how to fine-tune a pretrained language model to build a text classifier. In this chapter, we will explain to you what exactly is inside that model, and what an RNN is. First, let&#39;s gather some data that will allow us to quickly prototype our various models.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Data . Whenever we start working on a new problem, we always first try to think of the simplest dataset we can that will allow us to try out methods quickly and easily, and interpret the results. When we started working on language modeling a few years ago we didn&#39;t find any datasets that would allow for quick prototyping, so we made one. We call it Human Numbers, and it simply contains the first 10,000 numbers written out in English. . j:One of the most common practical mistakes I see even amongst highly experienced practitioners is failing to use appropriate datasets at appropriate times during the analysis process. In particular, most people tend to start with datasets that are too big and too complicated. . We can download, extract, and take a look at our dataset in the usual way: . from fastai.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) . path.ls() . (#2) [Path(&#39;train.txt&#39;),Path(&#39;valid.txt&#39;)] . Let&#39;s open those two files and see what&#39;s inside. At first we&#39;ll join all of the texts together and ignore the train/valid split given by the dataset (we&#39;ll come back to that later): . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . lines[-3:] . (#3) [&#39;nine thousand nine hundred ninety seven n&#39;,&#39;nine thousand nine hundred ninety eight n&#39;,&#39;nine thousand nine hundred ninety nine n&#39;] . We take all those lines and concatenate them in one big stream. To mark when we go from one number to the next, we use a . as a separator: . text = &#39; . &#39;.join([l.strip() for l in lines]) text[:100] . &#39;one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo&#39; . len(text) . 365478 . . Note: That means all words on the whole dataset (both training and validation) is in the text. and this dataset has 365478 words that creates 10000 h&#8217;uman numbers&#8217; . We can tokenize this dataset by splitting on spaces: . tokens = text.split(&#39; &#39;) tokens[:300] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;, &#39;six&#39;, &#39;.&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;, &#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;, &#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;, &#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;, &#39;sixteen&#39;, &#39;.&#39;, &#39;seventeen&#39;, &#39;.&#39;, &#39;eighteen&#39;, &#39;.&#39;, &#39;nineteen&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;one&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;two&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;three&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;four&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;five&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;six&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;twenty&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;one&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;two&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;three&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;four&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;five&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;six&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;thirty&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;one&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;two&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;three&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;four&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;five&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;six&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;forty&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;one&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;two&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;three&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;four&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;five&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;six&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;fifty&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;one&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;two&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;three&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;four&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;five&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;six&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;sixty&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;one&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;two&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;three&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;four&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;five&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;six&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;seventy&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;one&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;two&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;three&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;four&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;five&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;six&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;eighty&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;one&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;two&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;three&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;four&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;five&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;six&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;seven&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;eight&#39;, &#39;.&#39;, &#39;ninety&#39;, &#39;nine&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;one&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;two&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;three&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;four&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;five&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;six&#39;, &#39;.&#39;, &#39;one&#39;, &#39;hundred&#39;, &#39;seven&#39;] . To numericalize, we have to create a list of all the unique tokens (our vocab): . vocab = L(*tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . type(vocab) . fastcore.foundation.L . for i in vocab: print(i) . one . two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen seventeen eighteen nineteen twenty thirty forty fifty sixty seventy eighty ninety hundred thousand . Then we can convert our tokens into numbers by looking up the index of each in the vocab: . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . nums[1001:1011] . (#10) [2,28,23,7,1,2,28,23,8,1] . . Note: that means two hurdred fifty six and two hurdred fifty seven . word2idx . {&#39;one&#39;: 0, &#39;.&#39;: 1, &#39;two&#39;: 2, &#39;three&#39;: 3, &#39;four&#39;: 4, &#39;five&#39;: 5, &#39;six&#39;: 6, &#39;seven&#39;: 7, &#39;eight&#39;: 8, &#39;nine&#39;: 9, &#39;ten&#39;: 10, &#39;eleven&#39;: 11, &#39;twelve&#39;: 12, &#39;thirteen&#39;: 13, &#39;fourteen&#39;: 14, &#39;fifteen&#39;: 15, &#39;sixteen&#39;: 16, &#39;seventeen&#39;: 17, &#39;eighteen&#39;: 18, &#39;nineteen&#39;: 19, &#39;twenty&#39;: 20, &#39;thirty&#39;: 21, &#39;forty&#39;: 22, &#39;fifty&#39;: 23, &#39;sixty&#39;: 24, &#39;seventy&#39;: 25, &#39;eighty&#39;: 26, &#39;ninety&#39;: 27, &#39;hundred&#39;: 28, &#39;thousand&#39;: 29} . Now that we have a small dataset on which language modeling should be an easy task, we can build our first model. . Our First Language Model from Scratch . One simple way to turn this into a neural network would be to specify that we are going to predict each word based on the previous three words. We could create a list of every sequence of three words as our independent variables, and the next word after each sequence as the dependent variable. . We can do that with plain Python. Let&#39;s do it first with tokens just to confirm what it looks like: . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . Now we will do it with tensors of the numericalized values, which is what the model will actually use: . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . We can batch those easily using the DataLoader class. For now we will split the sequences randomly: . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . We can now create a neural network architecture that takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. We will use three standard linear layers, but with two tweaks. . The first tweak is that the first linear layer will use only the first word&#39;s embedding as activations, the second layer will use the second word&#39;s embedding plus the first layer&#39;s output activations, and the third layer will use the third word&#39;s embedding plus the second layer&#39;s output activations. The key effect of this is that every word is interpreted in the information context of any words preceding it. . The second tweak is that each of these three layers will use the same weight matrix. The way that one word impacts the activations from previous words should not change depending on the position of a word. In other words, activation values will change as data moves through the layers, but the layer weights themselves will not change from layer to layer. So, a layer does not learn one sequence position; it must learn to handle all positions. . Since layer weights do not change, you might think of the sequential layers as &quot;the same layer&quot; repeated. In fact, PyTorch makes this concrete; we can just create one layer, and use it multiple times. . Our Language Model in PyTorch . We can now create the language model module that we described earlier: . class LMModel1(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = F.relu(self.h_h(self.i_h(x[:,0]))) h = h + self.i_h(x[:,1]) h = F.relu(self.h_h(h)) h = h + self.i_h(x[:,2]) h = F.relu(self.h_h(h)) return self.h_o(h) . As you see, we have created three layers: . The embedding layer (i_h, for input to hidden) | The linear layer to create the activations for the next word (h_h, for hidden to hidden) | A final linear layer to predict the fourth word (h_o, for hidden to output) | . This might be easier to represent in pictorial form, so let&#39;s define a simple pictorial representation of basic neural networks. &lt;&gt; shows how we&#39;re going to represent a neural net with one hidden layer.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Pictorial representation of a simple neural network . Each shape represents activations: rectangle for input, circle for hidden (inner) layer activations, and triangle for output activations. We will use those shapes (summarized in &lt;&gt;) in all the diagrams in this chapter.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Shapes used in our pictorial representations . An arrow represents the actual layer computation—i.e., the linear layer followed by the activation function. Using this notation, &lt;&gt; shows what our simple language model looks like.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Representation of our basic language model . To simplify things, we&#39;ve removed the details of the layer computation from each arrow. We&#39;ve also color-coded the arrows, such that all arrows with the same color have the same weight matrix. For instance, all the input layers use the same embedding matrix, so they all have the same color (green). . Let&#39;s try training this model and see how it goes: . learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.824297 | 1.970941 | 0.467554 | 00:01 | . 1 | 1.386973 | 1.823243 | 0.467554 | 00:01 | . 2 | 1.417556 | 1.654498 | 0.494414 | 00:01 | . 3 | 1.376440 | 1.650849 | 0.494414 | 00:01 | . To see if this is any good, let&#39;s check what a very simple model would give us. In this case we could always predict the most common token, so let&#39;s find out which token is most often the target in our validation set: . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . The most common token has the index 29, which corresponds to the token thousand. Always predicting this token would give us an accuracy of roughly 15 %, so we are faring way better! . A:My first guess was that the separator would be the most common token, since there is one for every number. But looking at tokens reminded me that large numbers are written with many words, so on the way to 10,000 you write &quot;thousand&quot; a lot: five thousand, five thousand and one, five thousand and two, etc. Oops! Looking at your data is great for noticing subtle features and also embarrassingly obvious ones. . This is a nice first baseline. Let&#39;s see how we can refactor it with a loop. . Our First Recurrent Neural Network . Looking at the code for our module, we could simplify it by replacing the duplicated code that calls the layers with a for loop. As well as making our code simpler, this will also have the benefit that we will be able to apply our module equally well to token sequences of different lengths—we won&#39;t be restricted to token lists of length three: . class LMModel2(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = 0 for i in range(3): h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . Let&#39;s check that we get the same results using this refactoring: . learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.816274 | 1.964143 | 0.460185 | 00:02 | . 1 | 1.423805 | 1.739964 | 0.473259 | 00:01 | . 2 | 1.430327 | 1.685172 | 0.485382 | 00:01 | . 3 | 1.388390 | 1.657033 | 0.470406 | 00:01 | . We can also refactor our pictorial representation in exactly the same way, as shown in &lt;&gt; (we&#39;re also removing the details of activation sizes here, and using the same arrow colors as in &lt;&gt;).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Basic recurrent neural network . You will see that there is a set of activations that are being updated each time through the loop, stored in the variable h—this is called the hidden state. . Jargon:hidden state: The activations that are updated at each step of a recurrent neural network. . A neural network that is defined using a loop like this is called a recurrent neural network (RNN). It is important to realize that an RNN is not a complicated new architecture, but simply a refactoring of a multilayer neural network using a for loop. . A:My true opinion: if they were called &quot;looping neural networks,&quot; or LNNs, they would seem 50% less daunting! . Now that we know what an RNN is, let&#39;s try to make it a little bit better. . Improving the RNN . Looking at the code for our RNN, one thing that seems problematic is that we are initializing our hidden state to zero for every new input sequence. Why is that a problem? We made our sample sequences short so they would fit easily into batches. But if we order the samples correctly, those sample sequences will be read in order by the model, exposing the model to long stretches of the original sequence. . Another thing we can look at is having more signal: why only predict the fourth word when we could use the intermediate predictions to also predict the second and third words? . Let&#39;s see how we can implement those changes, starting with adding some state. . Maintaining the State of an RNN . Because we initialize the model&#39;s hidden state to zero for each new sample, we are throwing away all the information we have about the sentences we have seen so far, which means that our model doesn&#39;t actually know where we are up to in the overall counting sequence. This is easily fixed; we can simply move the initialization of the hidden state to __init__. . But this fix will create its own subtle, but important, problem. It effectively makes our neural network as deep as the entire number of tokens in our document. For instance, if there were 10,000 tokens in our dataset, we would be creating a 10,000-layer neural network. . To see why this is the case, consider the original pictorial representation of our recurrent neural network in &lt;&gt;, before refactoring it with a for loop. You can see each layer corresponds with one token input. When we talk about the representation of a recurrent neural network before refactoring with the for loop, we call this the unrolled representation. It is often helpful to consider the unrolled representation when trying to understand an RNN.&lt;/p&gt; The problem with a 10,000-layer neural network is that if and when you get to the 10,000th word of the dataset, you will still need to calculate the derivatives all the way back to the first layer. This is going to be very slow indeed, and very memory-intensive. It is unlikely that you&#39;ll be able to store even one mini-batch on your GPU. . The solution to this problem is to tell PyTorch that we do not want to back propagate the derivatives through the entire implicit neural network. Instead, we will just keep the last three layers of gradients. To remove all of the gradient history in PyTorch, we use the detach method. . Here is the new version of our RNN. It is now stateful, because it remembers its activations between different calls to forward, which represent its use for different samples in the batch: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class LMModel3(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): for i in range(3): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) out = self.h_o(self.h) self.h = self.h.detach() return out def reset(self): self.h = 0 . This model will have the same activations whatever sequence length we pick, because the hidden state will remember the last activation from the previous batch. The only thing that will be different is the gradients computed at each step: they will only be calculated on sequence length tokens in the past, instead of the whole stream. This approach is called backpropagation through time (BPTT) . Note: backpropagation through time (BPTT). Need to understand better this. . jargon:Back propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which &quot;detaches&quot; the history of computation steps in the hidden state every few time steps. . To use LMModel3, we need to make sure the samples are going to be seen in a certain order. As we saw in &lt;&gt;, if the first line of the first batch is our dset[0] then the second batch should have dset[1] as the first line, so that the model sees the text flowing.&lt;/p&gt; LMDataLoader was doing this for us in &lt;&gt;. This time we&#39;re going to do it ourselves.&lt;/p&gt; To do this, we are going to rearrange our dataset. First we divide the samples into m = len(dset) // bs groups (this is the equivalent of splitting the whole concatenated dataset into, for example, 64 equally sized pieces, since we&#39;re using bs=64 here). m is the length of each of these pieces. For instance, if we&#39;re using our whole dataset (although we&#39;ll actually split it into train versus valid in a moment), that will be: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; m = len(seqs)//bs m,bs,len(seqs) . (328, 64, 21031) . The first batch will be composed of the samples: . (0, m, 2*m, ..., (bs-1)*m) . the second batch of the samples: . (1, m+1, 2*m+1, ..., (bs-1)*m+1) . and so forth. This way, at each epoch, the model will see a chunk of contiguous text of size 3*m (since each text is of size 3) on each line of the batch. . The following function does that reindexing: . def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds . Then we just pass drop_last=True when building our DataLoaders to drop the last batch that does not have a shape of bs. We also pass shuffle=False to make sure the texts are read in order: . cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . The last thing we add is a little tweak of the training loop via a Callback. We will talk more about callbacks in &lt;&gt;; this one will call the reset method of our model at the beginning of each epoch and before each validation phase. Since we implemented that method to zero the hidden state of the model, this will make sure we start with a clean state before reading those continuous chunks of text. We can also start training a bit longer:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.677074 | 1.827367 | 0.467548 | 00:01 | . 1 | 1.282722 | 1.870913 | 0.388942 | 00:01 | . 2 | 1.090705 | 1.651793 | 0.462500 | 00:01 | . 3 | 1.012735 | 1.495139 | 0.547115 | 00:01 | . 4 | 0.986055 | 1.617414 | 0.538221 | 00:01 | . 5 | 0.955431 | 1.820802 | 0.513942 | 00:01 | . 6 | 0.919223 | 1.672606 | 0.546635 | 00:01 | . 7 | 0.868542 | 1.708052 | 0.570913 | 00:01 | . 8 | 0.834806 | 1.736881 | 0.584135 | 00:01 | . 9 | 0.819670 | 1.720637 | 0.583894 | 00:01 | . This is already better! The next step is to use more targets and compare them to the intermediate predictions. . Creating More Signal . Another problem with our current approach is that we only predict one output word for each three input words. That means that the amount of signal that we are feeding back to update weights with is not as large as it could be. It would be better if we predicted the next word after every single word, rather than every three words, as shown in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; RNN predicting after every token . This is easy enough to add. We need to first change our data so that the dependent variable has each of the three next words after each of our three input words. Instead of 3, we use an attribute, sl (for sequence length), and make it a bit bigger: . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . Looking at the first element of seqs, we can see that it contains two lists of the same size. The second list is the same as the first, but offset by one element: . [L(vocab[o] for o in s) for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Now we need to modify our model so that it outputs a prediction after every word, rather than just at the end of a three-word sequence: . class LMModel4(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): outs = [] for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(outs, dim=1) def reset(self): self.h = 0 . This model will return outputs of shape bs x sl x vocab_sz (since we stacked on dim=1). Our targets are of shape bs x sl, so we need to flatten those before using them in F.cross_entropy: . def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . We can now use this loss function to train the model: . learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.285931 | 3.072032 | 0.212565 | 00:01 | . 1 | 2.330371 | 1.969522 | 0.425781 | 00:01 | . 2 | 1.742317 | 1.841378 | 0.441488 | 00:00 | . 3 | 1.470120 | 1.810857 | 0.494303 | 00:00 | . 4 | 1.298154 | 1.866850 | 0.479248 | 00:01 | . 5 | 1.176869 | 1.732026 | 0.536458 | 00:01 | . 6 | 1.072540 | 1.708713 | 0.555339 | 00:01 | . 7 | 0.971644 | 1.731369 | 0.563883 | 00:00 | . 8 | 0.899695 | 1.682442 | 0.581055 | 00:01 | . 9 | 0.833159 | 1.647265 | 0.575602 | 00:01 | . 10 | 0.785973 | 1.661793 | 0.592692 | 00:01 | . 11 | 0.742612 | 1.697425 | 0.601481 | 00:01 | . 12 | 0.713223 | 1.802843 | 0.583903 | 00:01 | . 13 | 0.695678 | 1.791676 | 0.588623 | 00:01 | . 14 | 0.681255 | 1.746720 | 0.599040 | 00:01 | . We need to train for longer, since the task has changed a bit and is more complicated now. But we end up with a good result... At least, sometimes. If you run it a few times, you&#39;ll see that you can get quite different results on different runs. That&#39;s because effectively we have a very deep network here, which can result in very large or very small gradients. We&#39;ll see in the next part of this chapter how to deal with this. . Now, the obvious way to get a better model is to go deeper: we only have one linear layer between the hidden state and the output activations in our basic RNN, so maybe we&#39;ll get better results with more. . Multilayer RNNs . In a multilayer RNN, we pass the activations from our recurrent neural network into a second recurrent neural network, like in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 2-layer RNN . The unrolled representation is shown in &lt;&gt; (similar to &lt;&gt;).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Two-layer unrolled RNN . Let&#39;s see how to implement this in practice. . The Model . We can save some time by using PyTorch&#39;s RNN class, which implements exactly what we created earlier, but also gives us the option to stack multiple RNNs, as we have discussed: . class LMModel5(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(n_layers, bs, n_hidden) def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = h.detach() return self.h_o(res) def reset(self): self.h.zero_() . learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.041790 | 2.548715 | 0.455811 | 00:01 | . 1 | 2.128514 | 1.708763 | 0.471029 | 00:01 | . 2 | 1.699163 | 1.866050 | 0.340576 | 00:01 | . 3 | 1.499681 | 1.738478 | 0.471517 | 00:00 | . 4 | 1.339090 | 1.729539 | 0.494792 | 00:01 | . 5 | 1.206317 | 1.835855 | 0.502848 | 00:01 | . 6 | 1.088242 | 1.845555 | 0.520101 | 00:01 | . 7 | 0.982788 | 1.856255 | 0.522624 | 00:01 | . 8 | 0.890793 | 1.940333 | 0.525716 | 00:01 | . 9 | 0.809587 | 2.028805 | 0.529785 | 00:01 | . 10 | 0.743085 | 2.074603 | 0.535075 | 00:01 | . 11 | 0.694128 | 2.153413 | 0.540039 | 00:01 | . 12 | 0.660761 | 2.137610 | 0.547689 | 00:00 | . 13 | 0.640680 | 2.169351 | 0.547363 | 00:01 | . 14 | 0.630333 | 2.168201 | 0.548828 | 00:01 | . Now that&#39;s disappointing... our previous single-layer RNN performed better. Why? The reason is that we have a deeper model, leading to exploding or vanishing activations. . Exploding or Disappearing Activations . In practice, creating accurate models from this kind of RNN is difficult. We will get better results if we call detach less often, and have more layers—this gives our RNN a longer time horizon to learn from, and richer features to create. But it also means we have a deeper model to train. The key challenge in the development of deep learning has been figuring out how to train these kinds of models. . The reason this is challenging is because of what happens when you multiply by a matrix many times. Think about what happens when you multiply by a number many times. For example, if you multiply by 2, starting at 1, you get the sequence 1, 2, 4, 8,... after 32 steps you are already at 4,294,967,296. A similar issue happens if you multiply by 0.5: you get 0.5, 0.25, 0.125… and after 32 steps it&#39;s 0.00000000023. As you can see, multiplying by a number even slightly higher or lower than 1 results in an explosion or disappearance of our starting number, after just a few repeated multiplications. . Because matrix multiplication is just multiplying numbers and adding them up, exactly the same thing happens with repeated matrix multiplications. And that&#39;s all a deep neural network is —each extra layer is another matrix multiplication. This means that it is very easy for a deep neural network to end up with extremely large or extremely small numbers. . This is a problem, because the way computers store numbers (known as &quot;floating point&quot;) means that they become less and less accurate the further away the numbers get from zero. The diagram in &lt;&gt;, from the excellent article &quot;What You Never Wanted to Know About Floating Point but Will Be Forced to Find Out&quot;, shows how the precision of floating-point numbers varies over the number line.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Precision of floating-point numbers . This inaccuracy means that often the gradients calculated for updating the weights end up as zero or infinity for deep networks. This is commonly referred to as the vanishing gradients or exploding gradients problem. It means that in SGD, the weights are either not updated at all or jump to infinity. Either way, they won&#39;t improve with training. . Researchers have developed a number of ways to tackle this problem, which we will be discussing later in the book. One option is to change the definition of a layer in a way that makes it less likely to have exploding activations. We&#39;ll look at the details of how this is done in &lt;&gt;, when we discuss batch normalization, and &lt;&gt;, when we discuss ResNets, although these details don&#39;t generally matter in practice (unless you are a researcher that is creating new approaches to solving this problem). Another strategy for dealing with this is by being careful about initialization, which is a topic we&#39;ll investigate in &lt;&gt;.&lt;/p&gt; For RNNs, there are two types of layers that are frequently used to avoid exploding activations: gated recurrent units (GRUs) and long short-term memory (LSTM) layers. Both of these are available in PyTorch, and are drop-in replacements for the RNN layer. We will only cover LSTMs in this book; there are plenty of good tutorials online explaining GRUs, which are a minor variant on the LSTM design. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Note: you can see exploding and vanishing activations with the help of class ActivationStats its in the video too. . LSTM . LSTM is an architecture that was introduced back in 1997 by Jürgen Schmidhuber and Sepp Hochreiter. In this architecture, there are not one but two hidden states. In our base RNN, the hidden state is the output of the RNN at the previous time step. That hidden state is then responsible for two things: . Having the right information for the output layer to predict the correct next token | Retaining memory of everything that happened in the sentence | . Consider, for example, the sentences &quot;Henry has a dog and he likes his dog very much&quot; and &quot;Sophie has a dog and she likes her dog very much.&quot; It&#39;s very clear that the RNN needs to remember the name at the beginning of the sentence to be able to predict he/she or his/her. . In practice, RNNs are really bad at retaining memory of what happened much earlier in the sentence, which is the motivation to have another hidden state (called cell state) in the LSTM. The cell state will be responsible for keeping long short-term memory, while the hidden state will focus on the next token to predict. Let&#39;s take a closer look at how this is achieved and build an LSTM from scratch. . Building an LSTM from Scratch . In order to build an LSTM, we first have to understand its architecture. &lt;&gt; shows its inner structure.&lt;/p&gt; Architecture of an LSTM . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; In this picture, our input $x_{t}$ enters on the left with the previous hidden state ($h_{t-1}$) and cell state ($c_{t-1}$). The four orange boxes represent four layers (our neural nets) with the activation being either sigmoid ($ sigma$) or tanh. tanh is just a sigmoid function rescaled to the range -1 to 1. Its mathematical expression can be written like this: . $$ tanh(x) = frac{e^{x} - e^{-x}}{e^{x}+e^{-x}} = 2 sigma(2x) - 1$$ . where $ sigma$ is the sigmoid function. The green circles are elementwise operations. What goes out on the right is the new hidden state ($h_{t}$) and new cell state ($c_{t}$), ready for our next input. The new hidden state is also used as output, which is why the arrow splits to go up. . Let&#39;s go over the four neural nets (called gates) one by one and explain the diagram—but before this, notice how very little the cell state (at the top) is changed. It doesn&#39;t even go directly through a neural net! This is exactly why it will carry on a longer-term state. . First, the arrows for input and old hidden state are joined together. In the RNN we wrote earlier in this chapter, we were adding them together. In the LSTM, we stack them in one big tensor. This means the dimension of our embeddings (which is the dimension of $x_{t}$) can be different than the dimension of our hidden state. If we call those n_in and n_hid, the arrow at the bottom is of size n_in + n_hid; thus all the neural nets (orange boxes) are linear layers with n_in + n_hid inputs and n_hid outputs. . The first gate (looking from left to right) is called the forget gate. Since it’s a linear layer followed by a sigmoid, its output will consist of scalars between 0 and 1. We multiply this result by the cell state to determine which information to keep and which to throw away: values closer to 0 are discarded and values closer to 1 are kept. This gives the LSTM the ability to forget things about its long-term state. For instance, when crossing a period or an xxbos token, we would expect to it to (have learned to) reset its cell state. . The second gate is called the input gate. It works with the third gate (which doesn&#39;t really have a name but is sometimes called the cell gate) to update the cell state. For instance, we may see a new gender pronoun, in which case we&#39;ll need to replace the information about gender that the forget gate removed. Similar to the forget gate, the input gate decides which elements of the cell state to update (values close to 1) or not (values close to 0). The third gate determines what those updated values are, in the range of –1 to 1 (thanks to the tanh function). The result is then added to the cell state. . The last gate is the output gate. It determines which information from the cell state to use to generate the output. The cell state goes through a tanh before being combined with the sigmoid output from the output gate, and the result is the new hidden state. . In terms of code, we can write the same steps like this: . . Note: Never ever seen any better explanation to RNN LSTM. Great. Need to come back later. . class LSTMCell(Module): def __init__(self, ni, nh): self.forget_gate = nn.Linear(ni + nh, nh) self.input_gate = nn.Linear(ni + nh, nh) self.cell_gate = nn.Linear(ni + nh, nh) self.output_gate = nn.Linear(ni + nh, nh) def forward(self, input, state): h,c = state h = torch.cat([h, input], dim=1) forget = torch.sigmoid(self.forget_gate(h)) c = c * forget inp = torch.sigmoid(self.input_gate(h)) cell = torch.tanh(self.cell_gate(h)) c = c + inp * cell out = torch.sigmoid(self.output_gate(h)) h = out * torch.tanh(c) return h, (h,c) . In practice, we can then refactor the code. Also, in terms of performance, it&#39;s better to do one big matrix multiplication than four smaller ones (that&#39;s because we only launch the special fast kernel on the GPU once, and it gives the GPU more work to do in parallel). The stacking takes a bit of time (since we have to move one of the tensors around on the GPU to have it all in a contiguous array), so we use two separate layers for the input and the hidden state. The optimized and refactored code then looks like this: . class LSTMCell(Module): def __init__(self, ni, nh): self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state # One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . Here we use the PyTorch chunk method to split our tensor into four pieces. It works like this: . t = torch.arange(0,10); t . tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . t.chunk(2) . (tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9])) . Let&#39;s now use this architecture to train a language model! . Training a Language Model Using LSTMs . Here is the same network as LMModel5, using a two-layer LSTM. We can train it at a higher learning rate, for a shorter time, and get better accuracy: . class LMModel6(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = [h_.detach() for h_ in h] return self.h_o(res) def reset(self): for h in self.h: h.zero_() . learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.026114 | 2.772101 | 0.153076 | 00:01 | . 1 | 2.216185 | 2.089064 | 0.269124 | 00:01 | . 2 | 1.613938 | 1.826188 | 0.478760 | 00:01 | . 3 | 1.315927 | 2.045684 | 0.507406 | 00:01 | . 4 | 1.086288 | 2.025785 | 0.597005 | 00:01 | . 5 | 0.859458 | 2.025288 | 0.665853 | 00:01 | . 6 | 0.641744 | 1.841781 | 0.675456 | 00:01 | . 7 | 0.433617 | 1.679927 | 0.699951 | 00:01 | . 8 | 0.278683 | 1.551113 | 0.744303 | 00:01 | . 9 | 0.171600 | 1.599915 | 0.722819 | 00:01 | . 10 | 0.106909 | 1.481836 | 0.767497 | 00:01 | . 11 | 0.070766 | 1.453160 | 0.774577 | 00:01 | . 12 | 0.050269 | 1.427950 | 0.778564 | 00:01 | . 13 | 0.039361 | 1.437351 | 0.781657 | 00:01 | . 14 | 0.034361 | 1.442622 | 0.778971 | 00:01 | . Now that&#39;s better than a multilayer RNN! We can still see there is a bit of overfitting, however, which is a sign that a bit of regularization might help. . Regularizing an LSTM . Recurrent neural networks, in general, are hard to train, because of the problem of vanishing activations and gradients we saw before. Using LSTM (or GRU) cells makes training easier than with vanilla RNNs, but they are still very prone to overfitting. Data augmentation, while a possibility, is less often used for text data than for images because in most cases it requires another model to generate random augmentations (e.g., by translating the text into another language and then back into the original language). Overall, data augmentation for text data is currently not a well-explored space. . However, there are other regularization techniques we can use instead to reduce overfitting, which were thoroughly studied for use with LSTMs in the paper &quot;Regularizing and Optimizing LSTM Language Models&quot; by Stephen Merity, Nitish Shirish Keskar, and Richard Socher. This paper showed how effective use of dropout, activation regularization, and temporal activation regularization could allow an LSTM to beat state-of-the-art results that previously required much more complicated models. The authors called an LSTM using these techniques an AWD-LSTM. We&#39;ll look at each of these techniques in turn. . Dropout . Dropout is a regularization technique that was introduced by Geoffrey Hinton et al. in Improving neural networks by preventing co-adaptation of feature detectors. The basic idea is to randomly change some activations to zero at training time. This makes sure all neurons actively work toward the output, as seen in &lt;&gt; (from &quot;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&quot; by Nitish Srivastava et al.).&lt;/p&gt; Applying dropout in a neural network (courtesy of Nitish Srivastava et al.) . Hinton used a nice metaphor when he explained, in an interview, the inspiration for dropout: . :I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting. In the same interview, he also explained that neuroscience provided additional inspiration: :We don&#39;t really know why neurons spike. One theory is that they want to be noisy so as to regularize, because we have many more parameters than we have data points. The idea of dropout is that if you have noisy activations, you can afford to use a much bigger model. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; This explains the idea behind why dropout helps to generalize: first it helps the neurons to cooperate better together, then it makes the activations more noisy, thus making the model more robust. . We can see, however, that if we were to just zero those activations without doing anything else, our model would have problems training: if we go from the sum of five activations (that are all positive numbers since we apply a ReLU) to just two, this won&#39;t have the same scale. Therefore, if we apply dropout with a probability p, we rescale all activations by dividing them by 1-p (on average p will be zeroed, so it leaves 1-p), as shown in &lt;&gt;.&lt;/p&gt; Why scale the activations when applying dropout (courtesy of Nitish Srivastava et al.) . This is a full implementation of the dropout layer in PyTorch (although PyTorch&#39;s native layer is actually written in C, not Python): . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class Dropout(Module): def __init__(self, p): self.p = p def forward(self, x): if not self.training: return x mask = x.new(*x.shape).bernoulli_(1-p) return x * mask.div_(1-p) . The bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p), which is then multiplied with our input before dividing by 1-p. Note the use of the training attribute, which is available in any PyTorch nn.Module, and tells us if we are doing training or inference. . Note: Do Your Own Experiments: In previous chapters of the book we&#8217;d be adding a code example for bernoulli_ here, so you can see exactly how it works. But now that you know enough to do this yourself, we&#8217;re going to be doing fewer and fewer examples for you, and instead expecting you to do your own experiments to see how things work. In this case, you&#8217;ll see in the end-of-chapter questionnaire that we&#8217;re asking you to experiment with bernoulli_—but don&#8217;t wait for us to ask you to experiment to develop your understanding of the code we&#8217;re studying; go ahead and do it anyway! Using dropout before passing the output of our LSTM to the final layer will help reduce overfitting. Dropout is also used in many other models, including the default CNN head used in fastai.vision, and is available in fastai.tabular by passing the ps parameter (where each &quot;p&quot; is passed to each added Dropout layer), as we&#39;ll see in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Dropout has different behavior in training and validation mode, which we specified using the training attribute in Dropout. Calling the train method on a Module sets training to True (both for the module you call the method on and for every module it recursively contains), and eval sets it to False. This is done automatically when calling the methods of Learner, but if you are not using that class, remember to switch from one to the other as needed. . Activation Regularization and Temporal Activation Regularization . Activation regularization (AR) and temporal activation regularization (TAR) are two regularization methods very similar to weight decay, discussed in &lt;&gt;. When applying weight decay, we add a small penalty to the loss that aims at making the weights as small as possible. For activation regularization, it&#39;s the final activations produced by the LSTM that we will try to make as small as possible, instead of the weights.&lt;/p&gt; To regularize the final activations, we have to store those somewhere, then add the means of the squares of them to the loss (along with a multiplier alpha, which is just like wd for weight decay): . loss += alpha * activations.pow(2).mean() . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Temporal activation regularization is linked to the fact we are predicting tokens in a sentence. That means it&#39;s likely that the outputs of our LSTMs should somewhat make sense when we read them in order. TAR is there to encourage that behavior by adding a penalty to the loss to make the difference between two consecutive activations as small as possible: our activations tensor has a shape bs x sl x n_hid, and we read consecutive activations on the sequence length axis (the dimension in the middle). With this, TAR can be expressed as: . loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean() . alpha and beta are then two hyperparameters to tune. To make this work, we need our model with dropout to return three things: the proper output, the activations of the LSTM pre-dropout, and the activations of the LSTM post-dropout. AR is often applied on the dropped-out activations (to not penalize the activations we turned into zeros afterward) while TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps). There is then a callback called RNNRegularizer that will apply this regularization for us. . Training a Weight-Tied Regularized LSTM . We can combine dropout (applied before we go into our output layer) with AR and TAR to train our previous LSTM. We just need to return three things instead of one: the normal output of our LSTM, the dropped-out activations, and the activations from our LSTMs. The last two will be picked up by the callback RNNRegularization for the contributions it has to make to the loss. . Another useful trick we can add from the AWD LSTM paper is weight tying. In a language model, the input embeddings represent a mapping from English words to activations, and the output hidden layer represents a mapping from activations to English words. We might expect, intuitively, that these mappings could be the same. We can represent this in PyTorch by assigning the same weight matrix to each of these layers: . self.h_o.weight = self.i_h.weight . In LMModel7, we include these final tweaks: . class LMModel7(Module): def __init__(self, vocab_sz, n_hidden, n_layers, p): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.drop = nn.Dropout(p) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h_o.weight = self.i_h.weight self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): raw,h = self.rnn(self.i_h(x), self.h) out = self.drop(raw) self.h = [h_.detach() for h_ in h] return self.h_o(out),raw,out def reset(self): for h in self.h: h.zero_() . We can create a regularized Learner using the RNNRegularizer callback: . learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)]) . A TextLearner automatically adds those two callbacks for us (with those values for alpha and beta as defaults), so we can simplify the preceding line to: . learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . We can then train the model, and add additional regularization by increasing the weight decay to 0.1: . learn.fit_one_cycle(15, 1e-2, wd=0.1) . epoch train_loss valid_loss accuracy time . 0 | 2.461772 | 1.946499 | 0.509277 | 00:01 | . 1 | 1.516597 | 1.259042 | 0.637370 | 00:01 | . 2 | 0.801908 | 0.857417 | 0.779460 | 00:01 | . 3 | 0.403070 | 0.794164 | 0.790853 | 00:01 | . 4 | 0.210351 | 0.758830 | 0.823242 | 00:01 | . 5 | 0.115222 | 0.802138 | 0.827230 | 00:01 | . 6 | 0.072353 | 0.746733 | 0.830241 | 00:01 | . 7 | 0.049468 | 0.778808 | 0.839437 | 00:01 | . 8 | 0.034712 | 0.674311 | 0.852458 | 00:01 | . 9 | 0.028301 | 0.615052 | 0.867757 | 00:01 | . 10 | 0.024246 | 0.785523 | 0.831380 | 00:01 | . 11 | 0.020176 | 0.733221 | 0.841716 | 00:01 | . 12 | 0.016886 | 0.825557 | 0.830811 | 00:01 | . 13 | 0.014274 | 0.792114 | 0.836670 | 00:01 | . 14 | 0.012803 | 0.816476 | 0.833822 | 00:01 | . Now this is far better than our previous model! . Conclusion . You have now seen everything that is inside the AWD-LSTM architecture we used in text classification in &lt;&gt;. It uses dropout in a lot more places:&lt;/p&gt; Embedding dropout (inside the embedding layer, drops some random lines of embeddings) | Input dropout (applied after the embedding layer) | Weight dropout (applied to the weights of the LSTM at each training step) | Hidden dropout (applied to the hidden state between two layers) | . This makes it even more regularized. Since fine-tuning those five dropout values (including the dropout before the output layer) is complicated, we have determined good defaults and allow the magnitude of dropout to be tuned overall with the drop_mult parameter you saw in that chapter (which is multiplied by each dropout). . Another architecture that is very powerful, especially in &quot;sequence-to-sequence&quot; problems (that is, problems where the dependent variable is itself a variable-length sequence, such as language translation), is the Transformers architecture. You can find it in a bonus chapter on the book&#39;s website. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Questionnaire . If the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do? | Why do we concatenate the documents in our dataset before creating a language model? | To use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make to our model? | How can we share a weight matrix across multiple layers in PyTorch? | Write a module that predicts the third word given the previous two words of a sentence, without peeking. | What is a recurrent neural network? | What is &quot;hidden state&quot;? | What is the equivalent of hidden state in LMModel1? | To maintain the state in an RNN, why is it important to pass the text to the model in order? | What is an &quot;unrolled&quot; representation of an RNN? | Why can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem? | What is &quot;BPTT&quot;? | Write code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches of IMDb data in &lt;&gt;.&lt;/li&gt; What does the ModelResetter callback do? Why do we need it? | What are the downsides of predicting just one output word for each three input words? | Why do we need a custom loss function for LMModel4? | Why is the training of LMModel4 unstable? | In the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results? | Draw a representation of a stacked (multilayer) RNN. | Why should we get better results in an RNN if we call detach less often? Why might this not happen in practice with a simple RNN? | Why can a deep network result in very large or very small activations? Why does this matter? | In a computer&#39;s floating-point representation of numbers, which numbers are the most precise? | Why do vanishing gradients prevent training? | Why does it help to have two hidden states in the LSTM architecture? What is the purpose of each one? | What are these two states called in an LSTM? | What is tanh, and how is it related to sigmoid? | What is the purpose of this code in LSTMCell: h = torch.cat([h, input], dim=1) | What does chunk do in PyTorch? | Study the refactored version of LSTMCell carefully to ensure you understand how and why it does the same thing as the non-refactored version. | Why can we use a higher learning rate for LMModel6? | What are the three regularization techniques used in an AWD-LSTM model? | What is &quot;dropout&quot;? | Why do we scale the acitvations with dropout? Is this applied during training, inference, or both? | What is the purpose of this line from Dropout: if not self.training: return x | Experiment with bernoulli_ to understand how it works. | How do you set your model in training mode in PyTorch? In evaluation mode? | Write the equation for activation regularization (in math or code, as you prefer). How is it different from weight decay? | Write the equation for temporal activation regularization (in math or code, as you prefer). Why wouldn&#39;t we use this for computer vision problems? | What is &quot;weight tying&quot; in a language model? | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further Research . In LMModel2, why can forward start with h=0? Why don&#39;t we need to say h=torch.zeros(...)? | Write the code for an LSTM from scratch (you may refer to &lt;&gt;).&lt;/li&gt; Search the internet for the GRU architecture and implement it from scratch, and try training a model. See if you can get results similar to those we saw in this chapter. Compare your results to the results of PyTorch&#39;s built in GRU module. | Take a look at the source code for AWD-LSTM in fastai, and try to map each of the lines of code to the concepts shown in this chapter. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | | . . . .",
            "url": "https://niyazikemer.com/fastbook/2021/11/03/chapter-12.html",
            "relUrl": "/fastbook/2021/11/03/chapter-12.html",
            "date": " • Nov 3, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Chapter 11 - Data Munging with FastAi Mid-layer API",
            "content": ". [[chapter_midlevel_data]] Data Munging with fastai&#39;s Mid-Level API . We have seen what Tokenizer and Numericalize do to a collection of texts, and how they&#39;re used inside the data block API, which handles those transforms for us directly using the TextBlock. But what if we want to only apply one of those transforms, either to see intermediate results or because we have already tokenized texts? More generally, what can we do when the data block API is not flexible enough to accommodate our particular use case? For this, we need to use fastai&#39;s mid-level API for processing data. The data block API is built on top of that layer, so it will allow you to do everything the data block API does, and much much more. . Going Deeper into fastai&#39;s Layered API . The fastai library is built on a layered API. In the very top layer there are applications that allow us to train a model in five lines of codes, as we saw in &lt;&gt;. In the case of creating DataLoaders for a text classifier, for instance, we used the line:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) . The factory method TextDataLoaders.from_folder is very convenient when your data is arranged the exact same way as the IMDb dataset, but in practice, that often won&#39;t be the case. The data block API offers more flexibility. As we saw in the last chapter, we can get the same result with: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . But it&#39;s sometimes not flexible enough. For debugging purposes, for instance, we might need to apply just parts of the transforms that come with this data block. Or we might want to create a DataLoaders for some application that isn&#39;t directly supported by fastai. In this section, we&#39;ll dig into the pieces that are used inside fastai to implement the data block API. Understanding these will enable you to leverage the power and flexibility of this mid-tier API. . . Note: Mid-Level API: The mid-level API does not only contain functionality for creating DataLoaders. It also has the callback system, which allows us to customize the training loop any way we like, and the general optimizer. Both will be covered in &lt;&gt;. &lt;/div&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Transforms . When we studied tokenization and numericalization in the last chapter, we started by grabbing a bunch of texts: . files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) txts = L(o.open().read() for o in files[:2000]) . We then showed how to tokenize them with a Tokenizer: . tok = Tokenizer.from_folder(path) tok.setup(txts) toks = txts.map(tok) toks[0] . (#206) [&#39;xxbos&#39;,&#39;i&#39;,&#39;had&#39;,&#39;never&#39;,&#39;heard&#39;,&#39;of&#39;,&#39;this&#39;,&#39;film&#39;,&#39;until&#39;,&#39;i&#39;...] . and how to numericalize, including automatically creating the vocab for our corpus: . num = Numericalize() num.setup(toks) nums = toks.map(num) nums[0][:10] . TensorText([ 2, 19, 87, 137, 617, 14, 21, 30, 355, 19]) . The classes also have a decode method. For instance, Numericalize.decode gives us back the string tokens: . nums_dec = num.decode(nums[0][:10]); nums_dec . (#10) [&#39;xxbos&#39;,&#39;i&#39;,&#39;had&#39;,&#39;never&#39;,&#39;heard&#39;,&#39;of&#39;,&#39;this&#39;,&#39;film&#39;,&#39;until&#39;,&#39;i&#39;] . and Tokenizer.decode turns this back into a single string (it may not, however, be exactly the same as the original string; this depends on whether the tokenizer is reversible, which the default word tokenizer is not at the time we&#39;re writing this book): . tok.decode(nums_dec) . &#39;xxbos i had never heard of this film until i&#39; . decode is used by fastai&#39;s show_batch and show_results, as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation. . For each of tok or num in the preceding example, we created an object, called the setup method (which trains the tokenizer if needed for tok and creates the vocab for num), applied it to our raw texts (by calling the object as a function), and then finally decoded the result back to an understandable representation. These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them. This is the Transform class. Both Tokenize and Numericalize are Transforms. . In general, a Transform is an object that behaves like a function and has an optional setup method that will initialize some inner state (like the vocab inside num) and an optional decode that will reverse the function (this reversal may not be perfect, as we saw with tok). . A good example of decode is found in the Normalize transform that we saw in &lt;&gt;: to be able to plot the images its decode method undoes the normalization (i.e., it multiplies by the standard deviation and adds back the mean). On the other hand, data augmentation transforms do not have a decode method, since we want to show the effects on images to make sure the data augmentation is working as we want.&lt;/p&gt; A special behavior of Transforms is that they always get applied over tuples. In general, our data is always a tuple (input,target) (sometimes with more than one input or more than one target). When applying a transform on an item like this, such as Resize, we don&#39;t want to resize the tuple as a whole; instead, we want to resize the input (if applicable) and the target (if applicable) separately. It&#39;s the same for batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target. . We can see this behavior if we pass a tuple of texts to tok: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; tok((txts[0], txts[1])) . ((#206) [&#39;xxbos&#39;,&#39;i&#39;,&#39;had&#39;,&#39;never&#39;,&#39;heard&#39;,&#39;of&#39;,&#39;this&#39;,&#39;film&#39;,&#39;until&#39;,&#39;i&#39;...], (#498) [&#39;xxbos&#39;,&#39;&#34;&#39;,&#39;ardh&#39;,&#39;xxmaj&#39;,&#39;satya&#39;,&#39;&#34;&#39;,&#39;is&#39;,&#39;one&#39;,&#39;of&#39;,&#39;the&#39;...]) . Writing Your Own Transform . If you want to write a custom transform to apply to your data, the easiest way is to write a function. As you can see in this example, a Transform will only be applied to a matching type, if a type is provided (otherwise it will always be applied). In the following code, the :int in the function signature means that f only gets applied to ints. That&#39;s why tfm(2.0) returns 2.0, but tfm(2) returns 3 here: . def f(x:int): return x+1 tfm = Transform(f) tfm(2),tfm(2.0) . (3, 2.0) . . Note: I didn&#8217;t know that is possible. x:int usage is very surprising. Is it python or FastAI? . def f(x:int): return x+1 f(2),f(2.0) . (3, 3.0) . . Note: Oh it is FastAI. . doc(Transform) . Here, f is converted to a Transform with no setup and no decode method. . Python has a special syntax for passing a function (like f) to another function (or something that behaves like a function, known as a callable in Python), called a decorator. A decorator is used by prepending a callable with @ and placing it before a function definition (there are lots of good online tutorials about Python decorators, so take a look at one if this is a new concept for you). The following is identical to the previous code: . @Transform def f(x:int): return x+1 f(2),f(2.0) . (3, 2.0) . If you need either setup or decode, you will need to subclass Transform to implement the actual encoding behavior in encodes, then (optionally), the setup behavior in setups and the decoding behavior in decodes: . class NormalizeMean(Transform): def setups(self, items): self.mean = sum(items)/len(items) def encodes(self, x): return x-self.mean def decodes(self, x): return x+self.mean . Here, NormalizeMean will initialize some state during the setup (the mean of all elements passed), then the transformation is to subtract that mean. For decoding purposes, we implement the reverse of that transformation by adding the mean. Here is an example of NormalizeMean in action: . tfm = NormalizeMean() tfm.setup([1,2,3,4,5]) start = 2 y = tfm(start) z = tfm.decode(y) tfm.mean,y,z . (3.0, -1.0, 2.0) . Note that the method called and the method implemented are different, for each of these methods: . asciidoc [options=&quot;header&quot;] |====== | Class | To call | To implement | `nn.Module` (PyTorch) | `()` (i.e., call as function) | `forward` | `Transform` | `()` | `encodes` | `Transform` | `decode()` | `decodes` | `Transform` | `setup()` | `setups` |====== . So, for instance, you would never call setups directly, but instead would call setup. The reason for this is that setup does some work before and after calling setups for you. To learn more about Transforms and how you can use them to implement different behavior depending on the type of the input, be sure to check the tutorials in the fastai docs. . Pipeline . To compose several transforms together, fastai provides the Pipeline class. We define a Pipeline by passing it a list of Transforms; it will then compose the transforms inside it. When you call Pipeline on an object, it will automatically call the transforms inside, in order: . tfms = Pipeline([tok, num]) t = tfms(txts[0]); t[:20] . TensorText([ 2, 19, 87, 137, 617, 14, 21, 30, 355, 19, 356, 678, 18, 44, 1736, 68, 0, 7, 1292, 11]) . And you can call decode on the result of your encoding, to get back something you can display and analyze: . tfms.decode(t)[:100] . &#39;xxbos i had never heard of this film until i came across it by accident when xxunk xxup imdb . i saw&#39; . The only part that doesn&#39;t work the same way as in Transform is the setup. To properly set up a Pipeline of Transforms on some data, you need to use a TfmdLists. . TfmdLists and Datasets: Transformed Collections . Your data is usually a set of raw items (like filenames, or rows in a DataFrame) to which you want to apply a succession of transformations. We just saw that a succession of transformations is represented by a Pipeline in fastai. The class that groups together this Pipeline with your raw items is called TfmdLists. . TfmdLists . Here is the short way of doing the transformation we saw in the previous section: . tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize]) . At initialization, the TfmdLists will automatically call the setup method of each Transform in order, providing them not with the raw items but the items transformed by all the previous Transforms in order. We can get the result of our Pipeline on any raw element just by indexing into the TfmdLists: . t = tls[0]; t[:20] . TensorText([ 2, 19, 87, 133, 572, 14, 20, 32, 384, 19, 410, 619, 17, 48, 1639, 69, 13842, 7, 942, 10]) . tls.decode(t)[:100] . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesti&#39; . And the TfmdLists knows how to decode for show purposes: . In fact, it even has a show method: . tls.show(t) . xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesting and tricky ideas that xxmaj i &#39;ve ever seen when talking about movies . xxmaj they had just one scenery , a bunch of actors and a plot . xxmaj so , what made it so special were all the effective direction , great dialogs and a bizarre condition that characters had to deal like rats in a labyrinth . xxmaj his second movie , &#34; cypher &#34; ( 2002 ) , was all about its story , but it was n&#39;t so good as &#34; cube &#34; but here are the characters being tested like rats again . &#34; nothing &#34; is something very interesting and gets xxmaj vincenzo coming back to his &#39; cube days &#39; , locking the characters once again in a very different space with no time once more playing with the characters like playing with rats in an experience room . xxmaj but instead of a thriller sci - fi ( even some of the promotional teasers and trailers erroneous seemed like that ) , &#34; nothing &#34; is a loose and light comedy that for sure can be called a modern satire about our society and also about the intolerant world we &#39;re living . xxmaj once again xxmaj xxunk amaze us with a great idea into a so small kind of thing . 2 actors and a blinding white scenario , that &#39;s all you got most part of time and you do n&#39;t need more than that . xxmaj while &#34; cube &#34; is a claustrophobic experience and &#34; cypher &#34; confusing , &#34; nothing &#34; is completely the opposite but at the same time also desperate . xxmaj this movie proves once again that a smart idea means much more than just a millionaire budget . xxmaj of course that the movie fails sometimes , but its prime idea means a lot and offsets any flaws . xxmaj there &#39;s nothing more to be said about this movie because everything is a brilliant surprise and a totally different experience that i had in movies since &#34; cube &#34; . . The TfmdLists is named with an &quot;s&quot; because it can handle a training and a validation set with a splits argument. You just need to pass the indices of which elements are in the training set, and which are in the validation set: . cut = int(len(files)*0.8) splits = [list(range(cut)), list(range(cut,len(files)))] tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits) . You can then access them through the train and valid attributes: . tls.valid[0][:20] . TensorText([ 2, 8, 20, 30, 62, 208, 15, 43, 9, 463, 30, 19, 42, 145, 132, 10, 19, 83, 324, 60]) . If you have manually written a Transform that performs all of your preprocessing at once, turning raw items into a tuple with inputs and targets, then TfmdLists is the class you need. You can directly convert it to a DataLoaders object with the dataloaders method. This is what we will do in our Siamese example later in this chapter. . In general, though, you will have two (or more) parallel pipelines of transforms: one for processing your raw items into inputs and one to process your raw items into targets. For instance, here, the pipeline we defined only processes the raw text into inputs. If we want to do text classification, we also have to process the labels into targets. . For this we need to do two things. First we take the label name from the parent folder. There is a function, parent_label, for this: . lbls = files.map(parent_label) lbls . (#50000) [&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;...] . Then we need a Transform that will grab the unique items and build a vocab with them during setup, then transform the string labels into integers when called. fastai provides this for us; it&#39;s called Categorize: . cat = Categorize() cat.setup(lbls) cat.vocab, cat(lbls[0]) . ([&#39;neg&#39;, &#39;pos&#39;], TensorCategory(1)) . To do the whole setup automatically on our list of files, we can create a TfmdLists as before: . tls_y = TfmdLists(files, [parent_label, Categorize()]) tls_y[0] . TensorCategory(1) . But then we end up with two separate objects for our inputs and targets, which is not what we want. This is where Datasets comes to the rescue. . Datasets . Datasets will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result. Like TfmdLists, it will automatically do the setup for us, and when we index into a Datasets, it will return us a tuple with the results of each pipeline: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms]) x,y = dsets[0] x[:20],y . (TensorText([ 2, 19, 87, 133, 572, 14, 20, 32, 384, 19, 410, 619, 17, 48, 1639, 69, 13842, 7, 942, 10]), TensorCategory(1)) . Like a TfmdLists, we can pass along splits to a Datasets to split our data between training and validation sets: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms], splits=splits) x,y = dsets.valid[0] x[:20],y . (TensorText([ 2, 8, 20, 30, 62, 208, 15, 43, 9, 463, 30, 19, 42, 145, 132, 10, 19, 83, 324, 60]), TensorCategory(0)) . It can also decode any processed tuple or show it directly: . t = dsets.valid[0] dsets.decode(t) . (&#39;xxbos xxmaj this movie has got to be the worse movie i have ever seen . i only watched about a half an hour and i just shut it off . xxmaj the cars in this movie look like two geo metro &#39;s front ends smashed together . xxmaj this movie is n &#39;t even good for laughs . xxmaj the only time i laughed was when xxmaj dante kept saying in funny voice , &#34; nobody can beat xxmaj dante , xxmaj xxunk xxunk . &#34; i said holy god and shut it off . xxmaj bad , xxmaj bad movie . 2 / 10&#39;, &#39;neg&#39;) . The last step is to convert our Datasets object to a DataLoaders, which can be done with the dataloaders method. Here we need to pass along a special argument to take care of the padding problem (as we saw in the last chapter). This needs to happen just before we batch the elements, so we pass it to before_batch: . dls = dsets.dataloaders(bs=64, before_batch=pad_input) . dataloaders directly calls DataLoader on each subset of our Datasets. fastai&#39;s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization, but the most important ones that you should know are: . after_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock. | before_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size. | after_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock. | . As a conclusion, here is the full code necessary to prepare the data for text classification: . tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]] files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) splits = GrandparentSplitter(valid_name=&#39;test&#39;)(files) dsets = Datasets(files, tfms, splits=splits) dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input) . The two differences from the previous code are the use of GrandparentSplitter to split our training and validation data, and the dl_type argument. This is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches. . This does the exact same thing as our previous DataBlock: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . But now, you know how to customize every single piece of it! . Let&#39;s practice what we just learned about this mid-level API for data preprocessing, using a computer vision example now. . Applying the Mid-Level Data API: SiamesePair . A Siamese model takes two images and has to determine if they are of the same class or not. For this example, we will use the Pet dataset again and prepare the data for a model that will have to predict if two images of pets are of the same breed or not. We will explain here how to prepare the data for such a model, then we will train that model in &lt;&gt;.&lt;/p&gt; First things first, let&#39;s get the images in our dataset: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.vision.all import * path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) . If we didn&#39;t care about showing our objects at all, we could directly create one transform to completely preprocess that list of files. We will want to look at those images though, so we need to create a custom type. When you call the show method on a TfmdLists or a Datasets object, it will decode items until it reaches a type that contains a show method and use it to show the object. That show method gets passed a ctx, which could be a matplotlib axis for images, or a row of a DataFrame for texts. . Here we create a SiameseImage object that subclasses fastuple and is intended to contain three things: two images, and a Boolean that&#39;s True if the images are of the same breed. We also implement the special show method, such that it concatenates the two images with a black line in the middle. Don&#39;t worry too much about the part that is in the if test (which is to show the SiameseImage when the images are Python images, not tensors); the important part is in the last three lines: . . Note: need the check what is fastuple ? . doc(fastuple) . class SiameseImage(fastuple): def show(self, ctx=None, **kwargs): img1,img2,same_breed = self if not isinstance(img1, Tensor): if img2.size != img1.size: img2 = img2.resize(img1.size) t1,t2 = tensor(img1),tensor(img2) t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1) else: t1,t2 = img1,img2 line = t1.new_zeros(t1.shape[0], t1.shape[1], 10) return show_image(torch.cat([t1,line,t2], dim=2), title=same_breed, ctx=ctx) . Let&#39;s create a first SiameseImage and check our show method works: . img = PILImage.create(files[0]) s = SiameseImage(img, img, True) s.show(); . We can also try with a second image that&#39;s not from the same class: . img1 = PILImage.create(files[1]) s1 = SiameseImage(img, img1, False) s1.show(); . The important thing with transforms that we saw before is that they dispatch over tuples or their subclasses. That&#39;s precisely why we chose to subclass fastuple in this instance—this way we can apply any transform that works on images to our SiameseImage and it will be applied on each image in the tuple: . s2 = Resize(224)(s1) s2.show(); . Here the Resize transform is applied to each of the two images, but not the Boolean flag. Even if we have a custom type, we can thus benefit from all the data augmentation transforms inside the library. . We are now ready to build the Transform that we will use to get our data ready for a Siamese model. First, we will need a function to determine the classes of all our images: . def label_func(fname): return re.match(r&#39;^(.*)_ d+.jpg$&#39;, fname.name).groups()[0] . For each image our tranform will, with a probability of 0.5, draw an image from the same class and return a SiameseImage with a true label, or draw an image from another class and return a SiameseImage with a false label. This is all done in the private _draw function. There is one difference between the training and validation sets, which is why the transform needs to be initialized with the splits: on the training set we will make that random pick each time we read an image, whereas on the validation set we make this random pick once and for all at initialization. This way, we get more varied samples during training, but always the same validation set: . class SiameseTransform(Transform): def __init__(self, files, label_func, splits): self.labels = files.map(label_func).unique() self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels} self.label_func = label_func self.valid = {f: self._draw(f) for f in files[splits[1]]} def encodes(self, f): f2,t = self.valid.get(f, self._draw(f)) img1,img2 = PILImage.create(f),PILImage.create(f2) return SiameseImage(img1, img2, t) def _draw(self, f): same = random.random() &lt; 0.5 cls = self.label_func(f) if not same: cls = random.choice(L(l for l in self.labels if l != cls)) return random.choice(self.lbl2files[cls]),same . We can then create our main transform: . splits = RandomSplitter()(files) tfm = SiameseTransform(files, label_func, splits) tfm(files[0]).show(); . In the mid-level API for data collection we have two objects that can help us apply transforms on a set of items, TfmdLists and Datasets. If you remember what we have just seen, one applies a Pipeline of transforms and the other applies several Pipelines of transforms in parallel, to build tuples. Here, our main transform already builds the tuples, so we use TfmdLists: . tls = TfmdLists(files, tfm, splits=splits) show_at(tls.valid, 0); . And we can finally get our data in DataLoaders by calling the dataloaders method. One thing to be careful of here is that this method does not take item_tfms and batch_tfms like a DataBlock. The fastai DataLoader has several hooks that are named after events; here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it&#39;s built is called after_batch: . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . Note that we need to pass more transforms than usual—that&#39;s because the data block API usually adds them automatically: . ToTensor is the one that converts images to tensors (again, it&#39;s applied on every part of the tuple). | IntToFloatTensor converts the tensor of images containing integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1. | . We can now train a model using this DataLoaders. It will need a bit more customization than the usual model provided by cnn_learner since it has to take two images instead of one, but we will see how to create such a model and train it in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Conclusion . fastai provides a layered API. It takes one line of code to grab the data when it&#39;s in one of the usual settings, making it easy for beginners to focus on training a model without spending too much time assembling the data. Then, the high-level data block API gives you more flexibility by allowing you to mix and match some building blocks. Underneath it, the mid-level API gives you greater flexibility to apply any transformations on your items. In your real-world problems, this is probably what you will need to use, and we hope it makes the step of data-munging as easy as possible. . Questionnaire . Why do we say that fastai has a &quot;layered&quot; API? What does it mean? | Why does a Transform have a decode method? What does it do? | Why does a Transform have a setup method? What does it do? | How does a Transform work when called on a tuple? | Which methods do you need to implement when writing your own Transform? | Write a Normalize transform that fully normalizes items (subtract the mean and divide by the standard deviation of the dataset), and that can decode that behavior. Try not to peek! | Write a Transform that does the numericalization of tokenized texts (it should set its vocab automatically from the dataset seen and have a decode method). Look at the source code of fastai if you need help. | What is a Pipeline? | What is a TfmdLists? | What is a Datasets? How is it different from a TfmdLists? | Why are TfmdLists and Datasets named with an &quot;s&quot;? | How can you build a DataLoaders from a TfmdLists or a Datasets? | How do you pass item_tfms and batch_tfms when building a DataLoaders from a TfmdLists or a Datasets? | What do you need to do when you want to have your custom items work with methods like show_batch or show_results? | Why can we easily apply fastai data augmentation transforms to the SiamesePair we built? | Further Research . Use the mid-level API to prepare the data in DataLoaders on your own datasets. Try this with the Pet dataset and the Adult dataset from Chapter 1. | Look at the Siamese tutorial in the fastai documentation to learn how to customize the behavior of show_batch and show_results for new type of items. Implement it in your own project. | Understanding fastai&#39;s Applications: Wrap Up . Congratulations—you&#39;ve completed all of the chapters in this book that cover the key practical parts of training models and using deep learning! You know how to use all of fastai&#39;s built-in applications, and how to customize them using the data block API and loss functions. You even know how to create a neural network from scratch, and train it! (And hopefully you now know some of the questions to ask to make sure your creations help improve society too.) . The knowledge you already have is enough to create full working prototypes of many types of neural network applications. More importantly, it will help you understand the capabilities and limitations of deep learning models, and how to design a system that&#39;s well adapted to them. . In the rest of this book we will be pulling apart those applications, piece by piece, to understand the foundations they are built on. This is important knowledge for a deep learning practitioner, because it is what allows you to inspect and debug models that you build and create new applications that are customized for your particular projects. . &lt;/div&gt; . .",
            "url": "https://niyazikemer.com/fastbook/2021/11/02/chapter-11.html",
            "relUrl": "/fastbook/2021/11/02/chapter-11.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Chapter-10 - NLP  Deep Dive RNNs",
            "content": ". my sister&#39;s dogo. . [[chapter_nlp]] NLP Deep Dive: RNNs . In &lt;&gt; we saw that deep learning can be used to get great results with natural language datasets. Our example relied on using a pretrained language model and fine-tuning it to classify reviews. That example highlighted a difference between transfer learning in NLP and computer vision: in general in NLP the pretrained model is trained on a different task.&lt;/p&gt; What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called self-supervised learning: we do not need to give labels to our model, just feed it lots and lots of texts. It has a process to automatically get labels from the data, and this task isn&#39;t trivial: to properly guess the next word in a sentence, the model will have to develop an understanding of the English (or other) language. Self-supervised learning can also be used in other domains; for instance, see &quot;Self-Supervised Learning and Computer Vision&quot; for an introduction to vision applications. Self-supervised learning is not usually used for the model that is trained directly, but instead is used for pretraining a model used for transfer learning. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; jargon:Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text. . The language model we used in &lt;&gt; to classify IMDb reviews was pretrained on Wikipedia. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better. The Wikipedia English is slightly different from the IMDb English, so instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and then use that as the base for our classifier.&lt;/p&gt; Even if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targeting. It may be more informal language, or more technical, with new words to learn or different ways of composing sentences. In the case of the IMDb dataset, there will be lots of names of movie directors and actors, and often a less formal style of language than that seen in Wikipedia. . We already saw that with fastai, we can download a pretrained English language model and use it to get state-of-the-art results for NLP classification. (We expect pretrained models in many more languages to be available soon—they might well be available by the time you are reading this book, in fact.) So, why are we learning how to train a language model in detail? . One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model. For instance, for the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached. Since there are 25,000 labeled reviews in the training set and 25,000 in the validation set, that makes 100,000 movie reviews altogether. We can use all of these reviews to fine-tune the pretrained language model, which was trained only on Wikipedia articles; this will result in a language model that is particularly good at predicting the next word of a movie review. . This is known as the Universal Language Model Fine-tuning (ULMFit) approach. The paper showed that this extra stage of fine-tuning of the language model, prior to transfer learning to a classification task, resulted in significantly better predictions. Using this approach, we have three stages for transfer learning in NLP, as summarized in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Note: &quot;But there is another very practical reason, which is that you get even better results if you fine-tune the (sequence-based) language model prior to fine-tuning the classification model.&quot; Finetuning the language model before finetuning the classification model. . The ULMFiT process . We&#39;ll now explore how to apply a neural network to this language modeling problem, using the concepts introduced in the last two chapters. But before reading further, pause and think about how you would approach this. . Text Preprocessing . It&#39;s not at all obvious how we&#39;re going to use what we&#39;ve learned so far to build a language model. Sentences can be different lengths, and documents can be very long. So, how can we predict the next word of a sentence using a neural network? Let&#39;s find out! . We&#39;ve already seen how categorical variables can be used as independent variables for a neural network. The approach we took for a single categorical variable was to: . Make a list of all possible levels of that categorical variable (we&#39;ll call this list the vocab). | Replace each level with its index in the vocab. | Create an embedding matrix for this containing a row for each level (i.e., for each item of the vocab). | Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step 2; this is equivalent to but faster and more efficient than a matrix that takes as input one-hot-encoded vectors representing the indexes.) | We can do nearly the same thing with text! What is new is the idea of a sequence. First we concatenate all of the documents in our dataset into one big long string and split it into words, giving us a very long list of words (or &quot;tokens&quot;). Our independent variable will be the sequence of words starting with the first word in our very long list and ending with the second to last, and our dependent variable will be the sequence of words starting with the second word and ending with the last word. . Our vocab will consist of a mix of common words that are already in the vocabulary of our pretrained model and new words specific to our corpus (cinematographic terms or actors names, for instance). Our embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words we won&#39;t have anything, so we will just initialize the corresponding row with a random vector. . . Note: &quot;for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of the pretrained model; but for new words we won&#8217;t have anything, so we will just initialize the corresponding row with a random vector.&quot; interestin point I do not remember this from previous learning experience. . Each of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are: . Tokenization:: Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model) | Numericalization:: Make a list of all of the unique words that appear (the vocab), and convert each word into a number, by looking up its index in the vocab | Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required | Language model creation:: We need a special kind of model that does something we haven&#39;t seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). We will get to the details of these RNNs in the &lt;&gt;, but for now, you can think of it as just another deep neural network.&lt;/li&gt; &lt;/ul&gt; Let&#39;s take a look at how each step works in detail. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Tokenization . When we said &quot;convert the text into a list of words,&quot; we left out a lot of details. For instance, what do we do with punctuation? How do we deal with a word like &quot;don&#39;t&quot;? Is it one word, or two? What about long medical or chemical words? Should they be split into their separate pieces of meaning? How about hyphenated words? What about languages like German and Polish where we can create really long words from many, many pieces? What about languages like Japanese and Chinese that don&#39;t use bases at all, and don&#39;t really have a well-defined idea of word? . Because there is no one correct answer to these questions, there is no one approach to tokenization. There are three main approaches: . Word-based:: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning &quot;don&#39;t&quot; into &quot;do n&#39;t&quot;). Generally, punctuation marks are also split into separate tokens. | Subword based:: Split words into smaller parts, based on the most commonly occurring substrings. For instance, &quot;occasion&quot; might be tokenized as &quot;o c ca sion.&quot; | Character-based:: Split a sentence into its individual characters. | . We&#39;ll be looking at word and subword tokenization here, and we&#39;ll leave character-based tokenization for you to implement in the questionnaire at the end of this chapter. . jargon:token: One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character. . Word Tokenization with fastai . Rather than providing its own tokenizers, fastai instead provides a consistent interface to a range of tokenizers in external libraries. Tokenization is an active field of research, and new and improved tokenizers are coming out all the time, so the defaults that fastai uses change too. However, the API and options shouldn&#39;t change too much, since fastai tries to maintain a consistent API even as the underlying technology changes. . Let&#39;s try it out with the IMDb dataset that we used in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.text.all import * path = untar_data(URLs.IMDB) . We&#39;ll need to grab the text files in order to try out a tokenizer. Just like get_image_files, which we&#39;ve used many times already, gets all the image files in a path, get_text_files gets all the text files in a path. We can also optionally pass folders to restrict the search to a particular list of subfolders: . files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) . Here&#39;s a review that we&#39;ll tokenize (we&#39;ll just print the start of it here to save space): . txt = files[0].open().read(); txt[:75] . &#39;Nukie is widely regarded as the worst/most painful movie ever made. No one &#39; . As we write this book, the default English word tokenizer for fastai uses a library called spaCy. It has a sophisticated rules engine with special rules for URLs, individual special English words, and much more. Rather than directly using SpacyTokenizer, however, we&#39;ll use WordTokenizer, since that will always point to fastai&#39;s current default word tokenizer (which may not necessarily be spaCy, depending when you&#39;re reading this). . Let&#39;s try it out. We&#39;ll use fastai&#39;s coll_repr(collection, n) function to display the results. This displays the first n items of collection, along with the full size—it&#39;s what L uses by default. Note that fastai&#39;s tokenizers take a collection of documents to tokenize, so we have to wrap txt in a list: . spacy = WordTokenizer() toks = first(spacy([txt])) print(coll_repr(toks, 30)) . (#319) [&#39;Nukie&#39;,&#39;is&#39;,&#39;widely&#39;,&#39;regarded&#39;,&#39;as&#39;,&#39;the&#39;,&#39;worst&#39;,&#39;/&#39;,&#39;most&#39;,&#39;painful&#39;,&#39;movie&#39;,&#39;ever&#39;,&#39;made&#39;,&#39;.&#39;,&#39;No&#39;,&#39;one&#39;,&#39;who&#39;,&#39;has&#39;,&#39;seen&#39;,&#39;it&#39;,&#39;denies&#39;,&#39;this&#39;,&#39;assertion&#39;,&#39;.&#39;,&#39;It&#39;,&#39;tops&#39;,&#39;even&#39;,&#39;the&#39;,&#39;infamous&#39;,&#39;Manos&#39;...] . doc(first) . As you see, spaCy has mainly just separated out the words and punctuation. But it does something else here too: it has split &quot;it&#39;s&quot; into &quot;it&quot; and &quot;&#39;s&quot;. That makes intuitive sense; these are separate words, really. Tokenization is a surprisingly subtle task, when you think about all the little details that have to be handled. Fortunately, spaCy handles these pretty well for us—for instance, here we see that &quot;.&quot; is separated when it terminates a sentence, but not in an acronym or number: . first(spacy([&#39;The U.S. dollar $1 is $1.00.&#39;])) . (#9) [&#39;The&#39;,&#39;U.S.&#39;,&#39;dollar&#39;,&#39;$&#39;,&#39;1&#39;,&#39;is&#39;,&#39;$&#39;,&#39;1.00&#39;,&#39;.&#39;] . fastai then adds some additional functionality to the tokenization process with the Tokenizer class: . spacy([&#39;The U.S. dollar $1 is $1.00.&#39;]) . &lt;generator object SpacyTokenizer.__call__.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f0f36556b30&gt; . tkn = Tokenizer(spacy) print(coll_repr(tkn(txt), 31)) . (#344) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;nukie&#39;,&#39;is&#39;,&#39;widely&#39;,&#39;regarded&#39;,&#39;as&#39;,&#39;the&#39;,&#39;worst&#39;,&#39;/&#39;,&#39;most&#39;,&#39;painful&#39;,&#39;movie&#39;,&#39;ever&#39;,&#39;made&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;no&#39;,&#39;one&#39;,&#39;who&#39;,&#39;has&#39;,&#39;seen&#39;,&#39;it&#39;,&#39;denies&#39;,&#39;this&#39;,&#39;assertion&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;it&#39;,&#39;tops&#39;,&#39;even&#39;...] . Notice that there are now some tokens that start with the characters &quot;xx&quot;, which is not a common word prefix in English. These are special tokens. . For example, the first item in the list, xxbos, is a special token that indicates the start of a new text (&quot;BOS&quot; is a standard NLP acronym that means &quot;beginning of stream&quot;). By recognizing this start token, the model will be able to learn it needs to &quot;forget&quot; what was said previously and focus on upcoming words. . These special tokens don&#39;t come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognize the important parts of a sentence. In a sense, we are translating the original English language sequence into a simplified tokenized language—a language that is designed to be easy for a model to learn. . For instance, the rules will replace a sequence of four exclamation points with a special repeated character token, followed by the number four, and then a single exclamation point. In this way, the model&#39;s embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word. This way, the embedding matrix only needs the lowercase versions of the words, saving compute and memory resources, but can still learn the concept of capitalization. . Here are some of the main special tokens you&#39;ll see: . xxbos:: Indicates the beginning of a text (here, a review) | xxmaj:: Indicates the next word begins with a capital (since we lowercased everything) | xxunk:: Indicates the word is unknown | . To see the rules that were used, you can check the default rules: . defaults.text_proc_rules . [&lt;function fastai.text.core.fix_html(x)&gt;, &lt;function fastai.text.core.replace_rep(t)&gt;, &lt;function fastai.text.core.replace_wrep(t)&gt;, &lt;function fastai.text.core.spec_add_spaces(t)&gt;, &lt;function fastai.text.core.rm_useless_spaces(t)&gt;, &lt;function fastai.text.core.replace_all_caps(t)&gt;, &lt;function fastai.text.core.replace_maj(t)&gt;, &lt;function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)&gt;] . As always, you can look at the source code of each of them in a notebook by typing: . ??replace_rep . Here is a brief summary of what each does: . fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these) | replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it&#39;s repeated, then the character | replace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it&#39;s repeated, then the word | spec_add_spaces:: Adds spaces around / and # | rm_useless_spaces:: Removes all repetitions of the space character | replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it | replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it | lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos) | . Let&#39;s take a look at a few of them in action: . coll_repr(tkn(&#39;&amp;copy; Fast.ai www.fast.ai/INDEX&#39;), 31) . &#34;(#11) [&#39;xxbos&#39;,&#39;©&#39;,&#39;xxmaj&#39;,&#39;fast.ai&#39;,&#39;xxrep&#39;,&#39;3&#39;,&#39;w&#39;,&#39;.fast.ai&#39;,&#39;/&#39;,&#39;xxup&#39;,&#39;index&#39;]&#34; . Now let&#39;s take a look at how subword tokenization would work. . Subword Tokenization . In addition to the word tokenization approach seen in the last section, another popular tokenization method is subword tokenization. Word tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a sentence. However, this assumption is not always appropriate. For instance, consider this sentence: 我的名字是郝杰瑞 (&quot;My name is Jeremy Howard&quot; in Chinese). That&#39;s not going to work very well with a word tokenizer, because there are no spaces in it! Languages like Chinese and Japanese don&#39;t use spaces, and in fact they don&#39;t even have a well-defined concept of a &quot;word.&quot; There are also languages, like Turkish and Hungarian, that can add many subwords together without spaces, creating very long words that include a lot of separate pieces of information. . To handle these cases, it&#39;s generally best to use subword tokenization. This proceeds in two steps: . Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab. | Tokenize the corpus using this vocab of subword units. | Let&#39;s look at an example. For our corpus, we&#39;ll use the first 2,000 movie reviews: . txts = L(o.open().read() for o in files[:2000]) . We instantiate our tokenizer, passing in the size of the vocab we want to create, and then we need to &quot;train&quot; it. That is, we need to have it read our documents and find the common sequences of characters to create the vocab. This is done with setup. As we&#39;ll see shortly, setup is a special fastai method that is called automatically in our usual data processing pipelines. Since we&#39;re doing everything manually at the moment, however, we have to call it ourselves. Here&#39;s a function that does these steps for a given vocab size, and shows an example output: . def subword(sz): sp = SubwordTokenizer(vocab_sz=sz) sp.setup(txts) return &#39; &#39;.join(first(sp([txt]))[:40]) . Let&#39;s try it out: . subword(1000) . &#39;▁N u k ie ▁is ▁w ide ly ▁re g ard ed ▁as ▁the ▁worst / mo st ▁pa in ful ▁movie ▁ever ▁made . ▁No ▁one ▁who ▁has ▁seen ▁it ▁de n ies ▁this ▁as s er tion .&#39; . When using fastai&#39;s subword tokenizer, the special character ▁ represents a space character in the original text. . If we use a smaller vocab, then each token will represent fewer characters, and it will take more tokens to represent a sentence: . subword(200) . &#39;▁ N u k i e ▁is ▁w i d e ly ▁re g ar d ed ▁a s ▁the ▁w or st / m o st ▁p a in f u l ▁movie ▁ e ver ▁ma d e&#39; . On the other hand, if we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence: . subword(10000) . &#39;▁Nukie ▁is ▁wide ly ▁regard ed ▁as ▁the ▁worst / most ▁painful ▁movie ▁ever ▁made . ▁No ▁one ▁who ▁has ▁seen ▁it ▁deni es ▁this ▁as s er tion . ▁It ▁top s ▁even ▁the ▁in famous ▁Man os :&#39; . Picking a subword vocab size represents a compromise: a larger vocab means fewer tokens per sentence, which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn. . Overall, subword tokenization provides a way to easily scale between character tokenization (i.e., using a small subword vocab) and word tokenization (i.e., using a large subword vocab), and handles every human language without needing language-specific algorithms to be developed. It can even handle other &quot;languages&quot; such as genomic sequences or MIDI music notation! For this reason, in the last year its popularity has soared, and it seems likely to become the most common tokenization approach (it may well already be, by the time you read this!). . Once our texts have been split into tokens, we need to convert them to numbers. We&#39;ll look at that next. . Numericalization with fastai . Numericalization is the process of mapping tokens to integers. The steps are basically identical to those necessary to create a Category variable, such as the dependent variable of digits in MNIST: . Make a list of all possible levels of that categorical variable (the vocab). | Replace each level with its index in the vocab. | Let&#39;s take a look at this in action on the word-tokenized text we saw earlier: . toks = tkn(txt) print(coll_repr(tkn(txt), 31)) . (#344) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;nukie&#39;,&#39;is&#39;,&#39;widely&#39;,&#39;regarded&#39;,&#39;as&#39;,&#39;the&#39;,&#39;worst&#39;,&#39;/&#39;,&#39;most&#39;,&#39;painful&#39;,&#39;movie&#39;,&#39;ever&#39;,&#39;made&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;no&#39;,&#39;one&#39;,&#39;who&#39;,&#39;has&#39;,&#39;seen&#39;,&#39;it&#39;,&#39;denies&#39;,&#39;this&#39;,&#39;assertion&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;it&#39;,&#39;tops&#39;,&#39;even&#39;...] . Just like with SubwordTokenizer, we need to call setup on Numericalize; this is how we create the vocab. That means we&#39;ll need our tokenized corpus first. Since tokenization takes a while, it&#39;s done in parallel by fastai; but for this manual walkthrough, we&#39;ll use a small subset: . toks200 = txts[:200].map(tkn) toks200[0] . (#344) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;nukie&#39;,&#39;is&#39;,&#39;widely&#39;,&#39;regarded&#39;,&#39;as&#39;,&#39;the&#39;,&#39;worst&#39;,&#39;/&#39;...] . We can pass this to setup to create our vocab: . num = Numericalize() num.setup(toks200) coll_repr(num.vocab,20) . &#34;(#2104) [&#39;xxunk&#39;,&#39;xxpad&#39;,&#39;xxbos&#39;,&#39;xxeos&#39;,&#39;xxfld&#39;,&#39;xxrep&#39;,&#39;xxwrep&#39;,&#39;xxup&#39;,&#39;xxmaj&#39;,&#39;the&#39;,&#39;,&#39;,&#39;.&#39;,&#39;and&#39;,&#39;a&#39;,&#39;of&#39;,&#39;to&#39;,&#39;is&#39;,&#39;in&#39;,&#39;it&#39;,&#39;i&#39;...]&#34; . Our special rules tokens appear first, and then every word appears once, in frequency order. The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn&#39;t enough data to train useful representations for rare words. However, this last issue is better handled by setting min_freq; the default min_freq=3 means that any word appearing less than three times is replaced with xxunk. . fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter. . Once we&#39;ve created our Numericalize object, we can use it as if it were a function: . nums = num(toks)[:20]; nums . TensorText([ 2, 8, 1556, 16, 0, 0, 26, 9, 240, 142, 106, 1254, 31, 171, 132, 11, 8, 78, 44, 46]) . This time, our tokens have been converted to a tensor of integers that our model can receive. We can check that they map back to the original text: . &#39; &#39;.join(num.vocab[o] for o in nums) . &#39;xxbos xxmaj nukie is xxunk xxunk as the worst / most painful movie ever made . xxmaj no one who&#39; . Now that we have numbers, we need to put them in batches for our model. . Putting Our Texts into Batches for a Language Model . When dealing with images, we needed to resize them all to the same height and width before grouping them together in a mini-batch so they could stack together efficiently in a single tensor. Here it&#39;s going to be a little different, because one cannot simply resize text to a desired length. Also, we want our language model to read text in order, so that it can efficiently predict what the next word is. This means that each new batch should begin precisely where the previous one left off. . Suppose we have the following text: . :In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we&#39;ll have another example of the PreProcessor used in the data block API. nThen we will study how we build a language model and train it for a while. The tokenization process will add special tokens and deal with punctuation to return this text: :xxbos xxmaj in this chapter , we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj first we will look at the processing steps necessary to convert text into numbers and how to customize it . xxmaj by doing this , we &#39;ll have another example of the preprocessor used in the data block xxup api . n xxmaj then we will study how we build a language model and train it for a while . We now have 90 tokens, separated by spaces. Let&#39;s say we want a batch size of 6. We need to break this text into 6 contiguous parts of length 15: . xxbos | xxmaj | in | this | chapter | , | we | will | go | back | over | the | example | of | classifying | . movie | reviews | we | studied | in | chapter | 1 | and | dig | deeper | under | the | surface | . | xxmaj | . first | we | will | look | at | the | processing | steps | necessary | to | convert | text | into | numbers | and | . how | to | customize | it | . | xxmaj | by | doing | this | , | we | &#39;ll | have | another | example | . of | the | preprocessor | used | in | the | data | block | xxup | api | . | n | xxmaj | then | we | . will | study | how | we | build | a | language | model | and | train | it | for | a | while | . | . In a perfect world, we could then give this one batch to our model. But that approach doesn&#39;t scale, because outside of this toy example it&#39;s unlikely that a single batch containing all the texts would fit in our GPU memory (here we have 90 tokens, but all the IMDb reviews together give several million). . So, we need to divide this array more finely into subarrays of a fixed sequence length. It is important to maintain order within and across these subarrays, because we will use a model that maintains a state so that it remembers what it read previously when predicting what comes next. . Going back to our previous example with 6 batches of length 15, if we chose a sequence length of 5, that would mean we first feed the following array: . xxbos | xxmaj | in | this | chapter | . movie | reviews | we | studied | in | . first | we | will | look | at | . how | to | customize | it | . | . of | the | preprocessor | used | in | . will | study | how | we | build | . Then this one: . , | we | will | go | back | . chapter | 1 | and | dig | deeper | . the | processing | steps | necessary | to | . xxmaj | by | doing | this | , | . the | data | block | xxup | api | . a | language | model | and | train | . And finally: . over | the | example | of | classifying | . under | the | surface | . | xxmaj | . convert | text | into | numbers | and | . we | &#39;ll | have | another | example | . . | n | xxmaj | then | we | . it | for | a | while | . | . Going back to our movie reviews dataset, the first step is to transform the individual texts into a stream by concatenating them together. As with images, it&#39;s best to randomize the order of the inputs, so at the beginning of each epoch we will shuffle the entries to make a new stream (we shuffle the order of the documents, not the order of the words inside them, or the texts would not make sense anymore!). . We then cut this stream into a certain number of batches (which is our batch size). For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to 10,000...), because we want the model to read continuous rows of text (as in the preceding example). An xxbos token is added at the start of each during preprocessing, so that the model knows when it reads the stream when a new entry is beginning. . So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length we picked. . This is all done behind the scenes by the fastai library when we create an LMDataLoader. We do this by first applying our Numericalize object to the tokenized texts: . nums200 = toks200.map(num) . and then passing that to LMDataLoader: . dl = LMDataLoader(nums200) . Let&#39;s confirm that this gives the expected results, by grabbing the first batch: . x,y = first(dl) x.shape,y.shape . (torch.Size([64, 72]), torch.Size([64, 72])) . and then looking at the first row of the independent variable, which should be the start of the first text: . &#39; &#39;.join(num.vocab[o] for o in x[0][:20]) . &#39;xxbos xxmaj nukie is xxunk xxunk as the worst / most painful movie ever made . xxmaj no one who&#39; . The dependent variable is the same thing offset by one token: . &#39; &#39;.join(num.vocab[o] for o in y[0][:20]) . &#39;xxmaj nukie is xxunk xxunk as the worst / most painful movie ever made . xxmaj no one who has&#39; . This concludes all the preprocessing steps we need to apply to our data. We are now ready to train our text classifier. . Training a Text Classifier . As we saw at the beginning of this chapter, there are two steps to training a state-of-the-art text classifier using transfer learning: first we need to fine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews, and then we can use that model to train a classifier. . As usual, let&#39;s start with assembling our data. . Language Model Using DataBlock . fastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock. All of the arguments that can be passed to Tokenize and Numericalize can also be passed to TextBlock. In the next chapter we&#39;ll discuss the easiest ways to run each of these steps separately, to ease debugging—but you can always just debug by running them manually on a subset of your data as shown in the previous sections. And don&#39;t forget about DataBlock&#39;s handy summary method, which is very useful for debugging data issues. . Here&#39;s how we use TextBlock to create a language model, using fastai&#39;s defaults: . . Note: Although it is possible to Tokenize and Numericalize our data in the DataBlock sometimes it is make more sense to do it separetely especially for debugging but still DataBlock is handy because of it&#8217;s summary method. . get_imdb = partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) dls_lm = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1) ).dataloaders(path, path=path, bs=64, seq_len=60) . One thing that&#39;s different to previous types we&#39;ve used in DataBlock is that we&#39;re not just using the class directly (i.e., TextBlock(...), but instead are calling a class method. A class method is a Python method that, as the name suggests, belongs to a class rather than an object. (Be sure to search online for more information about class methods if you&#39;re not familiar with them, since they&#39;re commonly used in many Python libraries and applications; we&#39;ve used them a few times previously in the book, but haven&#39;t called attention to them.) The reason that TextBlock is special is that setting up the numericalizer&#39;s vocab can take a long time (we have to read and tokenize every document to get the vocab). To be as efficient as possible it performs a few optimizations: . It saves the tokenized documents in a temporary folder, so it doesn&#39;t have to tokenize them more than once | It runs multiple tokenization processes in parallel, to take advantage of your computer&#39;s CPUs | . We need to tell TextBlock how to access the texts, so that it can do this initial preprocessing—that&#39;s what from_folder does. . show_batch then works in the usual way: . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxmaj any film fan knows that this is where xxmaj steven xxmaj spielberg got his start , directing the second vignette &quot; eyes &quot; . xxmaj but xxup night xxup gallery deserves more respect and attention because of its overall creepiness than for the debut of a young &quot; genius &quot; . n n xxmaj rod xxmaj serling , creator | xxmaj any film fan knows that this is where xxmaj steven xxmaj spielberg got his start , directing the second vignette &quot; eyes &quot; . xxmaj but xxup night xxup gallery deserves more respect and attention because of its overall creepiness than for the debut of a young &quot; genius &quot; . n n xxmaj rod xxmaj serling , creator of | . 1 the tongue out of her rapist &#39;s mouth ? n n xxmaj in order to be fair , i must accept that the direction of the film is good . xxmaj but what difference does this make , when the film in itself is completely unwatchable ? i do n&#39;t object to bloody scenes being shown in a film , provided | tongue out of her rapist &#39;s mouth ? n n xxmaj in order to be fair , i must accept that the direction of the film is good . xxmaj but what difference does this make , when the film in itself is completely unwatchable ? i do n&#39;t object to bloody scenes being shown in a film , provided that | . Now that our data is ready, we can fine-tune the pretrained language model. . Fine-Tuning the Language Model . To convert the integer word indices into activations that we can use for our neural network, we will use embeddings, just like we did for collaborative filtering and tabular modeling. Then we&#39;ll feed those embeddings into a recurrent neural network (RNN), using an architecture called AWD-LSTM (we will show you how to write such a model from scratch in &lt;&gt;). As we discussed earlier, the embeddings in the pretrained model are merged with random embeddings added for words that weren&#39;t in the pretraining vocabulary. This is handled automatically inside language_model_learner:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . The loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). The perplexity metric used here is often used in NLP for language models: it is the exponential of the loss (i.e., torch.exp(cross_entropy)). We also include the accuracy metric, to see how many times our model is right when trying to predict the next word, since cross-entropy (as we&#39;ve seen) is both hard to interpret, and tells us more about the model&#39;s confidence than its accuracy. . Let&#39;s go back to the process diagram from the beginning of this chapter. The first arrow has been completed for us and made available as a pretrained model in fastai, and we&#39;ve just built the DataLoaders and Learner for the second stage. Now we&#39;re ready to fine-tune our language model! . . It takes quite a while to train each epoch, so we&#39;ll be saving the intermediate model results during the training process. Since fine_tune doesn&#39;t do that for us, we&#39;ll use fit_one_cycle. Just like cnn_learner, language_model_learner automatically calls freeze when using a pretrained model (which is the default), so this will only train the embeddings (the only part of the model that contains randomly initialized weights—i.e., embeddings for words that are in our IMDb vocab, but aren&#39;t in the pretrained model vocab): . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 7.674394 | 7.316052 | 0.082508 | 1504.253418 | 16:51 | . This model takes a while to train, so it&#39;s a good opportunity to talk about saving intermediary results. . Saving and Loading Models . You can easily save the state of your model like so: . learn.save(&#39;1epoch&#39;) . Path(&#39;/home/niyazi/.fastai/data/imdb/models/1epoch.pth&#39;) . This will create a file in learn.path/models/ named 1epoch.pth. If you want to load your model in another machine after creating your Learner the same way, or resume training later, you can load the content of this file with: . learn = learn.load(&#39;1epoch&#39;) . Once the initial training has completed, we can continue fine-tuning the model after unfreezing: . learn.unfreeze() learn.fit_one_cycle(10, 2e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.737140 | 4.649145 | 0.252651 | 104.495613 | 17:49 | . 1 | 4.555501 | 4.479167 | 0.270682 | 88.161247 | 17:55 | . 2 | 4.443948 | 4.389276 | 0.280824 | 80.582062 | 17:54 | . 3 | 4.386376 | 4.325250 | 0.287976 | 75.584419 | 17:04 | . 4 | 4.321561 | 4.270780 | 0.293830 | 71.577454 | 17:04 | . 5 | 4.271872 | 4.220235 | 0.299794 | 68.049469 | 17:03 | . 6 | 4.205110 | 4.176213 | 0.304618 | 65.118767 | 17:02 | . 7 | 4.131721 | 4.145057 | 0.308356 | 63.121231 | 17:25 | . 8 | 4.133890 | 4.129030 | 0.310462 | 62.117653 | 17:36 | . 9 | 4.078181 | 4.126318 | 0.310714 | 61.949432 | 17:36 | . Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder: . learn.save_encoder(&#39;finetuned&#39;) . jargon:Encoder: The model not including the task-specific final layer(s). This term means much the same thing as body when applied to vision CNNs, but &quot;encoder&quot; tends to be more used for NLP and generative models. . This completes the second stage of the text classification process: fine-tuning the language model. We can now use it to fine-tune a classifier using the IMDb sentiment labels. . Text Generation . Before we move on to fine-tuning the classifier, let&#39;s quickly try something different: using our model to generate random reviews. Since it&#39;s trained to guess what the next word of the sentence is, we can use the model to write new reviews: . TEXT = &quot;What is your name&quot; N_WORDS = 50 N_SENTENCES = 2 preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)] . print(&quot; n&quot;.join(preds)) . What is your name ? All you need to know is that all of the participants are naked c *** offoffoff ofofof thethethe Earth , and I &#39;ll never know . i know this is a very good film , and it &#39;s good to see many of the What is your name in this movie ? What a terrible movie . It has no visual context , only a single movie or two , and a total lack of anything that can be said for any of the characters . The acting is horrible , and the lead actress . As you can see, we add some randomness (we pick a random word based on the probabilities returned by the model) so we don&#39;t get exactly the same review twice. Our model doesn&#39;t have any programmed knowledge of the structure of a sentence or grammar rules, yet it has clearly learned a lot about English sentences: we can see it capitalizes properly (I is just transformed to i because our rules require two characters or more to consider a word as capitalized, so it&#39;s normal to see it lowercased) and is using consistent tense. The general review makes sense at first glance, and it&#39;s only if you read carefully that you can notice something is a bit off. Not bad for a model trained in a couple of hours! . But our end goal wasn&#39;t to train a model to generate reviews, but to classify them... so let&#39;s use this model to do just that. . Creating the Classifier DataLoaders . We&#39;re now moving from language model fine-tuning to classifier fine-tuning. To recap, a language model predicts the next word of a document, so it doesn&#39;t need any external labels. A classifier, however, predicts some external label—in the case of IMDb, it&#39;s the sentiment of a document. . This means that the structure of our DataBlock for NLP classification will look very familiar. It&#39;s actually nearly the same as we&#39;ve seen for the many image classification datasets we&#39;ve worked with: . dls_clas = DataBlock( blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path, path=path, bs=64, seq_len=72) . Just like with image classification, show_batch shows the dependent variable (sentiment, in this case) with each independent variable (movie review text): . dls_clas.show_batch(max_n=3) . text category . 0 xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero | pos | . 1 xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it | pos | . 2 xxbos xxmaj some have praised _ xxunk _ as a xxmaj disney adventure for adults . i do n&#39;t think so -- at least not for thinking adults . n n xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the &quot; crack staff &quot; of many older adventure movies has been done well before , ( think _ the xxmaj dirty xxmaj dozen _ ) but _ atlantis _ represents one of the worse films in that motif . xxmaj the characters are weak . xxmaj even the background that each member trots out seems stock and awkward at best . xxmaj an xxup md / xxmaj medicine xxmaj man , a tomboy mechanic whose father always wanted sons , if we have not at least seen these before | neg | . Looking at the DataBlock definition, every piece is familiar from previous data blocks we&#39;ve built, with two important exceptions: . TextBlock.from_folder no longer has the is_lm=True parameter. | We pass the vocab we created for the language model fine-tuning. | . The reason that we pass the vocab of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won&#39;t make any sense to this model, and the fine-tuning step won&#39;t be of any use. . By passing is_lm=False (or not passing is_lm at all, since it defaults to False) we tell TextBlock that we have regular labeled data, rather than using the next tokens as labels. There is one challenge we have to deal with, however, which is to do with collating multiple documents into a mini-batch. Let&#39;s see with an example, by trying to create a mini-batch containing the first 10 documents. First we&#39;ll numericalize them: . . Note: &quot;- We pass the vocab we created for the language model fine-tuning.&quot; this is important. . nums_samp = toks200[:10].map(num) . Let&#39;s now look at how many tokens each of these 10 movie reviews have: . nums_samp.map(len) . (#10) [344,350,239,111,194,133,202,556,374,500] . Remember, PyTorch DataLoaders need to collate all the items in a batch into a single tensor, and a single tensor has a fixed shape (i.e., it has some particular length on every axis, and all items must be consistent). This should sound familiar: we had the same issue with images. In that case, we used cropping, padding, and/or squishing to make all the inputs the same size. Cropping might not be a good idea for documents, because it seems likely we&#39;d remove some key information (having said that, the same issue is true for images, and we use cropping there; data augmentation hasn&#39;t been well explored for NLP yet, so perhaps there are actually opportunities to use cropping in NLP too!). You can&#39;t really &quot;squish&quot; a document. So that leaves padding! . We will expand the shortest texts to make them all the same size. To do this, we use a special padding token that will be ignored by our model. Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend of be of similar lengths. We won&#39;t pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, but at the time of writing no library provides good support for this yet, and there aren&#39;t any papers covering it. It&#39;s something we&#39;re planning to add to fastai soon, however, so keep an eye on the book&#39;s website; we&#39;ll add information about this as soon as we have it working well.) . The sorting and padding are automatically done by the data block API for us when using a TextBlock, with is_lm=False. (We don&#39;t have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.) . We can now create a model to classify our texts: . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() . The final step prior to training the classifier is to load the encoder from our fine-tuned language model. We use load_encoder instead of load because we only have pretrained weights available for the encoder; load by default raises an exception if an incomplete model is loaded: . learn = learn.load_encoder(&#39;finetuned&#39;) . Fine-Tuning the Classifier . The last step is to train with discriminative learning rates and gradual unfreezing. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference: . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.361321 | 0.281704 | 0.888120 | 00:47 | . In just one epoch we get the same result as our training in &lt;&gt;: not too bad! We can pass -2 to freeze_to to freeze all except the last two parameter groups:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.277524 | 0.202118 | 0.922880 | 00:54 | . Then we can unfreeze a bit more, and continue training: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.228207 | 0.183575 | 0.930520 | 01:07 | . And finally, the whole model! . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.200681 | 0.181829 | 0.932600 | 01:19 | . 1 | 0.185760 | 0.177043 | 0.933640 | 01:20 | . We reached 94.3% accuracy, which was state-of-the-art performance just three years ago. By training another model on all the texts read backwards and averaging the predictions of those two models, we can even get to 95.1% accuracy, which was the state of the art introduced by the ULMFiT paper. It was only beaten a few months ago, by fine-tuning a much bigger model and using expensive data augmentation techniques (translating sentences in another language and back, using another model for translation). . Using a pretrained model let us build a fine-tuned language model that was pretty powerful, to either generate fake reviews or help classify them. This is exciting stuff, but it&#39;s good to remember that this technology can also be used for malign purposes. . Disinformation and Language Models . Even simple algorithms based on rules, before the days of widely available deep learning language models, could be used to create fraudulent accounts and try to influence policymakers. Jeff Kao, now a computational journalist at ProPublica, analyzed the comments that were sent to the US Federal Communications Commission (FCC) regarding a 2017 proposal to repeal net neutrality. In his article &quot;More than a Million Pro-Repeal Net Neutrality Comments Were Likely Faked&quot;, he reports how he discovered a large cluster of comments opposing net neutrality that seemed to have been generated by some sort of Mad Libs-style mail merge. In &lt;&gt;, the fake comments have been helpfully color-coded by Kao to highlight their formulaic nature.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Comments received by the FCC during the net neutrality debate . Kao estimated that &quot;less than 800,000 of the 22M+ comments… could be considered truly unique&quot; and that &quot;more than 99% of the truly unique comments were in favor of keeping net neutrality.&quot; . Given advances in language modeling that have occurred since 2017, such fraudulent campaigns could be nearly impossible to catch now. You now have all the necessary tools at your disposal to create a compelling language model—that is, something that can generate context-appropriate, believable text. It won&#39;t necessarily be perfectly accurate or correct, but it will be plausible. Think about what this technology would mean when put together with the kinds of disinformation campaigns we have learned about in recent years. Take a look at the Reddit dialogue shown in &lt;&gt;, where a language model based on OpenAI&#39;s GPT-2 algorithm is having a conversation with itself about whether the US government should cut defense spending.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; An algorithm talking to itself on Reddit . In this case, it was explicitly said that an algorithm was used, but imagine what would happen if a bad actor decided to release such an algorithm across social networks. They could do it slowly and carefully, allowing the algorithm to gradually develop followers and trust over time. It would not take many resources to have literally millions of accounts doing this. In such a situation we could easily imagine getting to a point where the vast majority of discourse online was from bots, and nobody would have any idea that it was happening. . We are already starting to see examples of machine learning being used to generate identities. For example, &lt;&gt; shows a LinkedIn profile for Katie Jones.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Katie Jones&#39;s LinkedIn profile . Katie Jones was connected on LinkedIn to several members of mainstream Washington think tanks. But she didn&#39;t exist. That image you see was auto-generated by a generative adversarial network, and somebody named Katie Jones has not, in fact, graduated from the Center for Strategic and International Studies. . Many people assume or hope that algorithms will come to our defense here—that we will develop classification algorithms that can automatically recognise autogenerated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms. . Conclusion . In this chapter we explored the last application covered out of the box by the fastai library: text. We saw two types of models: language models that can generate texts, and a classifier that determines if a review is positive or negative. To build a state-of-the art classifier, we used a pretrained language model, fine-tuned it to the corpus of our task, then used its body (the encoder) with a new head to do the classification. . Before we end this section, we&#39;ll take a look at how the fastai library can help you assemble your data for your specific problems. . Questionnaire . What is &quot;self-supervised learning&quot;? | What is a &quot;language model&quot;? | Why is a language model considered self-supervised? | What are self-supervised models usually used for? | Why do we fine-tune language models? | What are the three steps to create a state-of-the-art text classifier? | How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset? | What are the three steps to prepare your data for a language model? | What is &quot;tokenization&quot;? Why do we need it? | Name three different approaches to tokenization. | What is xxbos? | List four rules that fastai applies to text during tokenization. | Why are repeated characters replaced with a token showing the number of repetitions and the character that&#39;s repeated? | What is &quot;numericalization&quot;? | Why might there be words that are replaced with the &quot;unknown word&quot; token? | With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book&#39;s website.) | Why do we need padding for text classification? Why don&#39;t we need it for language modeling? | What does an embedding matrix for NLP contain? What is its shape? | What is &quot;perplexity&quot;? | Why do we have to pass the vocabulary of the language model to the classifier data block? | What is &quot;gradual unfreezing&quot;? | Why is text generation always likely to be ahead of automatic identification of machine-generated texts? | Further Research . See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty? | Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning? | &lt;/div&gt; . | . .",
            "url": "https://niyazikemer.com/fastbook/2021/09/25/chapter-10.html",
            "relUrl": "/fastbook/2021/09/25/chapter-10.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Chapter 9 - Tabular Modelling Deep Dive",
            "content": ". They are beautiful. (from my sister&#39;s place) . [[chapter_tabular]] Tabular Modeling Deep Dive . Tabular modeling takes data in the form of a table (like a spreadsheet or CSV). The objective is to predict the value in one column based on the values in the other columns. In this chapter we will not only look at deep learning but also more general machine learning techniques like random forests, as they can give better results depending on your problem. . We will look at how we should preprocess and clean the data as well as how to interpret the result of our models after training, but first, we will see how we can feed columns that contain categories into a model that expects numbers by using embeddings. . Categorical Embeddings . In tabular data some columns may contain numerical data, like &quot;age,&quot; while others contain string values, like &quot;sex.&quot; The numerical data can be directly fed to the model (with some optional preprocessing), but the other columns need to be converted to numbers. Since the values in those correspond to different categories, we often call this type of variables categorical variables. The first type are called continuous variables. . jargon:Continuous and Categorical Variables: Continuous variables are numerical data, such as &quot;age,&quot; that can be directly fed to the model, since you can add and multiply them directly. Categorical variables contain a number of discrete levels, such as &quot;movie ID,&quot; for which addition and multiplication don&#39;t have meaning (even if they&#39;re stored as numbers). . At the end of 2015, the Rossmann sales competition ran on Kaggle. Competitors were given a wide range of information about various stores in Germany, and were tasked with trying to predict sales on a number of days. The goal was to help the company to manage stock properly and be able to satisfy demand without holding unnecessary inventory. The official training set provided a lot of information about the stores. It was also permitted for competitors to use additional data, as long as that data was made public and available to all participants. . One of the gold medalists used deep learning, in one of the earliest known examples of a state-of-the-art deep learning tabular model. Their method involved far less feature engineering, based on domain knowledge, than those of the other gold medalists. The paper, &quot;Entity Embeddings of Categorical Variables&quot; describes their approach. In an online-only chapter on the book&#39;s website we show how to replicate it from scratch and attain the same accuracy shown in the paper. In the abstract of the paper the authors (Cheng Guo and Felix Berkhahn) say: . :Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables... [It] is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit... As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering. . We have already noticed all of these points when we built our collaborative filtering model. We can clearly see that these insights go far beyond just collaborative filtering, however. . The paper also points out that (as we discussed in the last chapter) an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-encoded input layer. The authors used the diagram in &lt;&gt; to show this equivalence. Note that &quot;dense layer&quot; is a term with the same meaning as &quot;linear layer,&quot; and the one-hot encoding layers represent inputs. . Note: The paper also points out that (as we discussed in the last chapter) an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-encoded input layer. (remember matrix multiplication of user and movie id&#8217;s from collaborative filtering chapter) &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Entity embeddings in a neural network (courtesy of Cheng Guo and Felix Berkhahn) . The insight is important because we already know how to train linear layers, so this shows that from the point of view of the architecture and our training algorithm the embedding layer is just another layer. We also saw this in practice in the last chapter, when we built a collaborative filtering neural network that looks exactly like this diagram. . Where we analyzed the embedding weights for movie reviews, the authors of the entity embeddings paper analyzed the embedding weights for their sales prediction model. What they found was quite amazing, and illustrates their second key insight. This is that the embedding transforms the categorical variables into inputs that are both continuous and meaningful. . The images in &lt;&gt; illustrate these ideas. They are based on the approaches used in the paper, along with some analysis we have added.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; State embeddings and map (courtesy of Cheng Guo and Felix Berkhahn) . On the left is a plot of the embedding matrix for the possible values of the State category. For a categorical variable we call the possible values of the variable its &quot;levels&quot; (or &quot;categories&quot; or &quot;classes&quot;), so here one level is &quot;Berlin,&quot; another is &quot;Hamburg,&quot; etc. On the right is a map of Germany. The actual physical locations of the German states were not part of the provided data, yet the model itself learned where they must be, based only on the behavior of store sales! . Do you remember how we talked about distance between embeddings? The authors of the paper plotted the distance between store embeddings against the actual geographic distance between the stores (see &lt;&gt;). They found that they matched very closely!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Store distances (courtesy of Cheng Guo and Felix Berkhahn) . We&#39;ve even tried plotting the embeddings for days of the week and months of the year, and found that days and months that are near each other on the calendar ended up close as embeddings too, as shown in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Date embeddings . What stands out in these two examples is that we provide the model fundamentally categorical data about discrete entities (e.g., German states or days of the week), and then the model learns an embedding for these entities that defines a continuous notion of distance between them. Because the embedding distance was learned based on real patterns in the data, that distance tends to match up with our intuitions. . In addition, it is valuable in its own right that embeddings are continuous, because models are better at understanding continuous variables. This is unsurprising considering models are built of many continuous parameter weights and continuous activation values, which are updated via gradient descent (a learning algorithm for finding the minimums of continuous functions). . Another benefit is that we can combine our continuous embedding values with truly continuous input data in a straightforward manner: we just concatenate the variables, and feed the concatenation into our first dense layer. In other words, the raw categorical data is transformed by an embedding layer before it interacts with the raw continuous input data. This is how fastai and Guo and Berkhahn handle tabular models containing continuous and categorical variables. . An example using this concatenation approach is how Google does its recommendations on Google Play, as explained in the paper &quot;Wide &amp; Deep Learning for Recommender Systems&quot;. &lt;&gt; illustrates.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Google Play recommendation system . Interestingly, the Google team actually combined both approaches we saw in the previous chapter: the dot product (which they call cross product) and neural network approaches. . Let&#39;s pause for a moment. So far, the solution to all of our modeling problems has been: train a deep learning model. And indeed, that is a pretty good rule of thumb for complex unstructured data like images, sounds, natural language text, and so forth. Deep learning also works very well for collaborative filtering. But it is not always the best starting point for analyzing tabular data. . Beyond Deep Learning . Most machine learning courses will throw dozens of different algorithms at you, with a brief technical description of the math behind them and maybe a toy example. You&#39;re left confused by the enormous range of techniques shown and have little practical understanding of how to apply them. . The good news is that modern machine learning can be distilled down to a couple of key techniques that are widely applicable. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods: . Ensembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies) | Multilayered neural networks learned with SGD (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language) | Although deep learning is nearly always clearly superior for unstructured data, these two approaches tend to give quite similar results for many kinds of structured data. But ensembles of decision trees tend to train faster, are often easier to interpret, do not require special GPU hardware for inference at scale, and often require less hyperparameter tuning. They have also been popular for quite a lot longer than deep learning, so there is a more mature ecosystem of tooling and documentation around them. . Most importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles. There are tools and methods for answering the pertinent questions, like: Which columns in the dataset were the most important for your predictions? How are they related to the dependent variable? How do they interact with each other? And which particular features were most important for some particular observation? . Therefore, ensembles of decision trees are our first approach for analyzing a new tabular dataset. . The exception to this guideline is when the dataset meets one of these conditions: . There are some high-cardinality categorical variables that are very important (&quot;cardinality&quot; refers to the number of discrete levels representing categories, so a high-cardinality categorical variable is something like a zip code, which can take on thousands of possible levels). | There are some columns that contain data that would be best understood with a neural network, such as plain text data. | . In practice, when we deal with datasets that meet these exceptional conditions, we always try both decision tree ensembles and deep learning to see which works best. It is likely that deep learning will be a useful approach in our example of collaborative filtering, as we have at least two high-cardinality categorical variables: the users and the movies. But in practice things tend to be less cut-and-dried, and there will often be a mixture of high- and low-cardinality categorical variables and continuous variables. . Either way, it&#39;s clear that we are going to need to add decision tree ensembles to our modeling toolbox! . Up to now we&#39;ve used PyTorch and fastai for pretty much all of our heavy lifting. But these libraries are mainly designed for algorithms that do lots of matrix multiplication and derivatives (that is, stuff like deep learning!). Decision trees don&#39;t depend on these operations at all, so PyTorch isn&#39;t much use. . Instead, we will be largely relying on a library called scikit-learn (also known as sklearn). Scikit-learn is a popular library for creating machine learning models, using approaches that are not covered by deep learning. In addition, we&#39;ll need to do some tabular data processing and querying, so we&#39;ll want to use the Pandas library. Finally, we&#39;ll also need NumPy, since that&#39;s the main numeric programming library that both sklearn and Pandas rely on. . We don&#39;t have time to do a deep dive into all these libraries in this book, so we&#39;ll just be touching on some of the main parts of each. For a far more in depth discussion, we strongly suggest Wes McKinney&#39;s Python for Data Analysis (O&#39;Reilly). Wes is the creator of Pandas, so you can be sure that the information is accurate! . First, let&#39;s gather the data we will use. . The Dataset . The dataset we use in this chapter is from the Blue Book for Bulldozers Kaggle competition, which has the following description: &quot;The goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.&quot; . This is a very common type of dataset and prediction problem, similar to what you may see in your project or workplace. The dataset is available for download on Kaggle, a website that hosts data science competitions. . Kaggle Competitions . Kaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills. There is nothing like getting hands-on practice and receiving real-time feedback to help you improve your skills. . Kaggle provides: . Interesting datasets | Feedback on how you&#39;re doing | A leaderboard to see what&#39;s good, what&#39;s possible, and what&#39;s state-of-the-art | Blog posts by winning contestants sharing useful tips and techniques | . Until now all our datasets have been available to download through fastai&#39;s integrated dataset system. However, the dataset we will be using in this chapter is only available from Kaggle. Therefore, you will need to register on the site, then go to the page for the competition. On that page click &quot;Rules,&quot; then &quot;I Understand and Accept.&quot; (Although the competition has finished, and you will not be entering it, you still have to agree to the rules to be allowed to download the data.) . The easiest way to download Kaggle datasets is to use the Kaggle API. You can install this using pip by running this in a notebook cell: . !pip install kaggle . You need an API key to use the Kaggle API; to get one, click on your profile picture on the Kaggle website, and choose My Account, then click Create New API Token. This will save a file called kaggle.json to your PC. You need to copy this key on your GPU server. To do so, open the file you downloaded, copy the contents, and paste them in the following cell in the notebook associated with this chapter (e.g., creds = &#39;{&quot;username&quot;:&quot;xxx&quot;,&quot;key&quot;:&quot;xxx&quot;}&#39;): . creds = &#39;{&quot;username&quot;:&quot;nikemr&quot;,&quot;key&quot;:&quot;33d71d96879caa193ff2e354a50bed97&quot;}&#39; . Then execute this cell (this only needs to be run once): . cred_path = Path(&#39;~/.kaggle/kaggle.json&#39;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write_text(creds) cred_path.chmod(0o600) . Now you can download datasets from Kaggle! Pick a path to download the dataset to: . path = URLs.path(&#39;bluebook&#39;) path . Path(&#39;/home/niyazi/.fastai/archive/bluebook&#39;) . path.ls() . (#15) [Path(&#39;TrainAndValid.zip&#39;),Path(&#39;Data Dictionary.xlsx&#39;),Path(&#39;Valid.csv&#39;),Path(&#39;median_benchmark.csv&#39;),Path(&#39;TrainAndValid.7z&#39;),Path(&#39;Valid.7z&#39;),Path(&#39;Machine_Appendix.csv&#39;),Path(&#39;Train.7z&#39;),Path(&#39;ValidSolution.csv&#39;),Path(&#39;Train.zip&#39;)...] . And use the Kaggle API to download the dataset to that path, and extract it: . if not path.exists(): path.mkdir(parents=true) api.competition_download_cli(&#39;bluebook-for-bulldozers&#39;, path=path) file_extract(path/&#39;bluebook-for-bulldozers.zip&#39;) path.ls(file_type=&#39;text&#39;) . bluebook-for-bulldozers.zip: Skipping, found more recently modified local copy (use --force to force download) . NameError Traceback (most recent call last) &lt;ipython-input-9-a281f1d20580&gt; in &lt;module&gt; 2 path.mkdir(parents=true) 3 api.competition_download_cli(&#39;bluebook-for-bulldozers&#39;, path=path) -&gt; 4 file_extract(path/&#39;bluebook-for-bulldozers.zip&#39;) 5 6 path.ls(file_type=&#39;text&#39;) NameError: name &#39;file_extract&#39; is not defined . file_extract(path/&#39;bluebook-for-bulldozers.zip&#39;) . NameError Traceback (most recent call last) &lt;ipython-input-10-8dd366724759&gt; in &lt;module&gt; -&gt; 1 file_extract(path/&#39;bluebook-for-bulldozers.zip&#39;) NameError: name &#39;file_extract&#39; is not defined . Now that we have downloaded our dataset, let&#39;s take a look at it! . . Warning: Not so fast!. I&#8217;ve got errors while trying to download the dataset son check this link for solutions: https://forums.fast.ai/t/kaggle-api-issue/77213/3 and this https://githubmemory.com/repo/fastai/fastbook/issues/468 In my case first I couldn&#39;t download the dataset then second I&#39;ve got NameError: name &#39;file_extract&#39; is not defined&#39; and other people have totaly different errors. . . Tip: you can download directly from Kaggle and put into correct directory and unzip it: NameError: name &#39;file_extract&#39; is not defined unzip bluebook-for-bulldozers.zip . Look at the Data . Kaggle provides information about some of the fields of our dataset. The Data explains that the key fields in train.csv are: . SalesID:: The unique identifier of the sale. | MachineID:: The unique identifier of a machine. A machine can be sold multiple times. | saleprice:: What the machine sold for at auction (only provided in train.csv). | saledate:: The date of the sale. | . In any sort of data science work, it&#39;s important to look at your data directly to make sure you understand the format, how it&#39;s stored, what types of values it holds, etc. Even if you&#39;ve read a description of the data, the actual data may not be what you expect. We&#39;ll start by reading the training set into a Pandas DataFrame. Generally it&#39;s a good idea to specify low_memory=False unless Pandas actually runs out of memory and returns an error. The low_memory parameter, which is True by default, tells Pandas to only look at a few rows of data at a time to figure out what type of data is in each column. This means that Pandas can actually end up using different data type for different rows, which generally leads to data processing errors or model training problems later. . Let&#39;s load our data and have a look at the columns: . . Tip: check above cell for low_memory=False . df = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) . df.columns . Index([&#39;SalesID&#39;, &#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;) . That&#39;s a lot of columns for us to look at! Try looking through the dataset to get a sense of what kind of information is in each one. We&#39;ll shortly see how to &quot;zero in&quot; on the most interesting bits. . At this point, a good next step is to handle ordinal columns. This refers to columns containing strings or similar, but where those strings have a natural ordering. For instance, here are the levels of ProductSize: . df[&#39;ProductSize&#39;].unique() . array([nan, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;], dtype=object) . We can tell Pandas about a suitable ordering of these levels like so: . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; . df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) . . Tip: Now we have set categories for Product size. Check it again. . df[&#39;ProductSize&#39;].unique() . [NaN, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;] Categories (6, object): [&#39;Large&#39; &lt; &#39;Large / Medium&#39; &lt; &#39;Medium&#39; &lt; &#39;Small&#39; &lt; &#39;Mini&#39; &lt; &#39;Compact&#39;] . The most important data column is the dependent variable—that is, the one we want to predict. Recall that a model&#39;s metric is a function that reflects how good the predictions are. It&#39;s important to note what metric is being used for a project. Generally, selecting the metric is an important part of the project setup. In many cases, choosing a good metric will require more than just selecting a variable that already exists. It is more like a design process. You should think carefully about which metric, or set of metrics, actually measures the notion of model quality that matters to you. If no variable represents that metric, you should see if you can build the metric from the variables that are available. . However, in this case Kaggle tells us what metric to use: root mean squared log error (RMSLE) between the actual and predicted auction prices. We need do only a small amount of processing to use this: we take the log of the prices, so that rmse of that value will give us what we ultimately need: . dep_var = &#39;SalePrice&#39; . df[dep_var] = np.log(df[dep_var]) . We are now ready to explore our first machine learning algorithm for tabular data: decision trees. . Decision Trees . Decision tree ensembles, as the name suggests, rely on decision trees. So let&#39;s start there! A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a &quot;yes&quot; and a &quot;no&quot; branch, as shown in &lt;&gt;. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; An example of decision tree . This sequence of questions is now a procedure for taking any data item, whether an item from the training set or a new one, and assigning that item to a group. Namely, after asking and answering the questions, we can say the item belongs to the same group as all the other training data items that yielded the same set of answers to the questions. But what good is this? The goal of our model is to predict values for items, not to assign them into groups from the training dataset. The value is that we can now assign a prediction value for each of these groups—for regression, we take the target mean of the items in the group. . Let&#39;s consider how we find the right questions to ask. Of course, we wouldn&#39;t want to have to create all these questions ourselves—that&#39;s what computers are for! The basic steps to train a decision tree can be written down very easily: . Loop through each column of the dataset in turn. | For each column, loop through each possible level of that column in turn. . Note: What does tha mean(above)? 1. Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable). | Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple &quot;model&quot; where our predictions are simply the average sale price of the item&#39;s group. | After looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model. | We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group. | Continue this process recursively, until you have reached some stopping criterion for each group—for instance, stop splitting a group further when it has only 20 items in it. | Although this is an easy enough algorithm to implement yourself (and it is a good exercise to do so), we can save some time by using the implementation built into sklearn. . First, however, we need to do a little data preparation. . . Note: Additional information for understanding the loop above: https://www.youtube.com/watch?v=7VeUPuFGJHk . . Note: new version of above video. https://www.youtube.com/watch?v=_L39rN6gz7Y . A:Here&#39;s a productive question to ponder. If you consider that the procedure for defining a decision tree essentially chooses one sequence of splitting questions about variables, you might ask yourself, how do we know this procedure chooses the correct sequence? The rule is to choose the splitting question that produces the best split (i.e., that most accurately separates the items into two distinct categories), and then to apply the same rule to the groups that split produces, and so on. This is known in computer science as a &quot;greedy&quot; approach. Can you imagine a scenario in which asking a “less powerful” splitting question would enable a better split down the road (or should I say down the trunk!) and lead to a better result overall? . . Note: &quot;This is known in computer science as a &quot;greedy&quot; approach&quot;, wow very good touch. It doesnt&#8217;t work some of important real world problems. . Handling Dates . The first piece of data preparation we need to do is to enrich our representation of dates. The fundamental basis of the decision tree that we just described is bisection— dividing a group into two. We look at the ordinal variables and divide up the dataset based on whether the variable&#39;s value is greater (or lower) than a threshold, and we look at the categorical variables and divide up the dataset based on whether the variable&#39;s level is a particular level. So this algorithm has a way of dividing up the dataset based on both ordinal and categorical data. . But how does this apply to a common data type, the date? You might want to treat a date as an ordinal value, because it is meaningful to say that one date is greater than another. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others in a way that that is often relevant to the systems we are modeling. . In order to help our algorithm handle dates intelligently, we&#39;d like our model to know more than whether a date is more recent or less recent than another. We might want our model to make decisions based on that date&#39;s day of the week, on whether a day is a holiday, on what month it is in, and so forth. To do this, we replace every date column with a set of date metadata columns, such as holiday, day of week, and month. These columns provide categorical data that we suspect will be useful. . fastai comes with a function that will do this for us—we just have to pass a column name that contains dates: . . Note: &quot;we replace every date column with a set of date metadata columns, such as holiday, day of week, and month&quot; I find this above important. . df = add_datepart(df, &#39;saledate&#39;) . Let&#39;s do the same for the test set while we&#39;re there: . df_test = pd.read_csv(path/&#39;Test.csv&#39;, low_memory=False) df_test = add_datepart(df_test, &#39;saledate&#39;) . We can see that there are now lots of new columns in our DataFrame: . &#39; &#39;.join(o for o in df.columns if o.startswith(&#39;sale&#39;)) . &#39;saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed&#39; . This is a good first step, but we will need to do a bit more cleaning. For this, we will use fastai objects called TabularPandas and TabularProc. . Using TabularPandas and TabularProc . A second piece of preparatory processing is to be sure we can handle strings and missing data. Out of the box, sklearn cannot do either. Instead we will use fastai&#39;s class TabularPandas, which wraps a Pandas DataFrame and provides a few conveniences. To populate a TabularPandas, we will use two TabularProcs, Categorify and FillMissing. A TabularProc is like a regular Transform, except that: . It returns the exact same object that&#39;s passed to it, after modifying the object in place. | It runs the transform once, when data is first passed in, rather than lazily as the data is accessed. | . Categorify is a TabularProc that replaces a column with a numeric categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing. These two transforms are needed for nearly every tabular dataset you will use, so this is a good starting point for your data processing: . procs = [Categorify, FillMissing] . TabularPandas will also handle splitting the dataset into training and validation sets for us. However we need to be very careful about our validation set. We want to design it so that it is like the test set Kaggle will use to judge the contest. . Recall the distinction between a validation set and a test set, as discussed in &lt;&gt;. A validation set is data we hold back from training in order to ensure that the training process does not overfit on the training data. A test set is data that is held back even more deeply, from us ourselves, in order to ensure that we don&#39;t overfit on the validation data, as we explore various model architectures and hyperparameters.&lt;/p&gt; We don&#39;t get to see the test set. But we do want to define our validation data so that it has the same sort of relationship to the training data as the test set will have. . In some cases, just randomly choosing a subset of your data points will do that. This is not one of those cases, because it is a time series. . If you look at the date range represented in the test set, you will discover that it covers a six-month period from May 2012, which is later in time than any date in the training set. This is a good design, because the competition sponsor will want to ensure that a model is able to predict the future. But it means that if we are going to have a useful validation set, we also want the validation set to be later in time than the training set. The Kaggle training data ends in April 2012, so we will define a narrower training dataset which consists only of the Kaggle training data from before November 2011, and we&#39;ll define a validation set consisting of data from after November 2011. . To do this we use np.where, a useful function that returns (as the first element of a tuple) the indices of all True values: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; cond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) . TabularPandas needs to be told which columns are continuous and which are categorical. We can handle that automatically using the helper function cont_cat_split: . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . A TabularPandas behaves a lot like a fastai Datasets object, including providing train and valid attributes: . len(to.train),len(to.valid) . (404710, 7988) . We can see that the data is still displayed as strings for categories (we only show a few columns here because the full table is too big to fit on a page): . to.show(3) . state ProductGroup Drive_System Enclosure SalePrice . 0 Alabama | WL | #na# | EROPS w AC | 11.097410 | . 1 North Carolina | WL | #na# | EROPS w AC | 10.950807 | . 2 New York | SSL | #na# | OROPS | 9.210340 | . However, the underlying items are all numeric: . to.items.head(3) . state ProductGroup Drive_System Enclosure . 0 1 | 6 | 0 | 3 | . 1 33 | 6 | 0 | 3 | . 2 32 | 3 | 0 | 6 | . The conversion of categorical columns to numbers is done by simply replacing each unique level with a number. The numbers associated with the levels are chosen consecutively as they are seen in a column, so there&#39;s no particular meaning to the numbers in categorical columns after conversion. The exception is if you first convert a column to a Pandas ordered category (as we did for ProductSize earlier), in which case the ordering you chose is used. We can see the mapping by looking at the classes attribute: . . Note: It is integers, I think the reason is they are not used for &quot;learning&quot; through backprop SDG etc. . to.classes[&#39;ProductSize&#39;] . [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;] . Since it takes a minute or so to process the data to get to this point, we should save it—that way in the future we can continue our work from here without rerunning the previous steps. fastai provides a save method that uses Python&#39;s pickle system to save nearly any Python object: . save_pickle(path/&#39;to.pkl&#39;,to) . To read this back later, you would type: . to = (path/&#39;to.pkl&#39;).load() . Now that all this preprocessing is done, we are ready to create a decision tree. . Creating the Decision Tree . To begin, we define our independent and dependent variables: . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Now that our data is all numeric, and there are no missing values, we can create a decision tree: . m = DecisionTreeRegressor(max_leaf_nodes=4) m.fit(xs, y); . To keep it simple, we&#39;ve told sklearn to just create four leaf nodes. To see what it&#39;s learned, we can display the tree: . draw_tree(m, xs, size=10, leaves_parallel=True, precision=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 Coupler_System ≤ 0.5 mse = 0.48 samples = 404710 value = 10.1 1 YearMade ≤ 1991.5 mse = 0.42 samples = 360847 value = 10.21 0&#45;&gt;1 True 2 mse = 0.12 samples = 43863 value = 9.21 0&#45;&gt;2 False 3 mse = 0.37 samples = 155724 value = 9.97 1&#45;&gt;3 4 ProductSize ≤ 4.5 mse = 0.37 samples = 205123 value = 10.4 1&#45;&gt;4 5 mse = 0.31 samples = 182403 value = 10.5 4&#45;&gt;5 6 mse = 0.17 samples = 22720 value = 9.62 4&#45;&gt;6 Understanding this picture is one of the best ways to understand decision trees, so we will start at the top and explain each part step by step. . The top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remember that unless you see m_rmse, or a root mean squared error, then the value you are looking at is before taking the square root, so it is just the average of the square of the differences.) We can also see that there are 404,710 auction records in this group—that is the total size of our training set. The final piece of information shown here is the decision criterion for the best split that was found, which is to split based on the coupler_system column. . Moving down and to the left, this node shows us that there were 360,847 auction records for equipment where coupler_system was less than 0.5. The average value of our dependent variable in this group is 10.21. Moving down and to the right from the initial model takes us to the records where coupler_system was greater than 0.5. . The bottom row contains our leaf nodes: the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node containing records where coupler_system was greater than 0.5. The average value here is 9.21, so we can see the decision tree algorithm did find a single binary decision that separated high-value from low-value auction results. Asking only about coupler_system predicts an average value of 9.21 versus 10.1. . Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether YearMade is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, based on coupler_system and YearMade) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly. . We can show the same information using Terence Parr&#39;s powerful dtreeviz library: . samp_idx = np.random.permutation(len(y))[:500] dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 2021-10-27T11:06:41.559979 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ leaf5 2021-10-27T11:06:42.047131 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node4-&gt;leaf5 leaf6 2021-10-27T11:06:42.116196 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node4-&gt;leaf6 node1 2021-10-27T11:06:41.660614 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node1-&gt;node4 leaf3 2021-10-27T11:06:41.969824 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node1-&gt;leaf3 leaf2 2021-10-27T11:06:42.178326 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node0 2021-10-27T11:06:41.773121 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node0-&gt;node1 &#8804; node0-&gt;leaf2 &gt; This shows a chart of the distribution of the data for each split point. We can clearly see that there&#39;s a problem with our YearMade data: there are bulldozers made in the year 1000, apparently! Presumably this is actually just a missing value code (a value that doesn&#39;t otherwise appear in the data and that is used as a placeholder in cases where a value is missing). For modeling purposes, 1000 is fine, but as you can see this outlier makes visualization of the values we are interested in more difficult. So, let&#39;s replace it with 1950: . xs.loc[xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 valid_xs.loc[valid_xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 . That change makes the split much clearer in the tree visualization, even although it doesn&#39;t actually change the result of the model in any significant way. This is a great example of how resilient decision trees are to data issues! . m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y) dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 2021-10-27T12:47:13.413121 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ leaf5 2021-10-27T12:47:13.779827 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node4-&gt;leaf5 leaf6 2021-10-27T12:47:13.847579 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node4-&gt;leaf6 node1 2021-10-27T12:47:13.506539 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node1-&gt;node4 leaf3 2021-10-27T12:47:13.707468 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node1-&gt;leaf3 leaf2 2021-10-27T12:47:13.908752 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node0 2021-10-27T12:47:13.613369 image/svg+xml Matplotlib v3.3.4, https://matplotlib.org/ node0-&gt;node1 &#8804; node0-&gt;leaf2 &gt; Let&#39;s now have the decision tree algorithm build a bigger tree. Here, we are not passing in any stopping criteria such as max_leaf_nodes: . m = DecisionTreeRegressor() m.fit(xs, y); . We&#39;ll create a little function to check the root mean squared error of our model (m_rmse), since that&#39;s how the competition was judged: . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . m_rmse(m, xs, y) . 0.0 . So, our model is perfect, right? Not so fast... remember we really need to check the validation set, to ensure we&#39;re not overfitting: . . Note: Overfitting here! . m_rmse(m, valid_xs, valid_y) . 0.334935 . Oops—it looks like we might be overfitting pretty badly. Here&#39;s why: . m.get_n_leaves(), len(xs) . (324560, 404710) . We&#39;ve got nearly as many leaf nodes as data points! That seems a little over-enthusiastic. Indeed, sklearn&#39;s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let&#39;s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 auction records: . m = DecisionTreeRegressor(min_samples_leaf=25) m.fit(to.train.xs, to.train.y) m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.248593, 0.323339) . That looks much better. Let&#39;s check the number of leaves again: . m.get_n_leaves() . 12397 . Much more reasonable! . A:Here&#39;s my intuition for an overfitting decision tree with more leaf nodes than data items. Consider the game Twenty Questions. In that game, the chooser secretly imagines an object (like, &quot;our television set&quot;), and the guesser gets to pose 20 yes or no questions to try to guess what the object is (like &quot;Is it bigger than a breadbox?&quot;). The guesser is not trying to predict a numerical value, but just to identify a particular object out of the set of all imaginable objects. When your decision tree has more leaves than there are possible objects in your domain, then it is essentially a well-trained guesser. It has learned the sequence of questions needed to identify a particular data item in the training set, and it is &quot;predicting&quot; only by describing that item&#39;s value. This is a way of memorizing the training set—i.e., of overfitting. . Building a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees). . So how do we get the best of both worlds? We&#39;ll show you right after we handle an important missing detail: how to handle categorical variables. . Categorical Variables . In the previous chapter, when working with deep learning networks, we dealt with categorical variables by one-hot encoding them and feeding them to an embedding layer. The embedding layer helped the model to discover the meaning of the different levels of these variables (the levels of a categorical variable do not have an intrinsic meaning, unless we manually specify an ordering using Pandas). In a decision tree, we don&#39;t have embeddings layers—so how can these untreated categorical variables do anything useful in a decision tree? For instance, how could something like a product code be used? . The short answer is: it just works! Think about a situation where there is one product code that is far more expensive at auction than any other one. In that case, any binary split will result in that one product code being in some group, and that group will be more expensive than the other group. Therefore, our simple decision tree building algorithm will choose that split. Later during training the algorithm will be able to further split the subgroup that contains the expensive product code, and over time, the tree will home in on that one expensive product. . It is also possible to use one-hot encoding to replace a single categorical variable with multiple one-hot-encoded columns, where each column represents a possible level of the variable. Pandas has a get_dummies method which does just that. . However, there is not really any evidence that such an approach improves the end result. So, we generally avoid it where possible, because it does end up making your dataset harder to work with. In 2019 this issue was explored in the paper &quot;Splitting on Categorical Predictors in Random Forests&quot; by Marvin Wright and Inke König, which said: . :The standard approach for nominal predictors is to consider all $2^{k-1} − 1$ 2-partitions of the k predictor categories. However, this exponential relationship produces a large number of potential splits to be evaluated, increasing computational complexity and restricting the possible number of categories in most implementations. For binary classification and regression, it was shown that ordering the predictor categories in each split leads to exactly the same splits as the standard approach. This reduces computational complexity because only k − 1 splits have to be considered for a nominal predictor with k categories. . . Warning: Do not understand exactly.(above) . Now that you understand how decisions tree work, it&#39;s time for the best-of-both-worlds solution: random forests. . Random Forests . In 1994 Berkeley professor Leo Breiman, one year after his retirement, published a small technical report called &quot;Bagging Predictors&quot;, which turned out to be one of the most influential ideas in modern machine learning. The report began: . :Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions... The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests… show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. Here is the procedure that Breiman is proposing: . Randomly choose a subset of the rows of your data (i.e., &quot;bootstrap replicates of your learning set&quot;). | Train a model using this subset. | Save that model, and then return to step 1 a few times. | This will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each of those model&#39;s predictions. | This procedure is known as &quot;bagging.&quot; It is based on a deep and important insight: although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. Different models will make different errors. The average of those errors, therefore, is: zero! So if we take the average of all of the models&#39; predictions, then we should end up with a prediction that gets closer and closer to the correct answer, the more models we have. This is an extraordinary result—it means that we can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of the data, and averaging its predictions. . In 2001 Leo Breiman went on to demonstrate that this approach to building models, when applied to decision tree building algorithms, was particularly powerful. He went even further than just randomly choosing rows for each model&#39;s training, but also randomly selected from a subset of columns when choosing each split in each decision tree. He called this method the random forest. Today it is, perhaps, the most widely used and practically important machine learning method. . In essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. Bagging is a particular approach to &quot;ensembling,&quot; or combining the results of multiple models together. To see how it works in practice, let&#39;s get started on creating our own random forest! . Creating a Random Forest . We can create a random forest just like we created a decision tree, except now, we are also specifying parameters that indicate how many trees should be in the forest, how we should subset the data items (the rows), and how we should subset the fields (the columns). . In the following function definition n_estimators defines the number of trees we want, max_samples defines how many rows to sample for training each tree, and max_features defines how many columns to sample at each split point (where 0.5 means &quot;take half the total number of columns&quot;). We can also specify when to stop splitting the tree nodes, effectively limiting the depth of the tree, by including the same min_samples_leaf parameter we used in the last section. Finally, we pass n_jobs=-1 to tell sklearn to use all our CPUs to build the trees in parallel. By creating a little function for this, we can more quickly try different variations in the rest of this chapter: . . Note: I guess we still use greedy method. Picking best split for the first node then create models randomly. (is it?) . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . m = rf(xs, y); . Our validation RMSE is now much improved over our last result produced by the DecisionTreeRegressor, which made just one tree using all the available data: . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.170966, 0.232699) . One of the most important properties of random forests is that they aren&#39;t very sensitive to the hyperparameter choices, such as max_features. You can set n_estimators to as high a number as you have time to train—the more trees you have, the more accurate the model will be. max_samples can often be left at its default, unless you have over 200,000 data points, in which case setting it to 200,000 will make it train faster with little impact on accuracy. max_features=0.5 and min_samples_leaf=4 both tend to work well, although sklearn&#39;s defaults work well too. . The sklearn docs show an example of the effects of different max_features choices, with increasing numbers of trees. In the plot, the blue plot line uses the fewest features and the green line uses the most (it uses all the features). As you can see in &lt;&gt;, the models with the lowest error result from using a subset of features but with a larger number of trees.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Note: Very interesting result. Lowest features+highest number of trees ! . . To see the impact of n_estimators, let&#39;s get the predictions from each individual tree in our forest (these are in the estimators_ attribute): . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . As you can see, preds.mean(0) gives the same results as our random forest: . r_mse(preds.mean(0), valid_y) . 0.232699 . Let&#39;s see what happens to the RMSE as we add more and more trees. As you can see, the improvement levels off quite a bit after around 30 trees: . plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]); . The performance on our validation set is worse than on our training set. But is that because we&#39;re overfitting, or because the validation set covers a different time period, or a bit of both? With the existing information we&#39;ve seen, we can&#39;t tell. However, random forests have a very clever trick called out-of-bag (OOB) error that can help us with this (and more!). . Out-of-Bag Error . Recall that in a random forest, each tree is trained on a different subset of the training data. The OOB error is a way of measuring prediction error on the training set by only including in the calculation of a row&#39;s error trees where that row was not included in training. This allows us to see whether the model is overfitting, without needing a separate validation set. . A:My intuition for this is that, since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set. That validation set is simply the rows that were not selected for that tree&#39;s training. This is particularly beneficial in cases where we have only a small amount of training data, as it allows us to see whether our model generalizes without removing items to create a validation set. The OOB predictions are available in the oob_prediction_ attribute. Note that we compare them to the training labels, since this is being calculated on trees using the training set. . r_mse(m.oob_prediction_, y) . 0.210776 . We can see that our OOB error is much lower than our validation set error. This means that something else is causing that error, in addition to normal generalization error. We&#39;ll discuss the reasons for this later in this chapter. . This is one way to interpret our model&#39;s predictions—let&#39;s focus on more of those now. . Model Interpretation . For tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are: . How confident are we in our predictions using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors, which can we ignore? | Which columns are effectively redundant with each other, for purposes of prediction? | How do predictions vary, as we vary these columns? | . As we will see, random forests are particularly well suited to answering these questions. Let&#39;s start with the first one! . Tree Variance for Prediction Confidence . We saw how the model averages the individual tree&#39;s predictions to get an overall prediction—that is, an estimate of the value. But how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions across the trees, instead of just the mean. This tells us the relative confidence of predictions. In general, we would want to be more cautious of using the results for rows where trees give very different results (higher standard deviations), compared to cases where they are more consistent (lower standard deviations). . In the earlier section on creating a random forest, we saw how to get predictions over the validation set, using a Python list comprehension to do this for each tree in the forest: . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . . Note: Accessing every tree is great. . preds.shape . (40, 7988) . Now we have a prediction for every tree and every auction (40 trees and 7,988 auctions) in the validation set. . Using this we can get the standard deviation of the predictions over all the trees, for each auction: . preds_std = preds.std(0) . Here are the standard deviations for the predictions for the first five auctions—that is, the first five rows of the validation set: . preds_std[:5] . array([0.26069358, 0.10409366, 0.09904178, 0.27184634, 0.13110276]) . As you can see, the confidence in the predictions varies widely. For some auctions, there is a low standard deviation because the trees agree. For others it&#39;s higher, as the trees don&#39;t agree. This is information that would be useful in a production setting; for instance, if you were using this model to decide what items to bid on at auction, a low-confidence prediction might cause you to look more carefully at an item before you made a bid. . Feature Importance . It&#39;s not normally enough just to know that a model can make accurate predictions—we also want to know how it&#39;s making predictions. feature importance gives us insight into this. We can get these directly from sklearn&#39;s random forest by looking in the feature_importances_ attribute. Here&#39;s a simple function we can use to pop them into a DataFrame and sort them: . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . The feature importances for our model show that the first few most important columns have much higher importance scores than the rest, with (not surprisingly) YearMade and ProductSize being at the top of the list: . fi = rf_feat_importance(m, xs) fi[:10] . cols imp . 57 YearMade | 0.173023 | . 6 ProductSize | 0.117253 | . 30 Coupler_System | 0.117053 | . 7 fiProductClassDesc | 0.073112 | . 54 ModelID | 0.054777 | . 65 saleElapsed | 0.048835 | . 3 fiSecondaryDesc | 0.046104 | . 31 Grouser_Tracks | 0.041196 | . 12 Enclosure | 0.040495 | . 32 Hydraulics_Flow | 0.032725 | . A plot of the feature importances shows the relative importances more clearly: . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . The way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1. . Removing Low-Importance Variables . It seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let&#39;s try just keeping those with a feature importance greater than 0.005: . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) . 21 . We can retrain our model using just this subset of the columns: . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . m = rf(xs_imp, y) . And here&#39;s the result: . m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y) . (0.18131, 0.230503) . Our accuracy is about the same, but we have far fewer columns to study: . len(xs.columns), len(xs_imp.columns) . (66, 21) . We&#39;ve found that generally the first step to improving a model is simplifying it—78 columns was too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain. . This also makes our feature importance plot easier to interpret. Let&#39;s look at it again: . plot_fi(rf_feat_importance(m, xs_imp)); . One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings: for example, ProductGroup and ProductGroupDesc. Let&#39;s try to remove any redundent features. . Removing Redundant Features . Let&#39;s start with: . cluster_columns(xs_imp) . In this chart, the pairs of columns that are most similar are the ones that were merged together early, far from the &quot;root&quot; of the tree at the left. Unsurprisingly, the fields ProductGroup and ProductGroupDesc were merged quite early, as were saleYear and saleElapsed and fiModelDesc and fiBaseModel. These might be so closely correlated they are practically synonyms for each other. . Note: Determining Similarity: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e., first, second, third, etc. within the column), and then the correlation is calculated. (Feel free to skip over this minor detail though, since it&#8217;s not going to come up again in the book!) Let&#39;s try removing some of these closely related features to see if the model can be simplified without impacting the accuracy. First, we create a function that quickly trains a random forest and returns the OOB score, by using a lower max_samples and higher min_samples_leaf. The OOB score is a number returned by sklearn that ranges between 1.0 for a perfect model and 0.0 for a random model. (In statistics it&#39;s called R^2, although the details aren&#39;t important for this explanation.) We don&#39;t need it to be very accurate—we&#39;re just going to use it to compare different models, based on removing some of the possibly redundant columns: . def get_oob(df): m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15, max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True) m.fit(df, y) return m.oob_score_ . Here&#39;s our baseline: . get_oob(xs_imp) . 0.8781576734893485 . Now we try removing each of our potentially redundant variables, one at a time: . {c:get_oob(xs_imp.drop(c, axis=1)) for c in ( &#39;saleYear&#39;, &#39;saleElapsed&#39;, &#39;ProductGroupDesc&#39;,&#39;ProductGroup&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;Hydraulics_Flow&#39;,&#39;Grouser_Tracks&#39;, &#39;Coupler_System&#39;)} . {&#39;saleYear&#39;: 0.8768628311464004, &#39;saleElapsed&#39;: 0.8722097904682757, &#39;ProductGroupDesc&#39;: 0.8770087512874477, &#39;ProductGroup&#39;: 0.8778594920344923, &#39;fiModelDesc&#39;: 0.8754781084425128, &#39;fiBaseModel&#39;: 0.8761168180455399, &#39;Hydraulics_Flow&#39;: 0.8774995916903535, &#39;Grouser_Tracks&#39;: 0.8775565092698138, &#39;Coupler_System&#39;: 0.8770165273393064} . Now let&#39;s try dropping multiple variables. We&#39;ll drop one from each of the tightly aligned pairs we noticed earlier. Let&#39;s see what that does: . to_drop = [&#39;saleYear&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;Grouser_Tracks&#39;] get_oob(xs_imp.drop(to_drop, axis=1)) . 0.8750482697068109 . Looking good! This is really not much worse than the model with all the fields. Let&#39;s create DataFrames without these columns, and save them: . xs_final = xs_imp.drop(to_drop, axis=1) valid_xs_final = valid_xs_imp.drop(to_drop, axis=1) . save_pickle(path/&#39;xs_final.pkl&#39;, xs_final) save_pickle(path/&#39;valid_xs_final.pkl&#39;, valid_xs_final) . We can load them back later with: . xs_final = load_pickle(path/&#39;xs_final.pkl&#39;) valid_xs_final = load_pickle(path/&#39;valid_xs_final.pkl&#39;) . Now we can check our RMSE again, to confirm that the accuracy hasn&#39;t substantially changed. . m = rf(xs_final, y) m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) . (0.183251, 0.232259) . By focusing on the most important variables, and removing some redundant ones, we&#39;ve greatly simplified our model. Now, let&#39;s see how those variables affect our predictions using partial dependence plots. . Partial Dependence . As we&#39;ve seen, the two most important predictors are ProductSize and YearMade. We&#39;d like to understand the relationship between these predictors and sale price. It&#39;s a good idea to first check the count of values per category (provided by the Pandas value_counts method), to see how common each category is: . p = valid_xs_final[&#39;ProductSize&#39;].value_counts(sort=False).plot.barh() c = to.classes[&#39;ProductSize&#39;] plt.yticks(range(len(c)), c); . The largrest group is #na#, which is the label fastai applies to missing values. . Let&#39;s do the same thing for YearMade. Since this is a numeric feature, we&#39;ll need to draw a histogram, which groups the year values into a few discrete bins: . ax = valid_xs_final[&#39;YearMade&#39;].hist() . Other than the special value 1950 which we used for coding missing year values, most of the data is from after 1990. . Now we&#39;re ready to look at partial dependence plots. Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? . For instance, how does YearMade impact sale price, all other things being equal? . To answer this question, we can&#39;t just take the average sale price for each YearMade. The problem with that approach is that many other things vary from year to year as well, such as which products are sold, how many products have air-conditioning, inflation, and so forth. So, merely averaging over all the auctions that have the same YearMade would also capture the effect of how every other field also changed along with YearMade and how that overall change affected price. . Instead, what we do is replace every single value in the YearMade column with 1950, and then calculate the predicted sale price for every auction, and take the average over all auctions. Then we do the same for 1951, 1952, and so forth until our final year of 2011. This isolates the effect of only YearMade (even if it does so by averaging over some imagined records where we assign a YearMade value that might never actually exist alongside some other values). . A:If you are philosophically minded it is somewhat dizzying to contemplate the different kinds of hypotheticality that we are juggling to make this calculation. First, there&#39;s the fact that every prediction is hypothetical, because we are not noting empirical data. Second, there&#39;s the point that we&#39;re not merely interested in asking how sale price would change if we changed YearMade and everything else along with it. Rather, we&#39;re very specifically asking, how sale price would change in a hypothetical world where only YearMade changed. Phew! It is impressive that we can ask such questions. I recommend Judea Pearl and Dana Mackenzie&#39;s recent book on causality, The Book of Why (Basic Books), if you&#39;re interested in more deeply exploring formalisms for analyzing these subtleties. With these averages, we can then plot each of these years on the x-axis, and each of the predictions on the y-axis. This, finally, is a partial dependence plot. Let&#39;s take a look: . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(12, 4)) plot_partial_dependence(m, valid_xs_final, [&#39;YearMade&#39;,&#39;ProductSize&#39;], grid_resolution=20, ax=ax); . Looking first of all at the YearMade plot, and specifically at the section covering the years after 1990 (since as we noted this is where we have the most data), we can see a nearly linear relationship between year and price. Remember that our dependent variable is after taking the logarithm, so this means that in practice there is an exponential increase in price. This is what we would expect: depreciation is generally recognized as being a multiplicative factor over time, so, for a given sale date, varying year made ought to show an exponential relationship with sale price. . The ProductSize partial plot is a bit concerning. It shows that the final group, which we saw is for missing values, has the lowest price. To use this insight in practice, we would want to find out why it&#39;s missing so often, and what that means. Missing values can sometimes be useful predictors—it entirely depends on what causes them to be missing. Sometimes, however, they can indicate data leakage. . Data Leakage . In the paper &quot;Leakage in Data Mining: Formulation, Detection, and Avoidance&quot;, Shachar Kaufman, Saharon Rosset, and Claudia Perlich describe leakage as: . :The introduction of information about the target of a data mining problem, which should not be legitimately available to mine from. A trivial example of leakage would be a model that uses the target itself as an input, thus concluding for example that &#39;it rains on rainy days&#39;. In practice, the introduction of this illegitimate information is unintentional, and facilitated by the data collection, aggregation and preparation process. They give as an example: :A real-life business intelligence project at IBM where potential customers for certain products were identified, among other things, based on keywords found on their websites. This turned out to be leakage since the website content used for training had been sampled at the point in time where the potential customer has already become a customer, and where the website contained traces of the IBM products purchased, such as the word &#39;Websphere&#39; (e.g., in a press release about the purchase or a specific product feature the client uses). Data leakage is subtle and can take many forms. In particular, missing values often represent data leakage. . For instance, Jeremy competed in a Kaggle competition designed to predict which researchers would end up receiving research grants. The information was provided by a university and included thousands of examples of research projects, along with information about the researchers involved and data on whether or not each grant was eventually accepted. The university hoped to be able to use the models developed in this competition to rank which grant applications were most likely to succeed, so it could prioritize its processing. . Jeremy used a random forest to model the data, and then used feature importance to find out which features were most predictive. He noticed three surprising things: . The model was able to correctly predict who would receive grants over 95% of the time. | Apparently meaningless identifier columns were the most important predictors. | The day of week and day of year columns were also highly predictive; for instance, the vast majority of grant applications dated on a Sunday were accepted, and many accepted grant applications were dated on January 1. | . For the identifier columns, one partial dependence plot per column showed that when the information was missing the application was almost always rejected. It turned out that in practice, the university only filled out much of this information after a grant application was accepted. Often, for applications that were not accepted, it was just left blank. Therefore, this information was not something that was actually available at the time that the application was received, and it would not be available for a predictive model—it was data leakage. . In the same way, the final processing of successful applications was often done automatically as a batch at the end of the week, or the end of the year. It was this final processing date which ended up in the data, so again, this information, while predictive, was not actually available at the time that the application was received. . This example showcases the most practical and simple approaches to identifying data leakage, which are to build a model and then: . Check whether the accuracy of the model is too good to be true. | Look for important predictors that don&#39;t make sense in practice. | Look for partial dependence plot results that don&#39;t make sense in practice. | . Thinking back to our bear detector, this mirrors the advice that we provided in &lt;&gt;—it is often a good idea to build a model first and then do your data cleaning, rather than vice versa. The model can help you identify potentially problematic data issues.&lt;/p&gt; It can also help you identify which factors influence specific predictions, with tree interpreters. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Tree Interpreter . At the start of this section, we said that we wanted to be able to answer five questions: . How confident are we in our predictions using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors? | Which columns are effectively redundant with each other, for purposes of prediction? | How do predictions vary, as we vary these columns? | . We&#39;ve handled four of these already; only the second question remains. To answer this question, we need to use the treeinterpreter library. We&#39;ll also use the waterfallcharts library to draw the chart of the results. . !pip install treeinterpreter !pip install waterfallcharts . We have already seen how to compute feature importances across the entire random forest. The basic idea was to look at the contribution of each variable to improving the model, at each branch of every tree, and then add up all of these contributions per variable. . We can do exactly the same thing, but for just a single row of data. For instance, let&#39;s say we are looking at some particular item at auction. Our model might predict that this item will be very expensive, and we want to know why. So, we take that one row of data and put it through the first decision tree, looking to see what split is used at each point throughout the tree. For each split, we see what the increase or decrease in the addition is, compared to the parent node of the tree. We do this for every tree, and add up the total change in importance by split variable. . For instance, let&#39;s pick the first few rows of our validation set: . row = valid_xs_final.iloc[:5] . We can then pass these to treeinterpreter: . prediction,bias,contributions = treeinterpreter.predict(m, row.values) . prediction is simply the prediction that the random forest makes. bias is the prediction based on taking the mean of the dependent variable (i.e., the model that is the root of every tree). contributions is the most interesting bit—it tells us the total change in predicition due to each of the independent variables. Therefore, the sum of contributions plus bias must equal the prediction, for each row. Let&#39;s look just at the first row: . prediction[0], bias[0], contributions[0].sum() . (array([9.94708073]), 10.104746057831763, -0.15766532528651994) . The clearest way to display the contributions is with a waterfall plot. This shows how the positive and negative contributions from all the independent variables sum up to create the final prediction, which is the righthand column labeled &quot;net&quot; here: . waterfall(valid_xs_final.columns, contributions[0], threshold=0.08, rotation_value=45,formatting=&#39;{:,.3f}&#39;); . This kind of information is most useful in production, rather than during model development. You can use it to provide useful information to users of your data product about the underlying reasoning behind the predictions. . Now that we covered some classic machine learning techniques to solve this problem, let&#39;s see how deep learning can help! . Extrapolation and Neural Networks . A problem with random forests, like all machine learning or deep learning algorithms, is that they don&#39;t always generalize well to new data. We will see in which situations neural networks generalize better, but first, let&#39;s look at the extrapolation problem that random forests have. . The Extrapolation Problem . Let&#39;s consider the simple task of making predictions from 40 data points showing a slightly noisy linear relationship: . x_lin = torch.linspace(0,20, steps=40) y_lin = x_lin + torch.randn_like(x_lin) plt.scatter(x_lin, y_lin); . Although we only have a single independent variable, sklearn expects a matrix of independent variables, not a single vector. So we have to turn our vector into a matrix with one column. In other words, we have to change the shape from [40] to [40,1]. One way to do that is with the unsqueeze method, which adds a new unit axis to a tensor at the requested dimension: . xs_lin = x_lin.unsqueeze(1) x_lin.shape,xs_lin.shape . (torch.Size([40]), torch.Size([40, 1])) . A more flexible approach is to slice an array or tensor with the special value None, which introduces an additional unit axis at that location: . x_lin[:,None].shape . torch.Size([40, 1]) . We can now create a random forest for this data. We&#39;ll use only the first 30 rows to train the model: . m_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30]) . Then we&#39;ll test the model on the full dataset. The blue dots are the training data, and the red dots are the predictions: . plt.scatter(x_lin, y_lin, 20) plt.scatter(x_lin, m_lin.predict(xs_lin), color=&#39;red&#39;, alpha=0.5); . We have a big problem! Our predictions outside of the domain that our training data covered are all too low. Why do you suppose this is? . Remember, a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low. . But the problem extends beyond time variables. Random forests are not able to extrapolate outside of the types of data they have seen, in a more general sense. That&#39;s why we need to make sure our validation set does not contain out-of-domain data. . Finding Out-of-Domain Data . Sometimes it is hard to know whether your test set is distributed in the same way as your training data, or, if it is different, what columns reflect that difference. There&#39;s actually an easy way to figure this out, which is to use a random forest! . But in this case we don&#39;t use the random forest to predict our actual dependent variable. Instead, we try to predict whether a row is in the validation set or the training set. To see this in action, let&#39;s combine our training and validation sets together, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance: . df_dom = pd.concat([xs_final, valid_xs_final]) is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final)) m = rf(df_dom, is_valid) rf_feat_importance(m, df_dom)[:6] . cols imp . 5 saleElapsed | 0.858008 | . 10 SalesID | 0.098377 | . 13 MachineID | 0.035284 | . 0 YearMade | 0.002645 | . 4 ModelID | 0.001238 | . 7 Enclosure | 0.000891 | . This shows that there are three columns that differ significantly between the training and validation sets: saleElapsed, SalesID, and MachineID. It&#39;s fairly obvious why this is the case for saleElapsed: it&#39;s the number of days between the start of the dataset and each row, so it directly encodes the date. The difference in SalesID suggests that identifiers for auction sales might increment over time. MachineID suggests something similar might be happening for individual items sold in those auctions. . Let&#39;s get a baseline of the original random forest model&#39;s RMSE, then see what the effect is of removing each of these columns in turn: . . Warning: What is the difference/similarity(if any) between Data Leakage and Out-of-Domain Data. . m = rf(xs_final, y) print(&#39;orig&#39;, m_rmse(m, valid_xs_final, valid_y)) for c in (&#39;SalesID&#39;,&#39;saleElapsed&#39;,&#39;MachineID&#39;): m = rf(xs_final.drop(c,axis=1), y) print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y)) . orig 0.231847 SalesID 0.231492 saleElapsed 0.235826 MachineID 0.231672 . It looks like we should be able to remove SalesID and MachineID without losing any accuracy. Let&#39;s check: . time_vars = [&#39;SalesID&#39;,&#39;MachineID&#39;] xs_final_time = xs_final.drop(time_vars, axis=1) valid_xs_time = valid_xs_final.drop(time_vars, axis=1) m = rf(xs_final_time, y) m_rmse(m, valid_xs_time, valid_y) . 0.228826 . Removing these variables has slightly improved the model&#39;s accuracy; but more importantly, it should make it more resilient over time, and easier to maintain and understand. We recommend that for all datasets you try building a model where your dependent variable is is_valid, like we did here. It can often uncover subtle domain shift issues that you may otherwise miss. . One thing that might help in our case is to simply avoid using old data. Often, old data shows relationships that just aren&#39;t valid any more. Let&#39;s try just using the most recent few years of the data: . xs[&#39;saleYear&#39;].hist(); . Here&#39;s the result of training on this subset: . filt = xs[&#39;saleYear&#39;]&gt;2004 xs_filt = xs_final_time[filt] y_filt = y[filt] . m = rf(xs_filt, y_filt) m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y) . (0.177757, 0.229866) . It&#39;s a tiny bit better, which shows that you shouldn&#39;t always just use your entire dataset; sometimes a subset can be better. . Let&#39;s see if using a neural network helps. . Using a Neural Network . We can use the same approach to build a neural network model. Let&#39;s first replicate the steps we took to set up the TabularPandas object: . df_nn = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) df_nn[dep_var] = np.log(df_nn[dep_var]) df_nn = add_datepart(df_nn, &#39;saledate&#39;) . We can leverage the work we did to trim unwanted columns in the random forest by using the same set of columns for our neural network: . df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]] . Categorical columns are handled very differently in neural networks, compared to decision tree approaches. As we saw in &lt;&gt;, in a neural net a great way to handle categorical variables is by using embeddings. To create embeddings, fastai needs to determine which columns should be treated as categorical variables. It does this by comparing the number of distinct levels in the variable to the value of the max_card parameter. If it&#39;s lower, fastai will treat the variable as categorical. Embedding sizes larger than 10,000 should generally only be used after you&#39;ve tested whether there are better ways to group the variable, so we&#39;ll use 9,000 as our max_card:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var) . In this case, there&#39;s one variable that we absolutely do not want to treat as categorical: the saleElapsed variable. A categorical variable cannot, by definition, extrapolate outside the range of values that it has seen, but we want to be able to predict auction sale prices in the future. Let&#39;s verify that cont_cat_split did the correct thing. . cont_nn . [&#39;saleElapsed&#39;] . Let&#39;s take a look at the cardinality of each of the categorical variables that we have chosen so far: . df_nn_final[cat_nn].nunique() . YearMade 73 ProductSize 6 Coupler_System 2 fiProductClassDesc 74 ModelID 5281 fiSecondaryDesc 177 Enclosure 6 Hydraulics_Flow 3 fiModelDesc 5059 ProductGroup 6 fiModelDescriptor 140 Hydraulics 12 Drive_System 4 Tire_Size 17 dtype: int64 . The fact that there are two variables pertaining to the &quot;model&quot; of the equipment, both with similar very high cardinalities, suggests that they may contain similar, redundant information. Note that we would not necessarily see this when analyzing redundant features, since that relies on similar variables being sorted in the same order (that is, they need to have similarly named levels). Having a column with 5,000 levels means needing 5,000 columns in our embedding matrix, which would be nice to avoid if possible. Let&#39;s see what the impact of removing one of these model columns has on the random forest: . xs_filt2 = xs_filt.drop(&#39;fiModelDescriptor&#39;, axis=1) valid_xs_time2 = valid_xs_time.drop(&#39;fiModelDescriptor&#39;, axis=1) m2 = rf(xs_filt2, y_filt) m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y) . (0.176845, 0.229738) . There&#39;s minimal impact, so we will remove it as a predictor for our neural network: . cat_nn.remove(&#39;fiModelDescriptor&#39;) . We can create our TabularPandas object in the same way as when we created our random forest, with one very important addition: normalization. A random forest does not need any normalization—the tree building procedure cares only about the order of values in a variable, not at all about how they are scaled. But as we have seen, a neural network definitely does care about this. Therefore, we add the Normalize processor when we build our TabularPandas object: . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) . Tabular models and data don&#39;t generally require much GPU RAM, so we can use larger batch sizes: . dls = to_nn.dataloaders(1024) . As we&#39;ve discussed, it&#39;s a good idea to set y_range for regression models, so let&#39;s find the min and max of our dependent variable: . y = to_nn.train.y y.min(),y.max() . (8.465899, 11.863583) . We can now create the Learner to create this tabular model. As usual, we use the application-specific learner function, to take advantage of its application-customized defaults. We set the loss function to MSE, since that&#39;s what this competition uses. . By default, for tabular data fastai creates a neural network with two hidden layers, with 200 and 100 activations, respectively. This works quite well for small datasets, but here we&#39;ve got quite a large dataset, so we increase the layer sizes to 500 and 250: . learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) . learn.lr_find() . SuggestedLRs(valley=0.00015848931798245758) . There&#39;s no need to use fine_tune, so we&#39;ll train with fit_one_cycle for a few epochs and see how it looks: . learn.fit_one_cycle(5, 0.000158) . epoch train_loss valid_loss time . 0 | 0.067682 | 0.069504 | 00:05 | . 1 | 0.053827 | 0.084361 | 00:05 | . 2 | 0.047139 | 0.056659 | 00:05 | . 3 | 0.042519 | 0.052860 | 00:05 | . 4 | 0.039496 | 0.052774 | 00:05 | . We can use our r_mse function to compare the result to the random forest result we got earlier: . preds,targs = learn.get_preds() r_mse(preds,targs) . 0.229727 . It&#39;s quite a bit better than the random forest (although it took longer to train, and it&#39;s fussier about hyperparameter tuning). . Before we move on, let&#39;s save our model in case we want to come back to it again later: . learn.save(&#39;nn&#39;) . Path(&#39;models/nn.pth&#39;) . Sidebar: fastai&#39;s Tabular Classes . In fastai, a tabular model is simply a model that takes columns of continuous or categorical data, and predicts a category (a classification model) or a continuous value (a regression model). Categorical independent variables are passed through an embedding, and concatenated, as we saw in the neural net we used for collaborative filtering, and then continuous variables are concatenated as well. . The model created in tabular_learner is an object of class TabularModel. Take a look at the source for tabular_learner now (remember, that&#39;s tabular_learner?? in Jupyter). You&#39;ll see that like collab_learner, it first calls get_emb_sz to calculate appropriate embedding sizes (you can override these by using the emb_szs parameter, which is a dictionary containing any column names you want to set sizes for manually), and it sets a few other defaults. Other than that, it just creates the TabularModel, and passes that to TabularLearner (note that TabularLearner is identical to Learner, except for a customized predict method). . That means that really all the work is happening in TabularModel, so take a look at the source for that now. With the exception of the BatchNorm1d and Dropout layers (which we&#39;ll be learning about shortly), you now have the knowledge required to understand this whole class. Take a look at the discussion of EmbeddingNN at the end of the last chapter. Recall that it passed n_cont=0 to TabularModel. We now can see why that was: because there are zero continuous variables (in fastai the n_ prefix means &quot;number of,&quot; and cont is an abbreviation for &quot;continuous&quot;). . End sidebar . Another thing that can help with generalization is to use several models and average their predictions—a technique, as mentioned earlier, known as ensembling. . Ensembling . Think back to the original reasoning behind why random forests work so well: each tree has errors, but those errors are not correlated with each other, so the average of those errors should tend towards zero once there are enough trees. Similar reasoning could be used to consider averaging the predictions of models trained using different algorithms. . In our case, we have two very different models, trained using very different algorithms: a random forest, and a neural network. It would be reasonable to expect that the kinds of errors that each one makes would be quite different. Therefore, we might expect that the average of their predictions would be better than either one&#39;s individual predictions. . As we saw earlier, a random forest is itself an ensemble. But we can then include a random forest in another ensemble—an ensemble of the random forest and the neural network! While ensembling won&#39;t make the difference between a successful and an unsuccessful modeling process, it can certainly add a nice little boost to any models that you have built. . One minor issue we have to be aware of is that our PyTorch model and our sklearn model create data of different types: PyTorch gives us a rank-2 tensor (i.e, a column matrix), whereas NumPy gives us a rank-1 array (a vector). squeeze removes any unit axes from a tensor, and to_np converts it into a NumPy array: . . Note: &quot;expect that the kinds of errors that each one makes would be quite different.&quot; I don&#8217;t think I understand that completely. It is an open point for me. . rf_preds = m.predict(valid_xs_time) ens_preds = (to_np(preds.squeeze()) + rf_preds) /2 . This gives us a better result than either model achieved on its own: . r_mse(ens_preds,valid_y) . 0.224519 . In fact, this result is better than any score shown on the Kaggle leaderboard. It&#39;s not directly comparable, however, because the Kaggle leaderboard uses a separate dataset that we do not have access to. Kaggle does not allow us to submit to this old competition to find out how we would have done, but our results certainly look very encouraging! . Boosting . So far our approach to ensembling has been to use bagging, which involves combining many models (each trained on a different data subset) together by averaging them. As we saw, when this is applied to decision trees, this is called a random forest. . There is another important approach to ensembling, called boosting, where we add models instead of averaging them. Here is how boosting works: . Train a small model that underfits your dataset. | Calculate the predictions in the training set for this model. | Subtract the predictions from the targets; these are called the &quot;residuals&quot; and represent the error for each point in the training set. | Go back to step 1, but instead of using the original targets, use the residuals as the targets for the training. | Continue doing this until you reach some stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse. | . Using this approach, each new tree will be attempting to fit the error of all of the previous trees combined. Because we are continually creating new residuals, by subtracting the predictions of each new tree from the residuals from the previous tree, the residuals will get smaller and smaller. . To make predictions with an ensemble of boosted trees, we calculate the predictions from each tree, and then add them all together. There are many models following this basic approach, and many names for the same models. Gradient boosting machines (GBMs) and gradient boosted decision trees (GBDTs) are the terms you&#39;re most likely to come across, or you may see the names of specific libraries implementing these; at the time of writing, XGBoost is the most popular. . Note that, unlike with random forests, with this approach there is nothing to stop us from overfitting. Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others. But in a boosted ensemble, the more trees you have, the better the training error becomes, and eventually you will see overfitting on the validation set. . We are not going to go into detail on how to train a gradient boosted tree ensemble here, because the field is moving rapidly, and any guidance we give will almost certainly be outdated by the time you read this. As we write this, sklearn has just added a HistGradientBoostingRegressor class that provides excellent performance. There are many hyperparameters to tweak for this class, and for all gradient boosted tree methods we have seen. Unlike random forests, gradient boosted trees are extremely sensitive to the choices of these hyperparameters; in practice, most people use a loop that tries a range of different hyperparameters to find the ones that work best. . One more technique that has gotten great results is to use embeddings learned by a neural net in a machine learning model. . Combining Embeddings with Other Methods . The abstract of the entity embedding paper we mentioned at the start of this chapter states: &quot;the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead&quot;. It includes the very interesting table in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Effects of using neural network embeddings as input to other machine learning methods (courtesy of Cheng Guo and Felix Berkhahn) . This is showing the mean average percent error (MAPE) compared among four different modeling techniques, three of which we have already seen, along with k-nearest neighbors (KNN), which is a very simple baseline method. The first numeric column contains the results of using the methods on the data provided in the competition; the second column shows what happens if you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. As you see, in every case, the models are dramatically improved by using the embeddings instead of the raw categories. . This is a really important result, because it shows that you can get much of the performance improvement of a neural network without actually having to use a neural network at inference time. You could just use an embedding, which is literally just an array lookup, along with a small decision tree ensemble. . These embeddings need not even be necessarily learned separately for each model or task in an organization. Instead, once a set of embeddings are learned for some column for some task, they could be stored in a central place, and reused across multiple models. In fact, we know from private communication with other practitioners at large companies that this is already happening in many places. . . Note: without understanding &quot;Entity Embeddings of Categorical Variables&quot; not easy to get above cell. . Conclusion: Our Advice for Tabular Modeling . We have dicussed two approaches to tabular modeling: decision tree ensembles and neural networks. We&#39;ve also mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises: . Random forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods. . | Gradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests. . | Neural networks take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting. . | . We suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it&#39;s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data. . From that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better. . Questionnaire . What is a continuous variable? | What is a categorical variable? | Provide two of the words that are used for the possible values of a categorical variable. | What is a &quot;dense layer&quot;? | How do entity embeddings reduce memory usage and speed up neural networks? | What kinds of datasets are entity embeddings especially useful for? | What are the two main families of machine learning algorithms? | Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas? | Summarize what a decision tree algorithm does. | Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? | Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? | What is pickle and what is it useful for? | How are mse, samples, and values calculated in the decision tree drawn in this chapter? | How do we deal with outliers, before building a decision tree? | How do we handle categorical variables in a decision tree? | What is bagging? | What is the difference between max_samples and max_features when creating a random forest? | If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? | In the section &quot;Creating a Random Forest&quot;, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest?&lt;/li&gt; What is &quot;out-of-bag-error&quot;? | Make a list of reasons why a model&#39;s validation set error might be worse than the OOB error. How could you test your hypotheses? | Explain why random forests are well suited to answering each of the following question: How confident are we in our predictions using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors? | How do predictions vary as we vary these columns? | . | What&#39;s the purpose of removing unimportant variables? | What&#39;s a good type of plot for showing tree interpreter results? | What is the &quot;extrapolation problem&quot;? | How can you tell if your test or validation set is distributed in a different way than your training set? | Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values? | What is &quot;boosting&quot;? | How could we use embeddings with a random forest? Would we expect this to help? | Why might we not always use a neural net for tabular modeling? | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further Research . Pick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare your results to the private leaderboard. | Implement the decision tree algorithm in this chapter from scratch yourself, and try it on the dataset you used in the first exercise. | Use the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw. | Explain what each line of the source of TabularModel does (with the exception of the BatchNorm1d and Dropout layers). | &lt;/div&gt; | . . .",
            "url": "https://niyazikemer.com/fastbook/2021/09/18/chapter-09.html",
            "relUrl": "/fastbook/2021/09/18/chapter-09.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Chapter 14 - Resnets",
            "content": ". import fastbook fastbook.setup_book() . from fastbook import * . %config Completer.use_jedi = False . def get_data(url, presize, resize): path = untar_data(url) return DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(presize), batch_tfms=[*aug_transforms(min_scale=0.5, size=resize), Normalize.from_stats(*imagenet_stats)], ).dataloaders(path, bs=128) . dls = get_data(URLs.IMAGENETTE_160, 160, 128) . dls.show_batch(max_n=4) . Average Pooling: . def avg_pool(x): return x.mean((2,3)) . . Note: need to understand how x.mean((2,3)) works. . def block(ni, nf): return ConvLayer(ni, nf, stride=2) def get_model(): return nn.Sequential( block(3, 16), block(16, 32), block(32, 64), block(64, 128), block(128, 256), nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(256, dls.c)) . . Note: breakpoint usage: https://youtu.be/2AdGJVtP3ak?t=1757 . def get_learner(m): return Learner(dls, m, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() learn = get_learner(get_model()) . learn.lr_find() . SuggestedLRs(valley=tensor(0.0006)) . learn.fit_one_cycle(5, .0007) . epoch train_loss valid_loss accuracy time . 0 | 2.031526 | 1.935686 | 0.292994 | 00:05 | . 1 | 1.730698 | 1.553754 | 0.493758 | 00:05 | . 2 | 1.510944 | 1.496508 | 0.507516 | 00:05 | . 3 | 1.377473 | 1.410298 | 0.551338 | 00:05 | . 4 | 1.306587 | 1.341539 | 0.581146 | 00:05 | . What is Resnets. . first to make a very important leap: . :Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. As this is an academic paper this process is described in a rather inaccessible way, but the concept is actually very simple: start with a 20-layer neural network that is trained well, and add another 36 layers that do nothing at all (for instance, they could be linear layers with a single weight equal to 1, and bias equal to 0). The result will be a 56-layer network that does exactly the same thing as the 20-layer network, proving that there are always deep networks that should be at least as good as any shallow network. But for some reason, SGD does not seem able to find them. jargon:Identity mapping: Returning the input without changing it at all. This process is performed by an identity function. Actually, there is another way to create those extra 36 layers, which is much more interesting. What if we replaced every occurrence of conv(x) with x + conv(x), where conv is the function from the previous chapter that adds a second convolution, then a batchnorm layer, then a ReLU. Furthermore, recall that batchnorm does gamma*y + beta. What if we initialized gamma to zero for every one of those final batchnorm layers? Then our conv(x) for those extra 36 layers will always be equal to zero, which means x+conv(x) will always be equal to x. . What has that gained us? The key thing is that those 36 extra layers, as they stand, are an identity mapping, but they have parameters, which means they are trainable. So, we can start with our best 20-layer model, add these 36 extra layers which initially do nothing at all, and then fine-tune the whole 56-layer model. Those extra 36 layers can then learn the parameters that make them most useful. . The ResNet paper actually proposed a variant of this, which is to instead &quot;skip over&quot; every second convolution, so effectively we get x+conv2(conv1(x)). This is shown by the diagram in &lt;&gt; (from the paper).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Important: BatchNorm again, i need to learn how gamma and beta works and why. . What is under the hood in a Resnet . :Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers fit another mapping of F(x) := H(x)−x. The original mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. . Note: Again, this is rather inaccessible prose—so let&#8217;s try to restate it in plain English! If the outcome of a given layer is x, when using a ResNet block that returns y = x+block(x) we&#8217;re not asking the block to predict y, we are asking it to predict the difference between y and x. So the job of those blocks isn&#8217;t to predict certain features, but to minimize the error between x and the desired y. A ResNet is, therefore, good at learning about slight differences between doing nothing and passing though a block of two convolutional layers (with trainable weights). This is how these models got their name: they&#8217;re predicting residuals (reminder: &quot;residual&quot; is prediction minus target). One key concept that both of these two ways of thinking about ResNets share is the idea of ease of learning. This is an important theme. Recall the universal approximation theorem, which states that a sufficiently large network can learn anything. This is still true, but there turns out to be a very important difference between what a network can learn in principle, and what it is easy for it to learn with realistic data and training regimes. Many of the advances in neural networks over the last decade have been like the ResNet block: the result of realizing how to make something that was always possible actually feasible. . Note: True Identity Path: The original paper didn&#8217;t actually do the trick of using zero for the initial value of gamma in the last batchnorm layer of each block; that came a couple of years later. So, the original version of ResNet didn&#8217;t quite begin training with a truly identity path through the ResNet blocks, but nonetheless having the ability to &quot;navigate through&quot; the skip connections did indeed make it train better. Adding the batchnorm gamma init trick made the models train at even higher learning rates. Here&#39;s the definition of a simple ResNet block (where norm_type=NormType.BatchZero causes fastai to init the gamma weights of the last batchnorm layer to zero): . Nice exlanations of F(x)+x is here: roll it back a couple of minutes. . class ResBlock(Module): def __init__(self, ni, nf): self.convs = nn.Sequential( ConvLayer(ni,nf), ConvLayer(nf,nf, norm_type=NormType.BatchZero)) def forward(self, x): return x + self.convs(x) . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf, stride=stride), ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero)) . class ResBlock(Module): def __init__(self, ni, nf, stride=1): self.convs = _conv_block(ni,nf,stride) self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None) self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True) def forward(self, x): return F.relu(self.convs(x) + self.idconv(self.pool(x))) . def block(ni,nf): return ResBlock(ni, nf, stride=2) learn = get_learner(get_model()) . learn.fit_one_cycle(5, 0.0007) . epoch train_loss valid_loss accuracy time . 0 | 2.103873 | 1.946268 | 0.327134 | 00:08 | . 1 | 1.849293 | 1.683211 | 0.445860 | 00:08 | . 2 | 1.621537 | 1.475794 | 0.517452 | 00:08 | . 3 | 1.427451 | 1.351159 | 0.579363 | 00:08 | . 4 | 1.340495 | 1.328351 | 0.581656 | 00:08 | . def block(ni, nf): return nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf)) . learn = get_learner(get_model()) learn.fit_one_cycle(5, 0.0007) . epoch train_loss valid_loss accuracy time . 0 | 2.059933 | 1.906990 | 0.352357 | 00:12 | . 1 | 1.797654 | 1.609203 | 0.473885 | 00:11 | . 2 | 1.509096 | 1.369431 | 0.544713 | 00:11 | . 3 | 1.287784 | 1.194822 | 0.624204 | 00:11 | . 4 | 1.180562 | 1.157007 | 0.636433 | 00:11 | . def _resnet_stem(*sizes): return [ ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1) for i in range(len(sizes)-1) ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)] . _resnet_stem(3,32,32,64) . [ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ), ConvLayer( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ), ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ), MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)] . class ResNet(nn.Sequential): def __init__(self, n_out, layers, expansion=1): stem = _resnet_stem(3,32,32,64) self.block_szs = [64, 64, 128, 256, 512] for i in range(1,5): self.block_szs[i] *= expansion blocks = [self._make_layer(*o) for o in enumerate(layers)] super().__init__(*stem, *blocks, nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(self.block_szs[-1], n_out)) def _make_layer(self, idx, n_layers): stride = 1 if idx==0 else 2 ch_in,ch_out = self.block_szs[idx:idx+2] return nn.Sequential(*[ ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1) for i in range(n_layers) ]) . rn = ResNet(dls.c, [2,2,2,2]) . learn = get_learner(rn) learn.fit_one_cycle(5,.0007) . epoch train_loss valid_loss accuracy time . 0 | 1.856616 | 1.625414 | 0.467516 | 00:10 | . 1 | 1.412103 | 1.376660 | 0.543694 | 00:10 | . 2 | 1.141276 | 1.159298 | 0.631847 | 00:10 | . 3 | 0.952444 | 0.930670 | 0.702930 | 00:10 | . 4 | 0.854764 | 0.873461 | 0.727389 | 00:10 | . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . Bottleneck layers . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf//4, 1), ConvLayer(nf//4, nf//4, stride=stride), ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero)) . dls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224) . File downloaded is broken. Remove /home/niyazi/.fastai/archive/imagenette2-320.tgz and try again. . rn = ResNet(dls.c, [3,4,6,3], 4) . learn = get_learner(rn) learn.fit_one_cycle(20, .0006) . . 0.00% [0/20 00:00&lt;00:00] epoch train_loss valid_loss accuracy time . . 0.00% [0/73 00:00&lt;00:00] &lt;/div&gt; &lt;/div&gt; RuntimeError Traceback (most recent call last) &lt;ipython-input-46-e810235f7f83&gt; in &lt;module&gt; 1 learn = get_learner(rn) -&gt; 2 learn.fit_one_cycle(20, .0006) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt) 111 scheds = {&#39;lr&#39;: combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final), 112 &#39;mom&#39;: combined_cos(pct_start, *(self.moms if moms is None else moms))} --&gt; 113 self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd) 114 115 # Cell ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt) 219 self.opt.set_hypers(lr=self.lr if lr is None else lr) 220 self.n_epoch = n_epoch --&gt; 221 self._with_events(self._do_fit, &#39;fit&#39;, CancelFitException, self._end_cleanup) 222 223 def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 161 162 def _with_events(self, f, event_type, ex, final=noop): --&gt; 163 try: self(f&#39;before_{event_type}&#39;); f() 164 except ex: self(f&#39;after_cancel_{event_type}&#39;) 165 self(f&#39;after_{event_type}&#39;); final() ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _do_fit(self) 210 for epoch in range(self.n_epoch): 211 self.epoch=epoch --&gt; 212 self._with_events(self._do_epoch, &#39;epoch&#39;, CancelEpochException) 213 214 def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False): ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 161 162 def _with_events(self, f, event_type, ex, final=noop): --&gt; 163 try: self(f&#39;before_{event_type}&#39;); f() 164 except ex: self(f&#39;after_cancel_{event_type}&#39;) 165 self(f&#39;after_{event_type}&#39;); final() ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _do_epoch(self) 204 205 def _do_epoch(self): --&gt; 206 self._do_epoch_train() 207 self._do_epoch_validate() 208 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _do_epoch_train(self) 196 def _do_epoch_train(self): 197 self.dl = self.dls.train --&gt; 198 self._with_events(self.all_batches, &#39;train&#39;, CancelTrainException) 199 200 def _do_epoch_validate(self, ds_idx=1, dl=None): ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 161 162 def _with_events(self, f, event_type, ex, final=noop): --&gt; 163 try: self(f&#39;before_{event_type}&#39;); f() 164 except ex: self(f&#39;after_cancel_{event_type}&#39;) 165 self(f&#39;after_{event_type}&#39;); final() ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in all_batches(self) 167 def all_batches(self): 168 self.n_iter = len(self.dl) --&gt; 169 for o in enumerate(self.dl): self.one_batch(*o) 170 171 def _do_one_batch(self): ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in one_batch(self, i, b) 192 b = self._set_device(b) 193 self._split(b) --&gt; 194 self._with_events(self._do_one_batch, &#39;batch&#39;, CancelBatchException) 195 196 def _do_epoch_train(self): ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 161 162 def _with_events(self, f, event_type, ex, final=noop): --&gt; 163 try: self(f&#39;before_{event_type}&#39;); f() 164 except ex: self(f&#39;after_cancel_{event_type}&#39;) 165 self(f&#39;after_{event_type}&#39;); final() ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/learner.py in _do_one_batch(self) 170 171 def _do_one_batch(self): --&gt; 172 self.pred = self.model(*self.xb) 173 self(&#39;after_pred&#39;) 174 if len(self.yb): ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input) 137 def forward(self, input): 138 for module in self: --&gt; 139 input = module(input) 140 return input 141 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input) 137 def forward(self, input): 138 for module in self: --&gt; 139 input = module(input) 140 return input 141 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] &lt;ipython-input-32-d876acbc8c1b&gt; in forward(self, x) 6 7 def forward(self, x): -&gt; 8 return F.relu(self.convs(x) + self.idconv(self.pool(x))) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input) 137 def forward(self, input): 138 for module in self: --&gt; 139 input = module(input) 140 return input 141 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input) 137 def forward(self, input): 138 for module in self: --&gt; 139 input = module(input) 140 return input 141 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -&gt; 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/conv.py in forward(self, input) 441 442 def forward(self, input: Tensor) -&gt; Tensor: --&gt; 443 return self._conv_forward(input, self.weight, self.bias) 444 445 class Conv3d(_ConvNd): ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias) 437 weight, bias, self.stride, 438 _pair(0), self.dilation, self.groups) --&gt; 439 return F.conv2d(input, weight, bias, self.stride, 440 self.padding, self.dilation, self.groups) 441 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs) 338 convert=False 339 if _torch_handled(args, self._opt, func): convert,types = type(self),(torch.Tensor,) --&gt; 340 res = super().__torch_function__(func, types, args=args, kwargs=kwargs) 341 if convert: res = convert(res) 342 if isinstance(res, TensorBase): res.set_meta(self, as_copy=True) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py in __torch_function__(cls, func, types, args, kwargs) 1021 1022 with _C.DisableTorchFunction(): -&gt; 1023 ret = func(*args, **kwargs) 1024 return _convert(ret, cls) 1025 RuntimeError: Unable to find a valid cuDNN algorithm to run convolution . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; .",
            "url": "https://niyazikemer.com/fastbook/2021/09/11/chapter-14.html",
            "relUrl": "/fastbook/2021/09/11/chapter-14.html",
            "date": " • Sep 11, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Chapter 13 - Convolutional Neural Networks",
            "content": ". #!pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . [[chapter_convolutions]] Convolutional Neural Networks . In &lt;&gt; we learned how to create a neural network recognizing images. We were able to achieve a bit over 98% accuracy at distinguishing 3s from 7s—but we also saw that fastai&#39;s built-in classes were able to get close to 100%. Let&#39;s start trying to close the gap.&lt;/p&gt; In this chapter, we will begin by digging into what convolutions are and building a CNN from scratch. We will then study a range of techniques to improve training stability and learn all the tweaks the library usually applies for us to get great results. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Magic of Convolutions . One of the most powerful tools that machine learning practitioners have at their disposal is feature engineering. A feature is a transformation of the data which is designed to make it easier to model. For instance, the add_datepart function that we used for our tabular dataset preprocessing in &lt;&gt; added date features to the Bulldozers dataset. What kinds of features might we be able to create from images?&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; jargon:Feature engineering: Creating new transformations of the input data in order to make it easier to model. . In the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that. On the other hand, the number 3 is characterized by a diagonal edge in one direction at the top left and bottom right of the digit, the opposite diagonal at the bottom left and top right, horizontal edges at the middle, top, and bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that information as our features, instead of raw pixels? . It turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a convolution. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book! . A convolution applies a kernel across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Applying a kernel to one location . The 7×7 grid to the left is the image we&#39;re going to apply the kernel to. The convolution operation multiplies each element of the kernel by each element of a 3×3 block of the image. The results of these multiplications are then added together. The diagram in &lt;&gt; shows an example of applying a kernel to a single location in the image, the 3×3 block around cell 18.&lt;/p&gt; Let&#39;s do this with code. First, we create a little 3×3 matrix like so: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; top_edge = tensor([[-1,-1,-1], [ 0, 0, 0], [ 1, 1, 1]]).float() . We&#39;re going to call this our kernel (because that&#39;s what fancy computer vision researchers call these). And we&#39;ll need an image, of course: . path = untar_data(URLs.MNIST_SAMPLE) . Path.BASE_PATH = path . im3 = Image.open(path/&#39;train&#39;/&#39;3&#39;/&#39;12.png&#39;) show_image(im3); . Now we&#39;re going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we&#39;ll add them up, like so: . im3_t = tensor(im3) im3_t[0:3,0:3] * top_edge . tensor([[-0., -0., -0.], [0., 0., 0.], [0., 0., 0.]]) . (im3_t[0:3,0:3] * top_edge).sum() . tensor(0.) . Not very interesting so far—all the pixels in the top-left corner are white. But let&#39;s pick a couple of more interesting spots: . df = pd.DataFrame(im3_t[:10,:20]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . . There&#39;s a top edge at cell 5,8. Let&#39;s repeat our calculation there: . (im3_t[4:7,6:9] * top_edge).sum() . tensor(762.) . There&#39;s a right edge at cell 8,18. What does that give us?: . (im3_t[7:10,17:20] * top_edge).sum() . tensor(-29.) . As you can see, this little calculation is returning a high number where the 3×3-pixel square represents a top edge (i.e., where there are low values at the top of the square, and high values immediately underneath). That&#39;s because the -1 values in our kernel have little impact in that case, but the 1 values have a lot. . Let&#39;s look a tiny bit at the math. The filter will take any window of size 3×3 in our images, and if we name the pixel values like this: . $$ begin{matrix} a1 &amp; a2 &amp; a3 a4 &amp; a5 &amp; a6 a7 &amp; a8 &amp; a9 end{matrix}$$ . it will return $-a1-a2-a3+a7+a8+a9$. If we are in a part of the image where $a1$, $a2$, and $a3$ add up to the same as $a7$, $a8$, and $a9$, then the terms will cancel each other out and we will get 0. However, if $a7$ is greater than $a1$, $a8$ is greater than $a2$, and $a9$ is greater than $a3$, we will get a bigger number as a result. So this filter detects horizontal edges—more precisely, edges where we go from bright parts of the image at the top to darker parts at the bottom. . Changing our filter to have the row of 1s at the top and the -1s at the bottom would detect horizontal edges that go from dark to light. Putting the 1s and -1s in columns versus rows would give us filters that detect vertical edges. Each set of weights will produce a different kind of outcome. . Let&#39;s create a function to do this for one location, and check it matches our result from before: . def apply_kernel(row, col, kernel): return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum() . apply_kernel(5,7,top_edge) . tensor(762.) . But note that we can&#39;t apply it to the corner (e.g., location 0,0), since there isn&#39;t a complete 3×3 square there. . Mapping a Convolution Kernel . We can map apply_kernel() across the coordinate grid. That is, we&#39;ll be taking our 3×3 kernel, and applying it to each 3×3 section of our image. For instance, &lt;&gt; shows the positions a 3×3 kernel can be applied to in the first row of a 5×5 image.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Applying a kernel across a grid . To get a grid of coordinates we can use a nested list comprehension, like so: . [[(i,j) for j in range(1,5)] for i in range(1,5)] . [[(1, 1), (1, 2), (1, 3), (1, 4)], [(2, 1), (2, 2), (2, 3), (2, 4)], [(3, 1), (3, 2), (3, 3), (3, 4)], [(4, 1), (4, 2), (4, 3), (4, 4)]] . . Note: Nested List Comprehensions: Nested list comprehensions are used a lot in Python, so if you haven&#8217;t seen them before, take a few minutes to make sure you understand what&#8217;s happening here, and experiment with writing your own nested list comprehensions. . Here&#39;s the result of applying our kernel over a coordinate grid: . rng = range(1,27) top_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng]) show_image(top_edge3); . Looking good! Our top edges are black, and bottom edges are white (since they are the opposite of top edges). Now that our image contains negative numbers too, matplotlib has automatically changed our colors so that white is the smallest number in the image, black the highest, and zeros appear as gray. . We can try the same thing for left edges: . left_edge = tensor([[-1,1,0], [-1,1,0], [-1,1,0]]).float() left_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng]) show_image(left_edge3); . As we mentioned before, a convolution is the operation of applying such a kernel over a grid in this way. In the paper &quot;A Guide to Convolution Arithmetic for Deep Learning&quot; there are many great diagrams showing how image kernels can be applied. Here&#39;s an example from the paper showing (at the bottom) a light blue 4×4 image, with a dark blue 3×3 kernel being applied, creating a 2×2 green output activation map at the top. . Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent Dumoulin and Francesco Visin) . Look at the shape of the result. If the original image has a height of h and a width of w, how many 3×3 windows can we find? As you can see from the example, there are h-2 by w-2 windows, so the image we get has a result as a height of h-2 and a width of w-2. . We won&#39;t implement this convolution function from scratch, but use PyTorch&#39;s implementation instead (it is way faster than anything we could do in Python). . Convolutions in PyTorch . Convolution is such an important and widely used operation that PyTorch has it built in. It&#39;s called F.conv2d (recall that F is a fastai import from torch.nn.functional, as recommended by PyTorch). The PyTorch docs tell us that it includes these parameters: . input:: input tensor of shape (minibatch, in_channels, iH, iW) | weight:: filters of shape (out_channels, in_channels, kH, kW) | . Here iH,iW is the height and width of the image (i.e., 28,28), and kH,kW is the height and width of our kernel (3,3). But apparently PyTorch is expecting rank-4 tensors for both these arguments, whereas currently we only have rank-2 tensors (i.e., matrices, or arrays with two axes). . The reason for these extra axes is that PyTorch has a few tricks up its sleeve. The first trick is that PyTorch can apply a convolution to multiple images at the same time. That means we can call it on every item in a batch at once! . The second trick is that PyTorch can apply multiple kernels at the same time. So let&#39;s create the diagonal-edge kernels too, and then stack all four of our edge kernels into a single tensor: . diag1_edge = tensor([[ 0,-1, 1], [-1, 1, 0], [ 1, 0, 0]]).float() diag2_edge = tensor([[ 1,-1, 0], [ 0, 1,-1], [ 0, 0, 1]]).float() edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge]) edge_kernels.shape . torch.Size([4, 3, 3]) . To test this, we&#39;ll need a DataLoader and a sample mini-batch. Let&#39;s use the data block API: . mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label) dls = mnist.dataloaders(path) xb,yb = first(dls.valid) xb.shape . torch.Size([64, 1, 28, 28]) . By default, fastai puts data on the GPU when using data blocks. Let&#39;s move it to the CPU for our examples: . xb,yb = to_cpu(xb),to_cpu(yb) . One batch contains 64 images, each of 1 channel, with 28×28 pixels. F.conv2d can handle multichannel (i.e., color) images too. A channel is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions [channels, rows, columns]. . We&#39;ll see how to handle more than one channel later in this chapter. Kernels passed to F.conv2d need to be rank-4 tensors: [channels_in, features_out, rows, columns]. edge_kernels is currently missing one of these. We need to tell PyTorch that the number of input channels in the kernel is one, which we can do by inserting an axis of size one (this is known as a unit axis) in the first location, where the PyTorch docs show in_channels is expected. To insert a unit axis into a tensor, we use the unsqueeze method: . edge_kernels.shape,edge_kernels.unsqueeze(1).shape . (torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3])) . This is now the correct shape for edge_kernels. Let&#39;s pass this all to conv2d: . edge_kernels = edge_kernels.unsqueeze(1) . batch_features = F.conv2d(xb, edge_kernels) batch_features.shape . torch.Size([64, 4, 26, 26]) . The output shape shows we gave 64 images in the mini-batch, 4 kernels, and 26×26 edge maps (we started with 28×28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually: . show_image(batch_features[0,0]); . The most important trick that PyTorch has up its sleeve is that it can use the GPU to do all this work in parallel—that is, applying multiple kernels, to multiple images, across multiple channels. Doing lots of work in parallel is critical to getting GPUs to work efficiently; if we did each of these operations one at a time, we&#39;d often run hundreds of times slower (and if we used our manual convolution loop from the previous section, we&#39;d be millions of times slower!). Therefore, to become a strong deep learning practitioner, one skill to practice is giving your GPU plenty of work to do at a time. . It would be nice to not lose those two pixels on each axis. The way we do that is to add padding, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added. . Strides and Padding . With appropriate padding, we can ensure that the output activation map is the same size as the original image, which can make things a lot simpler when we construct our architectures. &lt;&gt; shows how adding padding allows us to apply the kernels in the image corners.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A convolution with padding . With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map, as we can see in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin) . If we add a kernel of size ks by ks (with ks an odd number), the necessary padding on each side to keep the same shape is ks//2. An even number for ks would require a different amount of padding on the top/bottom and left/right, but in practice we almost never use an even filter size. . So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application, as in &lt;&gt;. This is known as a stride-2 convolution. The most common kernel size in practice is 3×3, and the most common padding is 1. As you&#39;ll see, stride-2 convolutions are useful for decreasing the size of our outputs, and stride-1 convolutions are useful for adding layers without changing the output size.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding (courtesy of Vincent Dumoulin and Francesco Visin) . In an image of size h by w, using a padding of 1 and a stride of 2 will give us a result of size (h+1)//2 by (w+1)//2. The general formula for each dimension is (n + 2*pad - ks)//stride + 1, where pad is the padding, ks, the size of our kernel, and stride is the stride. . Let&#39;s now take a look at how the pixel values of the result of our convolutions are computed. . Understanding the Convolution Equations . To explain the math behind convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing CNNs from different viewpoints. In fact, it&#39;s so clever, and so helpful, we&#39;re going to show it here too! . Here&#39;s our 3×3 pixel image, with each pixel labeled with a letter: . . And here&#39;s our kernel, with each weight labeled with a Greek letter: . . Since the filter fits in the image four times, we have four results: . . &lt;&gt; shows how we applied the kernel to each section of the image to yield each result.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Applying the kernel . The equation view is in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The equation . Notice that the bias term, b, is the same for each section of the image. You can consider the bias as part of the filter, just like the weights (α, β, γ, δ) are part of the filter. . Here&#39;s an interesting insight—a convolution can be represented as a special kind of matrix multiplication, as illustrated in &lt;&gt;. The weight matrix is just like the ones from traditional neural networks. However, this weight matrix has two special properties:&lt;/p&gt; The zeros shown in gray are untrainable. This means that they’ll stay zero throughout the optimization process. | Some of the weights are equal, and while they are trainable (i.e., changeable), they must remain equal. These are called shared weights. | The zeros correspond to the pixels that the filter can&#39;t touch. Each row of the weight matrix corresponds to one application of the filter. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Convolution as matrix multiplication . Now that we understand what a convolution is, let&#39;s use them to build a neural net. . Our First Convolutional Neural Network . There is no reason to believe that some particular edge filters are the most useful kernels for image recognition. Furthermore, we&#39;ve seen that in later layers convolutional kernels become complex transformations of features from lower levels, but we don&#39;t have a good idea of how to manually construct these. . Instead, it would be best to learn the values of the kernels. We already know how to do this—SGD! In effect, the model will learn the features that are useful for classification. . When we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network (CNN). . Creating the CNN . Let&#39;s go back to the basic neural network we had in &lt;&gt;. It was defined like this:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . We can view a model&#39;s definition: . simple_net . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . We now want to create a similar architecture to this linear model, but using convolutional layers instead of linear. nn.Conv2d is the module equivalent of F.conv2d. It&#39;s more convenient than F.conv2d when creating an architecture, because it creates the weight matrix for us automatically when we instantiate it. . Here&#39;s a possible architecture: . broken_cnn = sequential( nn.Conv2d(1,30, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(30,1, kernel_size=3, padding=1) ) . One thing to note here is that we didn&#39;t need to specify 28×28 as the input size. That&#39;s because a linear layer needs a weight in the weight matrix for every pixel, so it needs to know how many pixels there are, but a convolution is applied over each pixel automatically. The weights only depend on the number of input and output channels and the kernel size, as we saw in the previous section. . Think about what the output shape is going to be, then let&#39;s try it and see: . broken_cnn(xb).shape . torch.Size([64, 1, 28, 28]) . This is not something we can use to do classification, since we need a single output activation per image, not a 28×28 map of activations. One way to deal with this is to use enough stride-2 convolutions such that the final layer is size 1. That is, after one stride-2 convolution the size will be 14×14, after two it will be 7×7, then 4×4, 2×2, and finally size 1. . Let&#39;s try that now. First, we&#39;ll define a function with the basic parameters we&#39;ll use in each convolution: . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res . . Important: Refactoring: Refactoring parts of your neural networks like this makes it much less likely you&#8217;ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing. . When we use a stride-2 convolution, we often increase the number of features at the same time. This is because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time. . jargon:channels and features: These two terms are largely used interchangeably, and refer to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution. Features is never used to refer to the input data, but channels can refer to either the input data (generally channels are colors) or activations inside the network. . Here is how we can build a simple CNN: . simple_cnn = sequential( conv(1 ,4), #14x14 conv(4 ,8), #7x7 conv(8 ,16), #4x4 conv(16,32), #2x2 conv(32,2, act=False), #1x1 Flatten(), ) . j:I like to add comments like the ones here after each convolution to show how large the activation map will be after each layer. These comments assume that the input size is 28*28 . Now the network outputs two activations, which map to the two possible levels in our labels: . simple_cnn(xb).shape . torch.Size([64, 2]) . We can now create our Learner: . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) . To see exactly what&#39;s going on in the model, we can use summary: . learn.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 14 x 14 Conv2d 40 True ReLU ____________________________________________________________________________ 64 x 8 x 7 x 7 Conv2d 296 True ReLU ____________________________________________________________________________ 64 x 16 x 4 x 4 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 2 x 2 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 2 x 1 x 1 Conv2d 578 True ____________________________________________________________________________ [] Flatten ____________________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f4ec3f2fee0&gt; Loss function: &lt;function cross_entropy at 0x7f4f13a71820&gt; Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Note that the output of the final Conv2d layer is 64x2x1x1. We need to remove those extra 1x1 axes; that&#39;s what Flatten does. It&#39;s basically the same as PyTorch&#39;s squeeze method, but as a module. . Let&#39;s see if this trains! Since this is a deeper network than we&#39;ve built from scratch before, we&#39;ll use a lower learning rate and more epochs: . learn.fit_one_cycle(2, 0.01) . epoch train_loss valid_loss accuracy time . 0 | 0.059677 | 0.040094 | 0.985770 | 00:05 | . 1 | 0.018634 | 0.025003 | 0.990677 | 00:04 | . Success! It&#39;s getting closer to the resnet18 result we had, although it&#39;s not quite there yet, and it&#39;s taking more epochs, and we&#39;re needing to use a lower learning rate. We still have a few more tricks to learn, but we&#39;re getting closer and closer to being able to create a modern CNN from scratch. . Understanding Convolution Arithmetic . We can see from the summary that we have an input of size 64x1x28x28. The axes are batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order. The first layer is: . m = learn.model[0] m . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) . So we have 1 input channel, 4 output channels, and a 3×3 kernel. Let&#39;s check the weights of the first convolution: . m[0].weight.shape . torch.Size([4, 1, 3, 3]) . The summary shows we have 40 parameters, and 4*1*3*3 is 36. What are the other four parameters? Let&#39;s see what the bias contains: . m[0].bias.shape . torch.Size([4]) . We can now use this information to clarify our statement in the previous section: &quot;When we use a stride-2 convolution, we often increase the number of features because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time.&quot; . There is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let&#39;s ignore the batch axis to keep things simple. So for each of 14*14=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that&#39;s 196*288=56_448 multiplications at this layer. The next layer will have 7*7*(1168-16)=56_448 multiplications. . What happened here is that our stride-2 convolution halved the grid size from 14x14 to 7x7, and we doubled the number of filters from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper. But we know that the deeper layers have to compute semantically rich features (such as eyes or fur), so we wouldn&#39;t expect that doing less computation would make sense. . Another way to think of this is based on receptive fields. . Receptive Fields . The receptive field is the area of an image that is involved in the calculation of a layer. On the book&#39;s website, you&#39;ll find an Excel spreadsheet called conv-example.xlsx that shows the calculation of two stride-2 convolutional layers using an MNIST digit. Each layer has a single kernel. &lt;&gt; shows what we see if we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precedents.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Immediate precedents of Conv2 layer . Here, the cell with the green border is the cell we clicked on, and the blue highlighted cells are its precedents—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Let&#39;s now click trace precedents again, to see what cells are used to calculate these inputs. &lt;&gt; shows what happens.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Secondary precedents of Conv2 layer . In this example, we have just two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This 7×7 area is the receptive field in the input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers. . As you see from this example, the deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer. A large receptive field means that a large amount of the input image is used to calculate each activation in that layer is. We now know that in the deeper layers of the network we have semantically rich features, corresponding to larger receptive fields. Therefore, we&#39;d expect that we&#39;d need more weights for each of our features to handle this increasing complexity. This is another way of saying the same thing we mentioned in the previous section: when we introduce a stride-2 conv in our network, we should also increase the number of channels. . When writing this particular chapter, we had a lot of questions we needed answers for, to be able to explain CNNs to you as best we could. Believe it or not, we found most of the answers on Twitter. We&#39;re going to take a quick break to talk to you about that now, before we move on to color images. . A Note About Twitter . We are not, to say the least, big users of social networks in general. But our goal in writing this book is to help you become the best deep learning practitioner you can, and we would be remiss not to mention how important Twitter has been in our own deep learning journeys. . You see, there&#39;s another part of Twitter, far away from Donald Trump and the Kardashians, which is the part of Twitter where deep learning researchers and practitioners talk shop every day. As we were writing this section, Jeremy wanted to double-check that what we were saying about stride-2 convolutions was accurate, so he asked on Twitter: . . A few minutes later, this answer popped up: . . Christian Szegedy is the first author of Inception, the 2014 ImageNet winner and source of many key insights used in modern neural networks. Two hours later, this appeared: . . Do you recognize that name? You saw it in &lt;&gt;, when we were talking about the Turing Award winners who established the foundations of deep learning today!&lt;/p&gt; Jeremy also asked on Twitter for help checking our description of label smoothing in &lt;&gt; was accurate, and got a response again from directly from Christian Szegedy (label smoothing was originally introduced in the Inception paper):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Many of the top people in deep learning today are Twitter regulars, and are very open about interacting with the wider community. One good way to get started is to look at a list of Jeremy&#39;s recent Twitter likes, or Sylvain&#39;s. That way, you can see a list of Twitter users that we think have interesting and useful things to say. . Twitter is the main way we both stay up to date with interesting papers, software releases, and other deep learning news. For making connections with the deep learning community, we recommend getting involved both in the fast.ai forums and on Twitter. . That said, let&#39;s get back to the meat of this chapter. Up until now, we have only shown you examples of pictures in black and white, with one value per pixel. In practice, most colored images have three values per pixel to define their color. We&#39;ll look at working with color images next. . Color Images . A colour picture is a rank-3 tensor: . im = image2tensor(Image.open(image_bear())) im.shape . torch.Size([3, 1000, 846]) . show_image(im); . The first axis contains the channels, red, green, and blue: . _,axs = subplots(1,3) for bear,ax,color in zip(im,axs,(&#39;Reds&#39;,&#39;Greens&#39;,&#39;Blues&#39;)): show_image(255-bear, ax=ax, cmap=color) . We saw what the convolution operation was for one filter on one channel of the image (our examples were done on a square). A convolutional layer will take an image with a certain number of channels (three for the first layer for regular RGB color images) and output an image with a different number of channels. Like our hidden size that represented the numbers of neurons in a linear layer, we can decide to have as many filters as we want, and each of them will be able to specialize, some to detect horizontal edges, others to detect vertical edges and so forth, to give something like we studied in &lt;&gt;.&lt;/p&gt; In one sliding window, we have a certain number of channels and we need as many filters (we don&#39;t use the same kernel for all the channels). So our kernel doesn&#39;t have a size of 3 by 3, but ch_in (for channels in) is 3 by 3. On each channel, we multiply the elements of our window by the elements of the coresponding filter, then sum the results (as we saw before) and sum over all the filters. In the example given in &lt;&gt;, the result of our conv layer on that window is red + green + blue.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Convolution over an RGB image . So, in order to apply a convolution to a color picture we require a kernel tensor with a size that matches the first axis. At each location, the corresponding parts of the kernel and the image patch are multiplied together. . These are then all added together, to produce a single number, for each grid location, for each output feature, as shown in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Adding the RGB filters . Then we have ch_out filters like this, so in the end, the result of our convolutional layer will be a batch of images with ch_out channels and a height and width given by the formula outlined earlier. This give us ch_out tensors of size ch_in x ks x ks that we represent in one big tensor of four dimensions. In PyTorch, the order of the dimensions for those weights is ch_out x ch_in x ks x ks. . Additionally, we may want to have a bias for each filter. In the preceding example, the final result for our convolutional layer would be $y_{R} + y_{G} + y_{B} + b$ in that case. Like in a linear layer, there are as many bias as we have kernels, so the biases is a vector of size ch_out. . There are no special mechanisms required when setting up a CNN for training with color images. Just make sure your first layer has three inputs. . There are lots of ways of processing color images. For instance, you can change them to black and white, change from RGB to HSV (hue, saturation, and value) color space, and so forth. In general, it turns out experimentally that changing the encoding of colors won&#39;t make any difference to your model results, as long as you don&#39;t lose information in the transformation. So, transforming to black and white is a bad idea, since it removes the color information entirely (and this can be critical; for instance, a pet breed may have a distinctive color); but converting to HSV generally won&#39;t make any difference. . Now you know what those pictures in &lt;&gt; of &quot;what a neural net learns&quot; from the Zeiler and Fergus paper mean! This is their picture of some of the layer 1 weights which we showed:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . This is taking the three slices of the convolutional kernel, for each output feature, and displaying them as images. We can see that even though the creators of the neural net never explicitly created kernels to find edges, for instance, the neural net automatically discovered these features using SGD. . Now let&#39;s see how we can train these CNNs, and show you all the techniques fastai uses under the hood for efficient training. . Improving Training Stability . Since we are so good at recognizing 3s from 7s, let&#39;s move on to something harder—recognizing all 10 digits. That means we&#39;ll need to use MNIST instead of MNIST_SAMPLE: . path = untar_data(URLs.MNIST) . path.ls() . (#2) [Path(&#39;training&#39;),Path(&#39;testing&#39;)] . The data is in two folders named training and testing, so we have to tell GrandparentSplitter about that (it defaults to train and valid). We did do that in the get_dls function, which we create to make it easy to change our batch size later: . def get_dls(bs=64): return DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;,&#39;testing&#39;), get_y=parent_label, batch_tfms=Normalize() ).dataloaders(path, bs=bs) dls = get_dls() . Remember, it&#39;s always a good idea to look at your data before you use it: . dls.show_batch(max_n=9, figsize=(4,4)) . Now that we have our data ready, we can train a simple model on it. . A Simple Baseline . Earlier in this chapter, we built a model based on a conv function like this: . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res . Let&#39;s start with a basic CNN as a baseline. We&#39;ll use the same one as earlier, but with one tweak: we&#39;ll use more activations. Since we have more numbers to differentiate, it&#39;s likely we will need to learn more filters. . As we discussed, we generally want to double the number of filters each time we have a stride-2 layer. One way to increase the number of filters throughout our network is to double the number of activations in the first layer–then every layer after that will end up twice as big as in the previous version as well. . Important: But there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3×3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn&#8217;t really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they&#8217;re forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs. To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features: . def simple_cnn(): return sequential( conv(1 ,8, ks=5), #14x14 conv(8 ,16), #7x7 conv(16,32), #4x4 conv(32,64), #2x2 conv(64,10, act=False), #1x1 Flatten(), ) . As you&#39;ll see in a moment, we can look inside our models while they&#39;re training in order to try to find ways to make them train better. To do this we use the ActivationStats callback, which records the mean, standard deviation, and histogram of activations of every trainable layer (as we&#39;ve seen, callbacks are used to add behavior to the training loop; we&#39;ll explore how they work in &lt;&gt;):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai.callback.hook import * . We want to train quickly, so that means training at a high learning rate. Let&#39;s see how we go at 0.06: . def fit(epochs=1): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit(epochs, 0.06) return learn . learn = fit() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/callback/core.py:51: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 2.304775 | 2.307039 | 0.098200 | 00:21 | . This didn&#39;t train at all well! Let&#39;s find out why. . One handy feature of the callbacks passed to Learner is that they are made available automatically, with the same name as the callback class, except in snake_case. So, our ActivationStats callback can be accessed through activation_stats. I&#39;m sure you remember learn.recorder... can you guess how that is implemented? That&#39;s right, it&#39;s a callback called Recorder! . ActivationStats includes some handy utilities for plotting the activations during training. plot_layer_stats(idx) plots the mean and standard deviation of the activations of layer number idx, along with the percentage of activations near zero. Here&#39;s the first layer&#39;s plot: . learn.activation_stats.plot_layer_stats(0) . . Important: Generally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training. Activations near zero are particularly problematic, because it means we have computation in the model that&#8217;s doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer... which will then create more zeros. Here&#8217;s the penultimate layer of our network: . learn.activation_stats.plot_layer_stats(-2) . As expected, the problems get worse towards the end of the network, as the instability and zero activations compound over layers. Let&#39;s look at what we can do to make training more stable. . Increase Batch Size . . Important: One way to make training more stable is to increase the batch size. Larger batches have gradients that are more accurate, since they&#8217;re calculated from more data. On the downside, though, a larger batch size means fewer batches per epoch, which means less opportunities for your model to update weights. Let&#8217;s see if a batch size of 512 helps: . dls = get_dls(512) . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 2.316074 | 2.301938 | 0.113500 | 00:11 | . Let&#39;s see what the penultimate layer looks like: . learn.activation_stats.plot_layer_stats(-2) . Again, we&#39;ve got most of our activations near zero. Let&#39;s see what else we can do to improve training stability. . 1cycle Training . Our initial weights are not well suited to the task we&#39;re trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly, as we&#39;ve seen. We probably don&#39;t want to end training with a high learning rate either, so that we don&#39;t skip over a minimum. But we want to train at a high learning rate for the rest of the training period, because we&#39;ll be able to train more quickly that way. Therefore, we should change the learning rate during training, from low, to high, and then back to low again. . Leslie Smith (yes, the same guy that invented the learning rate finder!) developed this idea in his article &quot;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&quot;. He designed a schedule for learning rate separated into two phases: one where the learning rate grows from the minimum value to the maximum value (warmup), and one where it decreases back to the minimum value (annealing). Smith called this combination of approaches 1cycle training. . Important: 1cycle training allows us to use a much higher maximum learning rate than other types of training, which gives two benefits: . By training with higher learning rates, we train faster—a phenomenon Smith named super-convergence. | By training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss. . Note: The second point is an interesting and subtle one; it is based on the observation that a model that generalizes well is one whose loss would not change very much if you changed the input by a small amount. If a model trains at a large learning rate for quite a while, and can find a good loss when doing so, it must have found an area that also generalizes well, because it is jumping around a lot from batch to batch (that is basically the definition of a high learning rate). The problem is that, as we have discussed, just jumping to a high learning rate is more likely to result in diverging losses, rather than seeing your losses improve. So we don&#8217;t jump straight to a high learning rate. Instead, we start at a low learning rate, where our losses do not diverge, and we allow the optimizer to gradually find smoother and smoother areas of our parameters by gradually going to higher and higher learning rates. Then, once we have found a nice smooth area for our parameters, we want to find the very best part of that area, which means we have to bring our learning rates down again. This is why 1cycle training has a gradual learning rate warmup, and a gradual learning rate cooldown. Many researchers have found that in practice this approach leads to more accurate models and trains more quickly. That is why it is the approach that is used by default for fine_tune in fastai. | . In &lt;&gt; we&#39;ll learn all about momentum in SGD. Briefly, momentum is a technique where the optimizer takes a step not only in the direction of the gradients, but also that continues in the direction of previous steps. Leslie Smith introduced the idea of cyclical momentums in &quot;A Disciplined Approach to Neural Network Hyper-Parameters: Part 1&quot;. It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase.&lt;/p&gt; We can use 1cycle training in fastai by calling fit_one_cycle: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def fit(epochs=1, lr=0.06): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, lr) return learn . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.207010 | 0.077605 | 0.975600 | 00:11 | . We&#39;re finally making some progress! It&#39;s giving us a reasonable accuracy now. . We can view the learning rate and momentum throughout training by calling plot_sched on learn.recorder. learn.recorder (as the name suggests) records everything that happens during training, including losses, metrics, and hyperparameters such as learning rate and momentum: . learn.recorder.plot_sched() . Smith&#39;s original 1cycle paper used a linear warmup and linear annealing. As you can see, we adapted the approach in fastai by combining it with another popular approach: cosine annealing. fit_one_cycle provides the following parameters you can adjust: . lr_max:: The highest learning rate that will be used (this can also be a list of learning rates for each layer group, or a Python slice object containing the first and last layer group learning rates) | div:: How much to divide lr_max by to get the starting learning rate | div_final:: How much to divide lr_max by to get the ending learning rate | pct_start:: What percentage of the batches to use for the warmup | moms:: A tuple (mom1,mom2,mom3) where mom1 is the initial momentum, mom2 is the minimum momentum, and mom3 is the final momentum | . Let&#39;s take a look at our layer stats again: . learn.activation_stats.plot_layer_stats(-2) . The percentage of near-zero weights is getting much better, although it&#39;s still quite high. . We can see even more about what&#39;s going on in our training using color_dim, passing it a layer index: . learn.activation_stats.color_dim(-2) . color_dim was developed by fast.ai in conjunction with a student, Stefano Giomo. Stefano, who refers to the idea as the colorful dimension, provides an in-depth explanation of the history and details behind the method. The basic idea is to create a histogram of the activations of a layer, which we would hope would follow a smooth pattern such as the normal distribution (colorful_dist). . Histogram in &#39;colorful dimension&#39; . To create color_dim, we take the histogram shown on the left here, and convert it into just the colored representation shown at the bottom. Then we flip it on its side, as shown on the right. We found that the distribution is clearer if we take the log of the histogram values. Then, Stefano describes: . :The final plot for each layer is made by stacking the histogram of the activations from each batch along the horizontal axis. So each vertical slice in the visualisation represents the histogram of activations for a single batch. The color intensity corresponds to the height of the histogram, in other words the number of activations in each histogram bin. &lt;&gt; shows how this all fits together.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Summary of the colorful dimension (courtesy of Stefano Giomo) . This illustrates why log(f) is more colorful than f when f follows a normal distribution because taking a log changes the Gaussian in a quadratic, which isn&#39;t as narrow. . So with that in mind, let&#39;s take another look at the result for the penultimate layer: . learn.activation_stats.color_dim(-2) . This shows a classic picture of &quot;bad training.&quot; We start with nearly all activations at zero—that&#39;s what we see at the far left, with all the dark blue. The bright yellow at the bottom represents the near-zero activations. Then, over the first few batches we see the number of nonzero activations exponentially increasing. But it goes too far, and collapses! We see the dark blue return, and the bottom becomes bright yellow again. It almost looks like training restarts from scratch. Then we see the activations increase again, and collapse again. After repeating this a few times, eventually we see a spread of activations throughout the range. . It&#39;s much better if training can be smooth from the start. The cycles of exponential increase and then collapse tend to result in a lot of near-zero activations, resulting in slow training and poor final results. One way to solve this problem is to use batch normalization. . Batch Normalization . To fix the slow training and poor final results we ended up with in the previous section, we need to fix the initial large percentage of near-zero activations, and then try to maintain a good distribution of activations throughout training. . Sergey Ioffe and Christian Szegedy presented a solution to this problem in the 2015 paper &quot;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&quot;. In the abstract, they describe just the problem that we&#39;ve seen: . Important: Training Deep Neural Networks is complicated by the fact that the distribution of each layer&#8217;s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization... We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Their solution, they say is: . Important: Making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. The paper caused great excitement as soon as it was released, because it included the chart in &lt;&gt;, which clearly demonstrated that batch normalization could train a model that was even more accurate than the current state of the art (the Inception architecture) and around 5x faster.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Impact of batch normalization (courtesy of Sergey Ioffe and Christian Szegedy) . Batch normalization (often just called batchnorm) works by taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the activations. However, this can cause problems because the network might want some activations to be really high in order to make accurate predictions. So they also added two learnable parameters (meaning they will be updated in the SGD step), usually called gamma and beta. After normalizing the activations to get some new activation vector y, a batchnorm layer returns gamma*y + beta. . That&#39;s why our activations can have any mean or variance, independent from the mean and standard deviation of the results of the previous layer. Those statistics are learned separately, making training easier on our model. The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training. . Let&#39;s add a batchnorm layer to conv: . def conv(ni, nf, ks=3, act=True): layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)] if act: layers.append(nn.ReLU()) layers.append(nn.BatchNorm2d(nf)) return nn.Sequential(*layers) . and fit our model: . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.139975 | 0.062125 | 0.983400 | 00:13 | . That&#39;s a great result! Let&#39;s take a look at color_dim: . learn.activation_stats.color_dim(-4) . This is just what we hope to see: a smooth development of activations, with no &quot;crashes.&quot; Batchnorm has really delivered on its promise here! In fact, batchnorm has been so successful that we see it (or something very similar) in nearly all modern neural networks. . An interesting observation about models containing batch normalization layers is that they tend to generalize better than models that don&#39;t contain them. Although we haven&#39;t as yet seen a rigorous analysis of what&#39;s going on here, most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and standard deviation than other mini-batches. Therefore, the activations will be normalized by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust to these variations. In general, adding additional randomization to the training process often helps. . Since things are going so well, let&#39;s train for a few more epochs and see how it goes. In fact, let&#39;s increase the learning rate, since the abstract of the batchnorm paper claimed we should be able to &quot;train at much higher learning rates&quot;: . learn = fit(5, lr=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.190594 | 0.121982 | 0.963600 | 00:13 | . 1 | 0.082335 | 0.078569 | 0.976400 | 00:14 | . 2 | 0.053771 | 0.060812 | 0.980500 | 00:13 | . 3 | 0.030423 | 0.029766 | 0.989200 | 00:13 | . 4 | 0.016372 | 0.024753 | 0.991800 | 00:13 | . At this point, I think it&#39;s fair to say we know how to recognize digits! It&#39;s time to move on to something harder... . Conclusions . We&#39;ve seen that convolutions are just a type of matrix multiplication, with two constraints on the weight matrix: some elements are always zero, and some elements are tied (forced to always have the same value). In &lt;&gt; we saw the eight requirements from the 1986 book Parallel Distributed Processing; one of them was &quot;A pattern of connectivity among units.&quot; That&#39;s exactly what these constraints do: they enforce a certain pattern of connectivity.&lt;/p&gt; These constraints allow us to use far fewer parameters in our model, without sacrificing the ability to represent complex visual features. That means we can train deeper models faster, with less overfitting. Although the universal approximation theorem shows that it should be possible to represent anything in a fully connected network in one hidden layer, we&#39;ve seen now that in practice we can train much better models by being thoughtful about network architecture. . Convolutions are by far the most common pattern of connectivity we see in neural nets (along with regular linear layers, which we refer to as fully connected), but it&#39;s likely that many more will be discovered. . We&#39;ve also seen how to interpret the activations of layers in the network to see whether training is going well or not, and how batchnorm helps regularize the training and makes it smoother. In the next chapter, we will use both of those layers to build the most popular architecture in computer vision: a residual network. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Questionnaire . What is a &quot;feature&quot;? | Write out the convolutional kernel matrix for a top edge detector. | Write out the mathematical operation applied by a 3×3 kernel to a single pixel in an image. | What is the value of a convolutional kernel apply to a 3×3 matrix of zeros? | What is &quot;padding&quot;? | What is &quot;stride&quot;? | Create a nested list comprehension to complete any task that you choose. | What are the shapes of the input and weight parameters to PyTorch&#39;s 2D convolution? | What is a &quot;channel&quot;? | What is the relationship between a convolution and a matrix multiplication? | What is a &quot;convolutional neural network&quot;? | What is the benefit of refactoring parts of your neural network definition? | What is Flatten? Where does it need to be included in the MNIST CNN? Why? | What does &quot;NCHW&quot; mean? | Why does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications? | What is a &quot;receptive field&quot;? | What is the size of the receptive field of an activation after two stride 2 convolutions? Why? | Run conv-example.xlsx yourself and experiment with trace precedents. | Have a look at Jeremy or Sylvain&#39;s list of recent Twitter &quot;like&quot;s, and see if you find any interesting resources or ideas there. | How is a color image represented as a tensor? | How does a convolution work with a color input? | What method can we use to see that data in DataLoaders? | Why do we double the number of filters after each stride-2 conv? | Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)? | What information does ActivationStats save for each layer? | How can we access a learner&#39;s callback after training? | What are the three statistics plotted by plot_layer_stats? What does the x-axis represent? | Why are activations near zero problematic? | What are the upsides and downsides of training with a larger batch size? | Why should we avoid using a high learning rate at the start of training? | What is 1cycle training? | What are the benefits of training with a high learning rate? | Why do we want to use a low learning rate at the end of training? | What is &quot;cyclical momentum&quot;? | What callback tracks hyperparameter values during training (along with other information)? | What does one column of pixels in the color_dim plot represent? | What does &quot;bad training&quot; look like in color_dim? Why? | What trainable parameters does a batch normalization layer contain? | What statistics are used to normalize in batch normalization during training? How about during validation? | Why do models with batch normalization layers generalize better? | Further Research . What features other than edge detectors have been used in computer vision (especially before deep learning became popular)? | There are other normalization layers available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed, and how they differ from batch normalization. | Try moving the activation function after the batch normalization layer in conv. Does it make a difference? See what you can find out about what order is recommended, and why. | &lt;/div&gt; . . . . .",
            "url": "https://niyazikemer.com/fastbook/2021/09/04/chapter-13.html",
            "relUrl": "/fastbook/2021/09/04/chapter-13.html",
            "date": " • Sep 4, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Chapter 8 - Collaborative Filtering Deep Dive",
            "content": "This my daughter at the IKEA very close to our home. . import fastbook fastbook.setup_book() from fastbook import * . %config Completer.use_jedi = False . Collaborative filtering modules: . Exploring the data . from fastai.collab import * from fastai.tabular.all import* . Downloading and extracting data from the URL list . path = untar_data(URLs.ML_100k) . Giving columns names and readind first five rows. . ratings = pd.read_csv(path/&#39;u.data&#39;,delimiter = &#39; t&#39;, header= None, engine=&#39;python&#39;,names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . How to recommend movies. Assume the movie has three properties, scince fiction(ness), action, old(ness). . Last skywalker is a sci-fi, and action and not old. . How model learn about our preferences . last_skywalker = np.array([0.98,0.9,-0.9]) . And a user who likes sci-fi and action movies and not so old movies would like this. . user1= np.array([.9,.8,-.6]) . If we multiply these two vectors and sum it. We get: . (user1*last_skywalker).sum() . 2.1420000000000003 . this our matching score, it is a positive value that shows there is a match between the movie and the user1 . casablanka= np.array([-.99,-.33,.8]) . (user1*casablanka).sum() . -1.635 . this is low at this time. There is no match. . Latent Factors . We can pick arbitrary number of parameters for the array. Above, we use three. That could be much more of them. We call them Latent Factors. We start training with random parameters and learn from the ratings given by users. . How to create Dataloaders . . Note: It is not easy to use data as it was. For this dataset, movie id and movie title are not on the same table. . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, engine= &#39;python&#39;,header=None,encoding=&#39;latin1&#39;, usecols=(0,1),names=(&#39;movie&#39;,&#39;title&#39;)) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . Let&#39;s bring ratings and movies together. (movie id will be the key parameter) . ratings=ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . For Dataloaders, we use CollabDataLoaders this Dataloader use first column for the user and second one for the item, in our situation we should change the default one because our item will be title. . dls=CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;,bs=64) dls.show_batch() . user title rating . 0 581 | Brassed Off (1996) | 3 | . 1 864 | Jaws (1975) | 4 | . 2 873 | Contact (1997) | 3 | . 3 58 | Wings of Desire (1987) | 3 | . 4 497 | Hard Target (1993) | 2 | . 5 892 | Jungle Book, The (1994) | 4 | . 6 43 | Santa Clause, The (1994) | 3 | . 7 751 | Strictly Ballroom (1992) | 4 | . 8 894 | Mighty Aphrodite (1995) | 4 | . 9 390 | Spitfire Grill, The (1996) | 5 | . dls.classes[&#39;user&#39;][:15] . (#15) [&#39;#na#&#39;,1,2,3,4,5,6,7,8,9...] . dls.classes[&#39;title&#39;][:15] . (#15) [&#39;#na#&#39;,&#34;&#39;Til There Was You (1997)&#34;,&#39;1-900 (1994)&#39;,&#39;101 Dalmatians (1996)&#39;,&#39;12 Angry Men (1957)&#39;,&#39;187 (1997)&#39;,&#39;2 Days in the Valley (1996)&#39;,&#39;20,000 Leagues Under the Sea (1954)&#39;,&#39;2001: A Space Odyssey (1968)&#39;,&#39;3 Ninjas: High Noon At Mega Mountain (1998)&#39;...] . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors = 5 user_factors = torch.randn(n_users,n_factors) movie_factors = torch.randn(n_movies, n_factors) . More PyTorch Less Python . . Tip: This is how one_hot works. . one_hot(0,5) . tensor([1, 0, 0, 0, 0], dtype=torch.uint8) . Let&#39;s create one_hot such that third index equal to one and the rest of them 0(lenght of number of users). one_hot_3 = one_hot(3,n_users).float() one_hot_3[:10] . tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]) . and multiply by users_factors(matrix multiplication) . user_factors.t() @ one_hot_3 . tensor([ 0.4286, 0.8374, -0.5413, -1.6935, 0.1618]) . This might look a bit daunting but it is not. Basically we want utilize pytorch more and python less. PyTorch very good at matrix multiplication, python is not. With this matrix multiplication we can access every index of the latent factor tensor in one move. Otherwise we would have use regular python loop and index which is very very slow. . This is Python version: . user_factors[3] . tensor([ 0.4286, 0.8374, -0.5413, -1.6935, 0.1618]) . This is same. Great. . Collaborative Filtering from Scratch . At this point there is a section regarding OOP if you want to learn OOP the check the original book page 260 (3rd release) or the course notebook . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) . . Important: This forward method is a bit confusing but I guess what happens there is, x is merged df (it became part of the dls) from above so first column is user id and the second is movie id. check this part: python ratings=ratings.merge(movies) ratings.head() . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . x[0] . tensor([804, 763]) . first one is user id and the second is movie. . y[0] . tensor([5], dtype=torch.int8) . must be the rating. . Let&#39;s train . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.382019 | 1.291539 | 00:06 | . 1 | 1.064109 | 1.072716 | 00:06 | . 2 | 0.977546 | 0.980324 | 00:06 | . 3 | 0.869058 | 0.885319 | 00:06 | . 4 | 0.803102 | 0.871484 | 00:07 | . not bad but we can force our model to make predictions into range 0-5 . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . The dls has values in this range as dependent variables (ratings) and there is a special method in the fastai(I assume) for that. . doc(sigmoid_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.991997 | 0.972494 | 00:08 | . 1 | 0.856079 | 0.889023 | 00:08 | . 2 | 0.677160 | 0.858434 | 00:07 | . 3 | 0.455940 | 0.864097 | 00:07 | . 4 | 0.371842 | 0.868755 | 00:07 | . A little bit better results. . Bias . Sometimes a user could give low (or high) ratings based on his/her subjective preference even the others thinks that is a very good movie. Let&#39;s add a net parameter for that is bias. Bias effects all other parameters in negative or positive way. . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.950526 | 0.923924 | 00:08 | . 1 | 0.811043 | 0.851933 | 00:08 | . 2 | 0.609098 | 0.852216 | 00:08 | . 3 | 0.400987 | 0.877794 | 00:08 | . 4 | 0.289632 | 0.884916 | 00:08 | . And the training loss goes down faster and faster, but valid loss not so. . Weight Decay (or L2 regularization) . from the book: . Weight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.** . Why would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is (&lt;&gt;).&lt;/p&gt; So, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting. . Limiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters): . loss_with_wd = loss + wd * (parameters**2).sum() . In practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing: . parameters.grad += wd * 2 * parameters . In practice, since wd is a parameter that we choose, we can just make it twice as big, so we don&#39;t even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; x = np.linspace(-2,2,100) a_s = [1,2,5,10,50] ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(8,6)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f&#39;a={a}&#39;) ax.set_ylim([0,5]) ax.legend(); . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.977155 | 0.931246 | 00:08 | . 1 | 0.870503 | 0.858914 | 00:07 | . 2 | 0.746876 | 0.823343 | 00:07 | . 3 | 0.573917 | 0.810015 | 00:07 | . 4 | 0.480767 | 0.810702 | 00:07 | . Not so good traing loss but at this time validation loss is far better. . Creating Our Own Embedding Module . class T(Module): def __init__(self): self.a = torch.ones(3) L(T().parameters()) . (#0) [] . There is no pararameters, by its definition parameters must be trainable. . type(torch.ones(3)[0]) . torch.Tensor . . Note: this is tensor not parameter.So it is not trainable. No gradiend tracking. . from the book: . To tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesn&#39;t actually add any functionality (other than automatically calling requires_grad_ for us). It&#39;s only used as a &quot;marker&quot; to show what to include in parameters: . class T(Module): def __init__(self): self.a = nn.Parameter(torch.ones(3)) L(T().parameters()) . (#1) [Parameter containing: tensor([1., 1., 1.], requires_grad=True)] . and . class T(Module): def __init__(self): self.a = nn.Linear(1, 3, bias=False) t = T() L(t.parameters()) . (#1) [Parameter containing: tensor([[-0.0643], [-0.8105], [ 0.1346]], requires_grad=True)] . type(t.a.weight) . torch.nn.parameter.Parameter . . Note: This is a parameter . type(t.a.weight.data) . torch.Tensor . . Note: This is not. . Custom Embedding without using Pytorch Embedding . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) . doc(create_params) . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.925678 | 0.936475 | 00:08 | . 1 | 0.820004 | 0.864908 | 00:08 | . 2 | 0.718156 | 0.818959 | 00:08 | . 3 | 0.589835 | 0.812693 | 00:08 | . 4 | 0.462965 | 0.813873 | 00:08 | . Very similiar results. . Interpreting Embeddings and Biases . Lowest biases in the model. . movie_bias=learn.model.movie_bias.squeeze() idxs=movie_bias.argsort()[0:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Crow: City of Angels, The (1996)&#39;, &#39;Jury Duty (1995)&#39;, &#39;Mortal Kombat: Annihilation (1997)&#39;, &#39;Cable Guy, The (1996)&#39;] . from the book: . Think about what this means. What it&#39;s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don&#39;t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias: . idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Titanic (1997)&#39;, &#34;Schindler&#39;s List (1993)&#34;, &#39;Star Wars (1977)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Rear Window (1954)&#39;] . from the book: . So, for instance, even if you don&#39;t normally enjoy detective movies, you might enjoy LA Confidential! . It is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course Computational Linear Algebra for Coders. &lt;&gt; shows what our movies look like based on two of the strongest PCA components.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . Lets try changing X axis. . g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = list(range(50)) X = fac1[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . Very interesting to study changes. . Using fastai.collab&#182; . Same thing with fastai collab_learner . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.940206 | 0.939116 | 00:07 | . 1 | 0.886674 | 0.867349 | 00:07 | . 2 | 0.750853 | 0.824058 | 00:07 | . 3 | 0.610644 | 0.811104 | 00:07 | . 4 | 0.497868 | 0.812237 | 00:07 | . learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1665, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1665, 1) ) . movie_bias = learn.model.i_bias.weight.squeeze() idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#34;Schindler&#39;s List (1993)&#34;, &#39;Titanic (1997)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Rear Window (1954)&#39;, &#39;Star Wars (1977)&#39;] . similar results. . Embedding Distance . Basically it means if two movies has similar latent factors.(embedding vector) This is the movie very similar latent factors with Silence of the lambs. . movie_factors = learn.model.i_weight.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Silence of the Lambs, The (1991)&#39;] distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) idx = distances.argsort(descending=True)[1] dls.classes[&#39;title&#39;][idx] . &#39;Body Snatcher, The (1945)&#39; . Bootstrapping a Collaborative Filtering Model . read the all section from the original book at page 270 (3rd release) or the course notebook . Deep Learning for Collaborative Filtering . First fastai could make a recommendation for right embedding sizes(latent factors). . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) . learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.959750 | 0.937144 | 00:09 | . 1 | 0.919930 | 0.894244 | 00:08 | . 2 | 0.857974 | 0.870025 | 00:08 | . 3 | 0.814399 | 0.854047 | 00:08 | . 4 | 0.763636 | 0.860031 | 00:08 | . with one step. . above is possibble(again) with collab_learner with one step. just use use_nn=True. . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.989683 | 0.959795 | 00:10 | . 1 | 0.902582 | 0.904747 | 00:10 | . 2 | 0.864139 | 0.879289 | 00:10 | . 3 | 0.824376 | 0.847727 | 00:10 | . 4 | 0.790679 | 0.850178 | 00:10 | . from the book: . Although the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That&#39;s exactly what TabularModel does. In fact, we&#39;ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, we&#39;d better spend some time learning about TabularModel, and how to use it to get great results! We&#39;ll do that in the next chapter. . &lt;/div&gt; .",
            "url": "https://niyazikemer.com/fastbook/2021/09/01/chapter-8.html",
            "relUrl": "/fastbook/2021/09/01/chapter-8.html",
            "date": " • Sep 1, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Chapter 7 - Training a State-of-the-Art Model",
            "content": "This my favorite Turkish coffe cup, designed by German-Turkish artist Taner Ceylan, check his works at here . import fastbook fastbook.setup_book() %config Completer.use_jedi = False . Imagenette . Imagenette is a subset of ImageNet that contains 10 classes from the full ImageNet that looked very different from one another. Considering the size of ImageNet, it is very costly and time consuming to create a prototype for your project. Smaller datasets lets you make much more experiments, and could provide insight for your projects direction. . from fastai.vision.all import * path = untar_data(URLs.IMAGENETTE) . dblock = DataBlock(blocks = (ImageBlock(),CategoryBlock()), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = dblock.dataloaders(path,bs=64) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . model=xresnet50(n_out=dls.c) learn=Learner(dls,model,loss_func=CrossEntropyLossFlat(),metrics=accuracy) learn.fit_one_cycle(5,3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.581075 | 3.990604 | 0.335325 | 01:11 | . 1 | 1.188324 | 2.071529 | 0.488798 | 01:12 | . 2 | 0.967764 | 1.166690 | 0.639656 | 01:12 | . 3 | 0.723403 | 0.728145 | 0.770724 | 01:12 | . 4 | 0.571699 | 0.579214 | 0.828603 | 01:12 | . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . Normalization . Normalized data helps better results. Normalization is your data has a mean of 0 and standart deviation of 1. But our data encoded with numbers between 0 and 255 or sometimes 0-1. Lets check the data in the Imaginette: . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.4661, 0.4575, 0.4309], device=&#39;cuda:0&#39;), TensorImage([0.2791, 0.2752, 0.2898], device=&#39;cuda:0&#39;)) . Our data is around 0.5 mean and 0.3 deviation. So it is not in desirable range.With fastai it is possible to normalize our data by adding Normalize transform. . def get_dls(bs, size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . dls = get_dls(64, 224) . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([-0.2460, -0.1802, -0.0632], device=&#39;cuda:0&#39;), TensorImage([1.2249, 1.1904, 1.2784], device=&#39;cuda:0&#39;)) . Now it is better. Let&#39;s check it if it helped the training process. Same code again for the training. . model = xresnet50(n_out=dls.c) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.612493 | 2.099523 | 0.436146 | 01:12 | . 1 | 1.253520 | 1.564609 | 0.538462 | 01:12 | . 2 | 0.957898 | 1.758915 | 0.567961 | 01:14 | . 3 | 0.760550 | 0.672671 | 0.788648 | 01:14 | . 4 | 0.613525 | 0.580995 | 0.819268 | 01:13 | . a little bit better but Normalization is much more important when we use pretrained model. Normalizing our data with the original data statistic helps better transfer learning results. . Progressive Resizing . from the book: . Spending most of the epochs training with small images, helps training complete much faster. Completing training using large images makes the final accuracy much higher. We call this approach progressive resizing. . This my check on using progressive resizing . import time start_time = time.time() dls = get_dls(128, 128) learn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(4, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.628276 | 3.793727 | 0.295370 | 00:35 | . 1 | 1.250497 | 1.006853 | 0.675878 | 00:36 | . 2 | 0.945165 | 0.896517 | 0.711352 | 00:35 | . 3 | 0.750154 | 0.655099 | 0.798730 | 00:35 | . learn.dls = get_dls(64, 224) learn.fine_tune(6, 3e-3) print(&quot; %s seconds &quot; % (time.time() - start_time)) . epoch train_loss valid_loss accuracy time . 0 | 1.072026 | 1.799888 | 0.481703 | 01:11 | . epoch train_loss valid_loss accuracy time . 0 | 0.740281 | 0.882515 | 0.753174 | 01:11 | . 1 | 0.772105 | 0.909184 | 0.714339 | 01:11 | . 2 | 0.671060 | 0.985478 | 0.727035 | 01:11 | . 3 | 0.588883 | 0.552914 | 0.830471 | 01:11 | . 4 | 0.464459 | 0.420264 | 0.870052 | 01:13 | . 5 | 0.404156 | 0.390893 | 0.877147 | 01:12 | . 649.6742904186249 seconds . import time start_time = time.time() dls = get_dls(32, 224) learn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(8, 3e-3) print(&quot; %s seconds &quot; % (time.time() - start_time)) . epoch train_loss valid_loss accuracy time . 0 | 1.662178 | 1.874999 | 0.467886 | 01:22 | . 1 | 1.307072 | 1.275218 | 0.576176 | 01:23 | . 2 | 1.070854 | 1.173411 | 0.638536 | 01:23 | . 3 | 0.842672 | 0.831104 | 0.728902 | 01:23 | . 4 | 0.699521 | 0.746880 | 0.774832 | 01:24 | . 5 | 0.579603 | 0.524914 | 0.828603 | 01:23 | . 6 | 0.457707 | 0.423468 | 0.868559 | 01:24 | . 7 | 0.401849 | 0.415911 | 0.872293 | 01:23 | . 670.1757352352142 seconds . I&#39;ve changed some hyperparameters like number of epochs and learning rate. It is faster and better result most of the time(not in every situation), nice. . Test Time Augmentation . Random cropping sometimes leads suprising problems.Especially if it used with multicategory images, for example the objects in the image that close to edges could be ignored totaly. There are some workarounds to solve this problem (squish or stretch them)but most of them couse other kind of problems that could hurt the results. Only downside is validation time would be slower. . . Warning: How is it possible? Since we do not use validation loss for backpropagation how come it improves our results. . preds,targs = learn.tta() accuracy(preds, targs).item() . . 0.8760268688201904 . from the book: . jargon: test time augmentation (TTA): During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image. . Mixup . Especially used when we don&#39;t have enough data and do not have pretrained model that was trained on similar to our dataset. . from the book: Mixup works as follows, for each image: . Select another image from your dataset at random. | Pick a weight at random. | Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable. | Take a weighted average (with the same weight) of this image&#39;s labels with your image&#39;s labels; this will be your dependent variable. | The paper explains: &quot;While data augmentation consistently leads to improved generalization, the procedure is dataset-dependent, and thus requires the use of expert knowledge.&quot; For instance, it&#39;s common to flip images as part of data augmentation, but should you flip only horizontally, or also vertically? The answer is that it depends on your dataset. In addition, if flipping (for instance) doesn&#39;t provide enough data augmentation for you, you can&#39;t &quot;flip more.&quot; It&#39;s helpful to have data augmentation techniques where you can &quot;dial up&quot; or &quot;dial down&quot; the amount of change, to see what works best for you. . shows what it looks like when we take a linear combination of images, as done in Mixup. . I&#39;ve replaced these rows like above. It seems there is no get_image_files_sorted method in the fastai. . church = PILImage.create(get_image_files_sorted(path/&#39;train&#39;/&#39;n03028079&#39;)[0]) gas = PILImage.create(get_image_files_sorted(path/&#39;train&#39;/&#39;n03425413&#39;)[0]) . Label Smoothing . . Warning: check the original notebook for this part. Only thing I can say is, it used for making the model less confident for the classification to overcome overfitting. .",
            "url": "https://niyazikemer.com/fastbook/2021/08/15/chapter-7.html",
            "relUrl": "/fastbook/2021/08/15/chapter-7.html",
            "date": " • Aug 15, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Chapter 6 - Other Computer Vision Problems",
            "content": "My Deep Learning for Coders with fastai &amp; PyTorch has arrived. It is very good for taking notes on it directly. (Some chapters are slightly different from the notebook version) . #!pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastbook import * . Multi-Label Classification . I think main theme of this lesson is &quot;Binary Cross Entropy&quot;. It is important when a photo have more than one category or when there is no category to find. Think about the Bear classifier. It classifies between two. But if there is an another bear breed in the picture it just tries to pick either black or grizzlie anyway which is not good, or if there is one grizzlie and one rabbit, probably it would be confused between these labels, at least its confidence level would be lower. . The Dataset . from fastai.vision.all import * path = untar_data(URLs.PASCAL_2007) . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . . Note: pd at the beginning is pandas it is library for creating data frames df from csv files. Data frame is a table contains columns and rows. there are some rows that contains more than one labels. Check row no 2. . Pandas and DataFrames . df.iloc[:,0] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . . Note: Very easy to navigate in a dataframe. . df.iloc[0,:] # Trailing :s are always optional (in numpy, pytorch, pandas, etc.), # so this is equivalent: df.iloc[0] . fname 000005.jpg labels chair is_valid True Name: 0, dtype: object . df[&#39;fname&#39;] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . . Note: it is possible to use column names to select a column in the dataframe. . tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) tmp_df . a b . 0 1 | 3 | . 1 2 | 4 | . tmp_df[&#39;c&#39;] = tmp_df[&#39;a&#39;]+tmp_df[&#39;b&#39;] tmp_df . a b c . 0 1 | 3 | 4 | . 1 2 | 4 | 6 | . . Note: It is also possible to create new column. . . Note: From the book: . Pandas is a fast and flexible library, and an important part of every data scientist’s Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it takes a while to get familiar with it. If you haven’t used Pandas before, we’d suggest going through a tutorial; we are particularly fond of the book Python for Data Analysis by Wes McKinney, the creator of Pandas (O&#39;Reilly). It also covers other important libraries like matplotlib and numpy. We will try to briefly describe Pandas functionality we use as we come across it, but will not go into the level of detail of McKinney’s book. . Constructing a DataBlock . Dataset and Dataloader . Dataset: Anything in which we can index to it and you can take the length to it. | DataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables | . a = list (enumerate(string.ascii_lowercase)) a[0],len(a) . ((0, &#39;a&#39;), 26) . . Note: above index and length. . dl_a = DataLoader(a, batch_size=8, shuffle=True) b = first(dl_a) b . (tensor([ 6, 11, 13, 0, 8, 22, 10, 3]), (&#39;g&#39;, &#39;l&#39;, &#39;n&#39;, &#39;a&#39;, &#39;i&#39;, &#39;w&#39;, &#39;k&#39;, &#39;d&#39;)) . . Note: batch_size=8 for mini-batch size. first just takes the first batch. . list(zip(b[0],b[1])) . [(tensor(6), &#39;g&#39;), (tensor(11), &#39;l&#39;), (tensor(13), &#39;n&#39;), (tensor(0), &#39;a&#39;), (tensor(8), &#39;i&#39;), (tensor(22), &#39;w&#39;), (tensor(10), &#39;k&#39;), (tensor(3), &#39;d&#39;)] . . Note: This is how you can see which independent and dependent variables are correspond each other. . list(zip(*b)) . [(tensor(6), &#39;g&#39;), (tensor(11), &#39;l&#39;), (tensor(13), &#39;n&#39;), (tensor(0), &#39;a&#39;), (tensor(8), &#39;i&#39;), (tensor(22), &#39;w&#39;), (tensor(10), &#39;k&#39;), (tensor(3), &#39;d&#39;)] . . Note: Short cut for zipping.*try to understand) It used for transposing (JH said in the lesson video, check it how) . Dataset(s) and Dataloader(s) . Datasets: An object that contains a training Dataset and a validation Dataset | DataLoaders: An object that contains a training DataLoader and a validation DataLoader | . a = list (string.ascii_lowercase) a[0],len(a) . (&#39;a&#39;, 26) . similar dataset as previous one. but there is no enumeration. . dss = Datasets(a) dss[0] . (&#39;a&#39;,) . For creating our dependent and independent variable we can use functions. e.g.: . def f1 (o): return o+&#39;a&#39; def f2 (o): return o+&#39;b&#39; . dss = Datasets(a,[[f1]]) dss[0] . (&#39;aa&#39;,) . dss = Datasets(a,[[f1,f2]]) dss[0] . (&#39;aab&#39;,) . . Note: that means if we have &#8217;a&#8217; in the inital dataset, our independent value should be &#8217;aa&#8217; and our dependent should be &#8217;ab&#8217;. But it is not at the moment. [[f1,f2]]) is a list of lists and if we change the shape of the input arguments a bit: . dss = Datasets(a,[[f1],[f2]]) dss[0] . (&#39;aa&#39;, &#39;ab&#39;) . . Note: Now we are good to go. Now we can create our Dataloaders from our Datasets. . dls = DataLoaders.from_dsets(dss, batch_size=4) . first(dls.train) . ((&#39;va&#39;, &#39;ra&#39;, &#39;ea&#39;, &#39;ua&#39;), (&#39;vb&#39;, &#39;rb&#39;, &#39;eb&#39;, &#39;ub&#39;)) . . Note: Our dataloaders is ready. This is how we create dataloaders from scratch. . What is DataBlock ? . . Note: There is much more easier way to create our datasets. . dblock = DataBlock() . . Note: An empty DataBlock. . dsets = dblock.datasets(df) . . Note: From the book: . We can create a Datasets object from this. The only thing needed is a source—in this case, our DataFrame df . len(dsets.train),len(dsets.valid) . (4009, 1002) . Our training and validation sets are ready. How? First: if we didn&#39;t give any argument for splitting then the split is random and the split ratio is %20. . x,y = dsets.train[0] x,y . (fname 001293.jpg labels dog is_valid True Name: 646, dtype: object, fname 001293.jpg labels dog is_valid True Name: 646, dtype: object) . This is first row of the batch repeated twice. (this is how default value works) . x[&#39;fname&#39;] . &#39;001293.jpg&#39; . . Note: However we need file name (fname) as a independent and labels as dependent variables. . dblock = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[0] . (&#39;006610.jpg&#39;, &#39;diningtable bottle person&#39;) . . Note: like this. . Same thing with functions without lambda functions. Most of the time it is much more relevant because Lambda causes problems if you try to serialize the model. . def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;] dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (&#39;006828.jpg&#39;, &#39;bottle&#39;) . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (Path(&#39;/home/niyazi/.fastai/data/pascal_2007/train/005042.jpg&#39;), [&#39;bicycle&#39;, &#39;person&#39;]) . . Note: all are ok again. Datasets object is ready, the shape is right. . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=375x500, TensorMultiCategory([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . . Note: blocks types are important, in previous lessons we used different type of blocks. Based on selected type Datablock gains additional capabilities. In the current one ImageBlock help us to see the information as image. MultiCategoryBlock encodes labels as a tensor that every index correspond to a object label. (onehot encoding). Only one thing I do not understand about it that how fastai understands number of categories.(total 20 now) . idxs = torch.where(dsets.train[0][1]==1.)[0] print(idxs) dsets.train.vocab[idxs] . TensorMultiCategory([4]) . (#1) [&#39;bottle&#39;] . . Note: Example above there are two categories. (it changes every run) . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . . Note: ~ is a bitwise operation that reverses bits. . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(df) . dls.show_batch(nrows=1, ncols=3) . What is Binary Cross-Entropy ? . learn = cnn_learner(dls, resnet18) . x,y = to_cpu(dls.train.one_batch()) activs = learn.model(x) activs.shape . torch.Size([64, 20]) . . Note: model refers to resnet at this statement and activs are activation from the last layer of the resnet18 for one batch. Jeremy says &#8217;learn.model(x)&#8217; is plain pytorch. (didn&#8217;t know it) . activs[0] . tensor([ 1.1090, -1.4315, 2.8930, 0.5827, 3.0797, 2.5147, -1.3310, 1.7237, -0.2547, 0.3985, -0.2740, -0.1811, -1.5258, -1.0918, -1.7862, 0.3597, -0.4354, -0.1203, 2.2807, -0.3097], grad_fn=&lt;SelectBackward&gt;) . . Note: Values are not between 0 and 1. . def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() . . Note: a couple of things going on here: - Sigmoid brings everything between zero and one. . Log just adjust results such a way based on their relative confidence level. (Check the section on Chapter -5) | Broadcasting. We&#39;ll get the results for every item. | . loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . TensorMultiCategory(1.0493, grad_fn=&lt;AliasBackward&gt;) . . Important: Although there is a pytorch equivalent for our binary_cross_entropy ( F.binary_cross_entropy and nn.BCELoss) they don&#8217;t include sigmoid. So instead we use F.binary_cross_entropy_with_logits or nn.BCEWithLogitsLoss . . . Note: Direct from the book: . We don&#39;t actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default. . One change compared to the last chapter is the metric we use: because this is a multilabel problem, we can&#39;t use the accuracy function. Why is that? Well, accuracy was comparing our outputs to our targets like so: . def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred = inp.argmax(dim=axis) return (pred == targ).float().mean() . The class predicted was the one with the highest activation (this is what argmax does). Here it doesn&#39;t work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a threshold. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0: . def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . . Note: We need to pass our accuracy function to the learner for getting accuracy. See learn statement below. The only problem is our default value is 0.5. When we need an another value, we need to use Python partial functionality. See usage below or check original file check original file and video. . What is partial function ? . it is used when there is a need to change default values. . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) . (&#39;Hello Jeremy.&#39;, &#39;Ahoy! Jeremy.&#39;) . f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) . (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.936221 | 0.694192 | 0.238267 | 00:11 | . 1 | 0.821117 | 0.555462 | 0.291215 | 00:11 | . 2 | 0.600436 | 0.203543 | 0.820837 | 00:10 | . 3 | 0.357310 | 0.127290 | 0.935956 | 00:10 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.133780 | 0.117657 | 0.943227 | 00:12 | . 1 | 0.117373 | 0.106338 | 0.949582 | 00:12 | . 2 | 0.097523 | 0.102176 | 0.954183 | 00:12 | . How to find right threshold ? . learn.metrics = partial(accuracy_multi, thresh=0.1) learn.validate() . (#2) [0.10217633843421936,0.9333268404006958] . . Note: Low threshold (selects even on low confidence) . learn.metrics = partial(accuracy_multi, thresh=0.99) learn.validate() . (#2) [0.10217633843421936,0.9434263110160828] . . Note: High threshold (selects only on high confidence) . preds,targs = learn.get_preds() . accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) . TensorBase(0.9558) . . Note: This a better way to pick the right threshold value.Testing for a range of values. . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . 0.5 looks best. . Image Regression . . Note: Classification is used for finding right classes and Regression for continuous values, e.g. house prices or a coordinates of something or length etc... . Assemble the Data . path = untar_data(URLs.BIWI_HEAD_POSE) . . Note: In this exaple we will find center point of a heads. . Path.BASE_PATH = path . path.ls().sorted() . (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...] . (path/&#39;01&#39;).ls().sorted() . (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...] . . Note: From the book: . Inside the subdirectories, we have different frames, each of them come with an image (_rgb.jpg) and a pose file (_pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that converts an image filename to its associated pose file: | . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) . Path(&#39;03/frame_00668_pose.txt&#39;) . img2pose creates a path based for coordinate file based on image name. . im = PILImage.create(img_files[0]) im.shape . (480, 640) . im.to_thumb(160) . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . . Note: From the book: - The Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren&#39;t important for our purposes, so we&#39;ll just show the function we use to extract the head center point: . get_ctr(img_files[0]) . tensor([447.7369, 283.9802]) . biwi = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) . . Note: Most important thing about this DataBlock is splitter, basically we only use person no 13 see explanation(of course lots of pics of the person no:13), if we&#8217;d split randomly then there would be a very high chance for same person to be in the both training and validations sets. (there are lots of pictures of same person in this dataset) Also see dependent variable is continious value which is PointBlock as coordinates. There is also a normalization process as batch_tmfs. . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . xb,yb = dls.one_batch() xb.shape,yb.shape . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) is important. . 64 : batch size | 3 : RGB | 240,320: image size | 1,2 : one row with two values (one point with two coordinates) | . yb[0] . TensorPoint([[-0.1250, 0.0771]], device=&#39;cuda:0&#39;) . Dependent variable. . Training the Model . learn = cnn_learner(dls, resnet18, y_range=(-1,1)) . y_range=(-1,1) is important we tell fast ai that we need results in this range.(for coordinates) y_range is implemented in fastai using sigmoid_range, which is defined as: . def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo . plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) . dls.loss_func . FlattenedLoss of MSELoss() . Default value . learn.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/callback/schedule.py:270: UserWarning: color is redundantly defined by the &#39;color&#39; keyword argument and the fmt string &#34;ro&#34; (-&gt; color=&#39;r&#39;). The keyword argument will take precedence. ax.plot(val, idx, &#39;ro&#39;, label=nm, c=color) . SuggestedLRs(valley=tensor(0.0012)) . lr = 1e-2 learn.fine_tune(3, lr) . epoch train_loss valid_loss time . 0 | 0.049803 | 0.002713 | 00:46 | . epoch train_loss valid_loss time . 0 | 0.008684 | 0.002087 | 00:56 | . 1 | 0.003187 | 0.000621 | 00:56 | . 2 | 0.001467 | 0.000064 | 00:57 | . math.sqrt(0.0001) . 0.01 . learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . Questionnaire . How could multi-label classification improve the usability of the bear classifier? | How do we encode the dependent variable in a multi-label classification problem? | How do you access the rows and columns of a DataFrame as if it was a matrix? | How do you get a column by name from a DataFrame? | What is the difference between a Dataset and DataLoader? | What does a Datasets object normally contain? | What does a DataLoaders object normally contain? | What does lambda do in Python? | What are the methods to customize how the independent and dependent variables are created with the data block API? | Why is softmax not an appropriate output activation function when using a one hot encoded target? | Why is nll_loss not an appropriate loss function when using a one-hot-encoded target? | What is the difference between nn.BCELoss and nn.BCEWithLogitsLoss? | Why can&#39;t we use regular accuracy in a multi-label problem? | When is it okay to tune a hyperparameter on the validation set? | How is y_range implemented in fastai? (See if you can implement it yourself and test it without peeking!) | What is a regression problem? What loss function should you use for such a problem? | What do you need to do to make sure the fastai library applies the same data augmentation to your inputs images and your target point coordinates? | Further Research . Read a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book&#39;s website for recommended tutorials. | Retrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don&#39;t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification. |",
            "url": "https://niyazikemer.com/fastbook/2021/08/02/chapter-6.html",
            "relUrl": "/fastbook/2021/08/02/chapter-6.html",
            "date": " • Aug 2, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Chapter 5 - Image Classification",
            "content": ". . I&#39;m a Doctor Who fan and this is my cyberman coffee cup, as I remember got it from Manchester Science Museum. . . import fastbook fastbook.setup_book() %config Completer.use_jedi = False . from fastbook import * . [[chapter_pet_breeds]] . PLAYING WITH THE DATASET . from fastai.vision.all import * path = untar_data(URLs.PETS) . . Note: With untar we download the data. This data originally come from Oxford University Visual Geomety Group and our dataset is here: . path . Path(&#39;/home/niyazi/.fastai/data/oxford-iiit-pet&#39;) . . Note: This is the local download path for my computer. . Path.BASE_PATH = path . . Tip: This is a trick to get the relative path, check above and below . path . Path(&#39;.&#39;) . Now the path is looks different. . path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . . Note: #2 is number of item in the list. annotations represents target variables of this datasets but we do not use them at this time instead we create our own labels. . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/staffordshire_bull_terrier_90.jpg&#39;),Path(&#39;images/Russian_Blue_70.jpg&#39;),Path(&#39;images/japanese_chin_69.jpg&#39;),Path(&#39;images/Maine_Coon_266.jpg&#39;),Path(&#39;images/japanese_chin_200.jpg&#39;),Path(&#39;images/Siamese_57.jpg&#39;),Path(&#39;images/Persian_175.jpg&#39;),Path(&#39;images/havanese_81.jpg&#39;),Path(&#39;images/Birman_72.jpg&#39;),Path(&#39;images/leonberger_55.jpg&#39;)...] . fname = (path/&quot;images&quot;).ls()[0] . fname . Path(&#39;images/staffordshire_bull_terrier_90.jpg&#39;) . . Note: The first image in the path list. . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;staffordshire_bull_terrier&#39;] . . Note: Since we don&#8217;t use the annonations in the Dataset we need to find a way to get breeds form the filename. This is regex findall method, Check geeksforgeeks.org tutorial here . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . . Note: now find all names with RegexLabeller. The item_tmsf and batch_transfdrms may look a bit meaningless. Check below to find out why. . . PRESIZING . As a summary FastAi gives a chance to augment our images in a smarter way (presizing) such that provide much more detail and information for the training. First, we presize images with item_tfms then push them to GPU and use augmentation. . check the original document for the whole idea . #caption A comparison of fastai&#39;s data augmentation strategy (left) and the traditional approach (right). dblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_y=parent_label, item_tfms=Resize(460)) # Place an image in the &#39;images/grizzly.jpg&#39; subfolder where this notebook is located before running this dls1 = dblock1.dataloaders([(Path.cwd()/&#39;images&#39;/&#39;chapter-05&#39;/&#39;grizzly.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . dls.show_batch(nrows=3, ncols=3) . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /home/niyazi/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /home/niyazi/.fastai/data/oxford-iiit-pet/images/British_Shorthair_110.jpg applying PILBase.create gives PILImage mode=RGB size=500x333 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /home/niyazi/.fastai/data/oxford-iiit-pet/images/British_Shorthair_110.jpg applying partial gives British_Shorthair applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(4) Final sample: (PILImage mode=RGB size=500x333, TensorCategory(4)) Collecting items from /home/niyazi/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} Building one batch Applying item_tfms to the first sample: Pipeline: ToTensor starting from (PILImage mode=RGB size=500x333, TensorCategory(4)) applying ToTensor gives (TensorImage of size 3x333x500, TensorCategory(4)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Error! It&#39;s not possible to collate your items in a batch Could not collate the 0-th members of your tuples because got the following shapes torch.Size([3, 333, 500]),torch.Size([3, 500, 396]),torch.Size([3, 375, 500]),torch.Size([3, 500, 281]) . RuntimeError Traceback (most recent call last) &lt;ipython-input-15-ead0dd2a047d&gt; in &lt;module&gt; 3 splitter=RandomSplitter(seed=42), 4 get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) -&gt; 5 pets1.summary(path/&#34;images&#34;) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs) 188 why = _find_fail_collate(s) 189 print(&#34;Make sure all parts of your samples are tensors of the same size&#34; if why is None else why) --&gt; 190 raise e 191 192 if len([f for f in dls.train.after_batch.fs if f.name != &#39;noop&#39;])!=0: ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs) 182 print(&#34; nCollating items in a batch&#34;) 183 try: --&gt; 184 b = dls.train.create_batch(s) 185 b = retain_types(b, s[0] if is_listy(s) else s) 186 except Exception as e: ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in create_batch(self, b) 141 elif s is None: return next(self.it) 142 else: raise IndexError(&#34;Cannot index an iterable dataset numerically - must use `None`.&#34;) --&gt; 143 def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b) 144 def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b) 145 def to(self, device): self.device = device ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in fa_collate(t) 48 b = t[0] 49 return (default_collate(t) if isinstance(b, _collate_types) &gt; 50 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 51 else default_collate(t)) 52 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in &lt;listcomp&gt;(.0) 48 b = t[0] 49 return (default_collate(t) if isinstance(b, _collate_types) &gt; 50 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 51 else default_collate(t)) 52 ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/data/load.py in fa_collate(t) 47 &#34;A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s&#34; 48 b = t[0] &gt; 49 return (default_collate(t) if isinstance(b, _collate_types) 50 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 51 else default_collate(t)) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py in default_collate(batch) 54 storage = elem.storage()._new_shared(numel) 55 out = elem.new(storage) &gt; 56 return torch.stack(batch, 0, out=out) 57 elif elem_type.__module__ == &#39;numpy&#39; and elem_type.__name__ != &#39;str_&#39; 58 and elem_type.__name__ != &#39;string_&#39;: ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs) 338 convert=False 339 if _torch_handled(args, self._opt, func): convert,types = type(self),(torch.Tensor,) --&gt; 340 res = super().__torch_function__(func, types, args=args, kwargs=kwargs) 341 if convert: res = convert(res) 342 if isinstance(res, TensorBase): res.set_meta(self, as_copy=True) ~/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/_tensor.py in __torch_function__(cls, func, types, args, kwargs) 1021 1022 with _C.DisableTorchFunction(): -&gt; 1023 ret = func(*args, **kwargs) 1024 return _convert(ret, cls) 1025 RuntimeError: stack expects each tensor to be equal size, but got [3, 333, 500] at entry 0 and [3, 500, 396] at entry 1 . . Note: It is alway good to get a quick summary. pets1.summary(path/&quot;images&quot;) Check the summary above, it has lots of details. It is natural to get an error in this example because we are trying the put diffent size images into the same DataBlock. . . BASELINE MODEL . For every project, just start with a Baseline. Baseline is a good point to think about the project/domain/problem at the same time, then start improve and make experiments about architecture, hyperparameters etc. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss error_rate time . 0 | 1.513288 | 0.355303 | 0.110284 | 00:22 | . epoch train_loss valid_loss error_rate time . 0 | 0.518711 | 0.313168 | 0.106225 | 00:27 | . 1 | 0.325613 | 0.261644 | 0.089310 | 00:27 | . . Note: A basic run is helpful as baseline for the beginning. . Defaults for the baseline . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . learn.lr . 0.001 . . Tip: Very easy to see default arguments for the learner. Above loss function loss_func and learning rate lr. . One Batch Run . first(dls.train) . (TensorImage([[[[ 7.7591e-02, -1.3409e-01, 1.4352e-01, ..., -8.8188e-01, -8.0163e-01, -1.4735e-01], [ 1.9115e-03, 4.8835e-01, 4.3845e-01, ..., -1.3028e+00, -1.4314e+00, -1.2478e+00], [-1.2349e-01, 7.3246e-02, -9.2777e-02, ..., -7.9699e-01, -1.1984e+00, -9.0709e-02], ..., [-1.4486e+00, -9.5970e-01, 8.6840e-02, ..., -1.1097e+00, -3.3829e-01, 8.2527e-02], [-1.4246e+00, -8.2784e-01, 8.7511e-02, ..., -9.5360e-01, -1.0563e-01, -5.1489e-01], [-1.3575e+00, -7.6923e-01, 1.0015e-01, ..., -1.0628e+00, 4.3092e-02, -6.2399e-01]], [[ 2.5566e-01, 7.5052e-02, 2.0962e-01, ..., -9.7342e-01, -8.9785e-01, -1.5707e-01], [ 8.3578e-02, 6.1146e-01, 5.1947e-01, ..., -1.3980e+00, -1.5514e+00, -1.3726e+00], [-1.2059e-02, 1.2505e-01, -2.9267e-03, ..., -9.0869e-01, -1.3052e+00, -2.3089e-01], ..., [-1.4979e+00, -1.1395e+00, -2.8139e-01, ..., -1.3591e+00, -4.8733e-01, -2.1415e-01], [-1.4548e+00, -9.8541e-01, -2.7210e-01, ..., -1.1278e+00, -3.0796e-01, -8.4852e-01], [-1.3689e+00, -9.2548e-01, -2.6808e-01, ..., -1.2366e+00, -6.3006e-02, -1.0183e+00]], [[-1.1168e+00, -1.2721e+00, -1.0968e+00, ..., -1.1363e+00, -9.8121e-01, -3.4084e-01], [-1.0031e+00, -6.8494e-01, -8.5066e-01, ..., -1.5088e+00, -1.6080e+00, -1.4639e+00], [-1.1476e+00, -1.0927e+00, -1.3264e+00, ..., -1.0406e+00, -1.3088e+00, -3.4494e-01], ..., [-1.4021e+00, -9.7390e-01, -4.7906e-01, ..., -1.4878e+00, -5.0896e-01, -3.1871e-01], [-1.3213e+00, -8.4023e-01, -5.3294e-01, ..., -1.3262e+00, -5.3787e-01, -1.0765e+00], [-1.1781e+00, -8.0876e-01, -5.8936e-01, ..., -1.3399e+00, -4.2362e-01, -1.1124e+00]]], [[[ 1.9623e+00, 2.0361e+00, 1.9064e+00, ..., 2.2392e+00, 2.2249e+00, 2.2211e+00], [ 2.0734e+00, 2.0294e+00, 2.1349e+00, ..., 2.2461e+00, 2.2249e+00, 2.2376e+00], [ 2.0202e+00, 1.9569e+00, 1.8405e+00, ..., 2.2373e+00, 2.2353e+00, 2.2223e+00], ..., [ 3.5436e-01, 2.5449e-01, 5.5067e-01, ..., 1.0332e+00, 1.0161e+00, 9.8812e-01], [ 3.5005e-01, 1.6332e-01, 3.8754e-01, ..., 9.7724e-01, 9.6458e-01, 1.0630e+00], [ 3.4791e-01, 8.2361e-02, 2.2118e-01, ..., 8.6120e-01, 1.0850e+00, 1.1228e+00]], [[ 2.1661e+00, 2.2408e+00, 2.0968e+00, ..., 1.6825e+00, 1.6601e+00, 1.6328e+00], [ 2.2785e+00, 2.2232e+00, 2.3262e+00, ..., 1.6905e+00, 1.6537e+00, 1.6441e+00], [ 2.2230e+00, 2.1458e+00, 2.0269e+00, ..., 1.6834e+00, 1.6742e+00, 1.6310e+00], ..., [ 8.0271e-01, 7.3445e-01, 1.0265e+00, ..., 8.7042e-01, 8.4557e-01, 8.8788e-01], [ 8.1180e-01, 6.5414e-01, 8.8815e-01, ..., 8.1193e-01, 7.7473e-01, 9.1055e-01], [ 8.1687e-01, 5.4252e-01, 6.8486e-01, ..., 6.5186e-01, 9.1647e-01, 9.5377e-01]], [[ 2.3636e+00, 2.4168e+00, 2.2347e+00, ..., 1.7282e+00, 1.6718e+00, 1.6554e+00], [ 2.4760e+00, 2.3875e+00, 2.4613e+00, ..., 1.7363e+00, 1.6699e+00, 1.6726e+00], [ 2.4182e+00, 2.2941e+00, 2.1473e+00, ..., 1.7294e+00, 1.6948e+00, 1.6592e+00], ..., [ 1.4156e+00, 1.3690e+00, 1.6562e+00, ..., 1.2044e+00, 1.2088e+00, 1.2487e+00], [ 1.4260e+00, 1.3050e+00, 1.5455e+00, ..., 1.0673e+00, 1.0365e+00, 1.1483e+00], [ 1.4369e+00, 1.2059e+00, 1.3302e+00, ..., 7.4460e-01, 9.8735e-01, 9.8728e-01]]], [[[ 7.9667e-01, 6.5725e-01, 6.7499e-01, ..., 2.2489e+00, 2.2489e+00, 2.2489e+00], [ 1.6647e+00, 1.8548e+00, 4.2411e-01, ..., 2.2489e+00, 2.2489e+00, 2.2489e+00], [ 2.0417e+00, 2.1499e+00, 1.9243e+00, ..., 2.2489e+00, 2.2489e+00, 2.2489e+00], ..., [-7.8885e-02, -8.4444e-02, -1.8854e-01, ..., -3.3191e-02, 1.6326e-01, -2.5189e-02], [-3.9591e-02, -3.7761e-02, -3.5708e-02, ..., 4.1777e-01, 3.0722e-01, -8.4517e-02], [-5.5125e-01, -3.7390e-01, -3.7190e-01, ..., 1.6706e-01, -3.8756e-02, -3.0213e-01]], [[ 3.3600e-01, 1.2690e-01, 8.4595e-02, ..., 2.4286e+00, 2.4286e+00, 2.4286e+00], [ 1.3936e+00, 1.5396e+00, -7.8121e-02, ..., 2.4286e+00, 2.4286e+00, 2.4286e+00], [ 1.7230e+00, 1.8204e+00, 1.5281e+00, ..., 2.4286e+00, 2.4286e+00, 2.4286e+00], ..., [-2.6621e-01, -3.4865e-01, -5.4389e-01, ..., 1.5566e-02, 3.6483e-01, 3.7018e-01], [-2.3416e-01, -2.9848e-01, -3.8383e-01, ..., 4.3211e-01, 5.4771e-01, 3.7147e-01], [-7.7599e-01, -6.7812e-01, -7.3404e-01, ..., 2.9308e-01, 2.0118e-01, 3.7493e-02]], [[-6.9486e-02, -3.3152e-01, -5.6258e-01, ..., 2.6400e+00, 2.6400e+00, 2.6400e+00], [ 9.0693e-01, 9.7337e-01, -5.6124e-01, ..., 2.6400e+00, 2.6400e+00, 2.6400e+00], [ 1.2463e+00, 1.1590e+00, 8.0907e-01, ..., 2.6400e+00, 2.6400e+00, 2.6400e+00], ..., [-3.1419e-01, -2.4941e-01, -4.5623e-01, ..., -6.5955e-01, -6.0038e-01, -8.8913e-01], [-2.6903e-01, -2.5050e-01, -3.9344e-01, ..., -3.7691e-01, -6.0662e-01, -9.9883e-01], [-6.3179e-01, -4.3123e-01, -5.1774e-01, ..., -7.1518e-01, -8.3215e-01, -9.5885e-01]]], ..., [[[ 2.6701e-03, 4.8764e-02, 1.3802e-01, ..., -3.5556e-01, -2.1186e-01, -6.3790e-02], [ 2.7203e-01, 2.9067e-01, 3.0956e-01, ..., -9.4003e-02, -5.8179e-02, -7.6002e-02], [ 3.5114e-01, 3.3277e-01, 3.2004e-01, ..., 2.0249e-02, -2.6842e-02, -4.4070e-02], ..., [ 1.9681e+00, 2.0169e+00, 2.0680e+00, ..., -2.0286e-01, 1.0193e-01, 3.1608e-01], [ 1.9411e+00, 2.0085e+00, 2.1026e+00, ..., -1.3970e-01, 1.2286e-01, 3.5735e-01], [ 1.8141e+00, 1.8327e+00, 1.9489e+00, ..., -1.0404e-01, 1.8111e-01, 3.2454e-01]], [[ 2.0398e-01, 2.9756e-01, 3.7903e-01, ..., -1.5909e-02, 4.7189e-02, 1.5181e-01], [ 5.4995e-01, 5.8114e-01, 6.0668e-01, ..., 2.2921e-02, 2.9592e-02, 1.2454e-01], [ 6.2037e-01, 6.1136e-01, 6.1487e-01, ..., 1.3991e-01, 7.2302e-02, 1.2691e-01], ..., [ 2.1161e+00, 2.1586e+00, 2.1592e+00, ..., 1.0077e-01, 4.3020e-01, 5.8235e-01], [ 2.0806e+00, 2.1535e+00, 2.2194e+00, ..., 1.1844e-01, 4.4620e-01, 5.9031e-01], [ 1.9537e+00, 1.9817e+00, 2.1045e+00, ..., 1.3738e-01, 4.2917e-01, 6.0165e-01]], [[-3.0177e-02, -3.0919e-02, 5.6294e-02, ..., 1.2119e-02, 2.9192e-01, 4.9523e-01], [ 1.4675e-01, 1.8120e-01, 2.0599e-01, ..., 6.5189e-02, 2.1124e-01, 4.7340e-01], [ 2.2902e-01, 2.3191e-01, 2.1012e-01, ..., 1.2057e-01, 1.4622e-01, 3.3338e-01], ..., [ 2.3455e+00, 2.3984e+00, 2.4285e+00, ..., -2.0567e-01, 8.8979e-02, 1.8777e-01], [ 2.3240e+00, 2.3670e+00, 2.4654e+00, ..., -1.8698e-01, 1.2802e-01, 2.0268e-01], [ 2.2811e+00, 2.2660e+00, 2.3926e+00, ..., -1.4246e-01, 1.2407e-01, 2.1404e-01]]], [[[ 2.1948e+00, 2.1682e+00, 2.1729e+00, ..., -8.0144e-02, -1.7157e-01, -2.2714e-01], [ 2.1642e+00, 2.1482e+00, 2.1615e+00, ..., -2.8867e-01, -3.3114e-01, -4.2198e-01], [ 2.1637e+00, 2.1500e+00, 2.1554e+00, ..., -5.3515e-01, -4.0755e-01, -3.9795e-01], ..., [ 1.0268e+00, 1.0389e+00, 1.0086e+00, ..., 2.1637e+00, 2.1637e+00, 2.1637e+00], [ 1.0284e+00, 1.0010e+00, 1.0453e+00, ..., 2.1637e+00, 2.1637e+00, 2.1637e+00], [ 1.0163e+00, 1.0296e+00, 1.0190e+00, ..., 2.1637e+00, 2.1637e+00, 2.1637e+00]], [[ 2.3155e+00, 2.2723e+00, 2.2853e+00, ..., 2.2712e-01, 2.1174e-01, 1.6661e-01], [ 2.2455e+00, 2.2432e+00, 2.2569e+00, ..., 7.0896e-02, 4.8820e-02, -4.3276e-02], [ 2.2668e+00, 2.2566e+00, 2.2656e+00, ..., -1.1109e-01, -5.4757e-02, -8.3705e-02], ..., [ 1.1080e+00, 1.0955e+00, 1.0469e+00, ..., 2.2754e+00, 2.2754e+00, 2.2754e+00], [ 1.1009e+00, 1.0698e+00, 1.0872e+00, ..., 2.2754e+00, 2.2754e+00, 2.2754e+00], [ 1.0910e+00, 1.1021e+00, 1.0618e+00, ..., 2.2754e+00, 2.2754e+00, 2.2754e+00]], [[ 2.5272e+00, 2.4830e+00, 2.4523e+00, ..., 8.5871e-01, 8.1094e-01, 7.8095e-01], [ 2.4560e+00, 2.4281e+00, 2.4305e+00, ..., 5.5803e-01, 4.9335e-01, 3.9940e-01], [ 2.4629e+00, 2.4525e+00, 2.4602e+00, ..., 1.7717e-01, 2.3084e-01, 2.2868e-01], ..., [ 1.1968e+00, 1.1985e+00, 1.1224e+00, ..., 2.4712e+00, 2.4712e+00, 2.4712e+00], [ 1.2228e+00, 1.1839e+00, 1.1597e+00, ..., 2.4712e+00, 2.4712e+00, 2.4712e+00], [ 1.2212e+00, 1.2520e+00, 1.1560e+00, ..., 2.4712e+00, 2.4712e+00, 2.4712e+00]]], [[[ 2.2489e+00, 2.2489e+00, 2.2489e+00, ..., 2.2312e+00, 2.2403e+00, 2.2489e+00], [ 2.2489e+00, 2.2489e+00, 2.2489e+00, ..., 2.2320e+00, 2.2485e+00, 2.2489e+00], [ 2.2489e+00, 2.2489e+00, 2.2489e+00, ..., 2.2291e+00, 2.2471e+00, 2.2489e+00], ..., [-1.7937e+00, -1.9148e+00, -1.9569e+00, ..., 7.9287e-01, 6.7453e-01, 7.8103e-01], [-1.6935e+00, -1.8518e+00, -1.8703e+00, ..., 4.8874e-01, 2.1611e-01, 1.1217e-01], [-1.6270e+00, -1.8958e+00, -1.8929e+00, ..., 8.0896e-01, 8.9964e-01, 1.0060e+00]], [[ 2.4286e+00, 2.4286e+00, 2.4286e+00, ..., 2.4109e+00, 2.4200e+00, 2.4286e+00], [ 2.4286e+00, 2.4286e+00, 2.4286e+00, ..., 2.4113e+00, 2.4282e+00, 2.4286e+00], [ 2.4286e+00, 2.4286e+00, 2.4286e+00, ..., 2.4083e+00, 2.4268e+00, 2.4286e+00], ..., [-1.4270e+00, -1.6502e+00, -1.6874e+00, ..., 7.1551e-01, 4.9632e-01, 6.5896e-01], [-1.2436e+00, -1.4754e+00, -1.4859e+00, ..., 2.8340e-01, -9.5874e-02, -1.2210e-01], [-1.1076e+00, -1.4404e+00, -1.4846e+00, ..., 6.4517e-01, 7.3422e-01, 8.5824e-01]], [[ 2.6400e+00, 2.6400e+00, 2.6400e+00, ..., 2.6223e+00, 2.6305e+00, 2.6147e+00], [ 2.6400e+00, 2.6400e+00, 2.6400e+00, ..., 2.6228e+00, 2.6396e+00, 2.6398e+00], [ 2.6400e+00, 2.6400e+00, 2.6400e+00, ..., 2.6198e+00, 2.6382e+00, 2.6400e+00], ..., [-1.0548e+00, -1.1392e+00, -1.2386e+00, ..., 6.2691e-01, 3.3431e-01, 4.8703e-01], [-8.5461e-01, -8.5948e-01, -9.4681e-01, ..., 2.0497e-01, -1.5078e-01, -2.6161e-01], [-9.2614e-01, -8.7474e-01, -8.2363e-01, ..., 6.1918e-01, 7.6773e-01, 8.3740e-01]]]], device=&#39;cuda:0&#39;), TensorCategory([25, 4, 27, 20, 12, 27, 31, 33, 14, 35, 16, 5, 22, 33, 3, 35, 3, 0, 32, 12, 1, 20, 18, 22, 15, 11, 13, 5, 35, 4, 22, 34, 15, 4, 3, 21, 5, 22, 27, 11, 15, 13, 14, 32, 13, 4, 7, 30, 9, 20, 7, 20, 9, 1, 6, 35, 23, 8, 14, 16, 18, 6, 2, 35], device=&#39;cuda:0&#39;)) . . Note: above and below is same . x,y = dls.one_batch() . . Understanding Labels . dls.vocab . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . dls.vocab[0] . &#39;Abyssinian&#39; . . Tip: vocab gives as all labels as text. . . What&#39;s inside the tensors? . y . TensorCategory([13, 35, 8, 36, 3, 10, 10, 14, 22, 1, 5, 5, 5, 0, 4, 7, 11, 33, 18, 25, 20, 3, 33, 0, 25, 15, 27, 9, 17, 25, 19, 26, 9, 0, 35, 5, 6, 1, 31, 14, 7, 9, 8, 27, 2, 7, 21, 13, 26, 17, 25, 30, 31, 5, 19, 17, 4, 12, 29, 8, 21, 33, 18, 9], device=&#39;cuda:0&#39;) . . Note: Targets as coded. . x . TensorImage([[[[-1.3790, -1.3778, -1.3984, ..., -0.1093, 0.1460, -0.0339], [-1.3243, -1.3580, -1.3804, ..., 0.0449, 0.0572, -0.0411], [-1.3337, -1.3652, -1.3996, ..., -0.0918, -0.1107, -0.0857], ..., [-0.4574, -0.3503, -0.3927, ..., -0.6010, -0.7011, -0.7119], [-0.3509, -0.1960, -0.2069, ..., -0.6884, -0.6634, -0.6341], [-0.3221, -0.3299, -0.3177, ..., -0.5625, -0.4453, -0.4082]], [[-1.6744, -1.6758, -1.6812, ..., -0.5800, -0.2989, -0.4613], [-1.5871, -1.6285, -1.6568, ..., -0.3977, -0.3745, -0.4682], [-1.5626, -1.6162, -1.6632, ..., -0.5169, -0.5306, -0.5112], ..., [-1.0813, -0.9612, -0.9992, ..., -1.1308, -1.2921, -1.3514], [-0.9441, -0.7857, -0.7948, ..., -1.2437, -1.2741, -1.3164], [-0.9350, -0.9371, -0.9192, ..., -1.1914, -1.1397, -1.1095]], [[-1.7511, -1.7434, -1.7629, ..., -0.6031, -0.3360, -0.5057], [-1.6791, -1.7414, -1.7652, ..., -0.4428, -0.4310, -0.5192], [-1.6424, -1.7149, -1.7630, ..., -0.5840, -0.6148, -0.5747], ..., [-1.6312, -1.4844, -1.5600, ..., -1.3854, -1.6552, -1.7876], [-1.4652, -1.2946, -1.3208, ..., -1.5283, -1.6561, -1.7149], [-1.4120, -1.4189, -1.4288, ..., -1.5720, -1.5691, -1.5458]]], [[[-1.1709, -1.0320, -0.3882, ..., -2.0315, -2.0706, -2.0406], [-0.7207, -1.2576, -0.8119, ..., -2.0559, -2.0684, -2.0728], [-0.2858, -0.7315, -1.1736, ..., -2.0433, -2.0766, -2.0962], ..., [-0.6050, -0.6222, -0.7002, ..., 0.0488, 0.0771, 0.0815], [-0.6429, -0.6763, -0.7053, ..., 0.1518, -0.0409, 0.1402], [-0.6518, -0.7125, -0.7378, ..., 0.1249, 0.0496, 0.1191]], [[-0.8777, -0.6010, 0.1436, ..., -1.6740, -1.8200, -1.7518], [-0.3933, -0.8562, -0.2790, ..., -1.7451, -1.8429, -1.8417], [ 0.0344, -0.4096, -0.7499, ..., -1.8033, -1.8966, -1.8918], ..., [-0.6012, -0.6491, -0.6763, ..., 0.2168, 0.2401, 0.2356], [-0.6647, -0.7301, -0.6992, ..., 0.3333, 0.1283, 0.3030], [-0.6832, -0.7672, -0.7454, ..., 0.2804, 0.2115, 0.2686]], [[-0.6769, -0.6395, 0.1047, ..., -1.7038, -1.7277, -1.7018], [-0.1480, -0.7290, -0.2868, ..., -1.7481, -1.7221, -1.7096], [ 0.2469, -0.1654, -0.6479, ..., -1.7517, -1.7447, -1.7343], ..., [-0.5935, -0.7393, -0.8030, ..., 0.4358, 0.4557, 0.3989], [-0.6801, -0.8271, -0.8223, ..., 0.5317, 0.3682, 0.4378], [-0.7225, -0.8686, -0.8416, ..., 0.4712, 0.3960, 0.3788]]], [[[ 0.4054, 0.4157, 0.4160, ..., 0.4003, 0.3259, 0.1861], [ 0.4487, 0.4643, 0.4645, ..., 0.3877, 0.2998, 0.1810], [ 0.4725, 0.4946, 0.4947, ..., 0.3752, 0.2639, 0.1651], ..., [-0.5970, -0.4684, -0.6175, ..., 1.5827, 1.6680, 1.6609], [-0.5992, -0.5694, -0.4757, ..., 1.2104, 1.3103, 1.3862], [-0.6427, -0.7463, -0.7177, ..., 0.8878, 0.8792, 1.0099]], [[ 0.9298, 0.9404, 0.9407, ..., 0.9246, 0.8562, 0.7873], [ 0.9741, 0.9901, 0.9902, ..., 0.9117, 0.8413, 0.7847], [ 0.9984, 1.0209, 1.0210, ..., 0.8998, 0.8208, 0.7683], ..., [-0.1210, 0.0165, -0.1429, ..., 1.7798, 1.8600, 1.8469], [-0.1235, -0.0915, 0.0087, ..., 1.4962, 1.5836, 1.6497], [-0.1703, -0.2819, -0.2510, ..., 1.2888, 1.2676, 1.3861]], [[ 1.4446, 1.4550, 1.4553, ..., 1.4395, 1.4119, 1.4488], [ 1.4881, 1.5037, 1.5038, ..., 1.4270, 1.4177, 1.4495], [ 1.5118, 1.5338, 1.5339, ..., 1.4189, 1.4155, 1.4334], ..., [ 0.3907, 0.5309, 0.3682, ..., 2.0347, 2.1100, 2.0935], [ 0.3881, 0.4208, 0.5230, ..., 1.8240, 1.9038, 1.9552], [ 0.3403, 0.2258, 0.2576, ..., 1.6937, 1.6618, 1.7696]]], ..., [[[-0.7000, -0.6986, -0.7306, ..., -1.6313, -1.7078, -1.6480], [-0.6932, -0.6908, -0.7237, ..., -1.5230, -1.6776, -1.6388], [-0.6817, -0.6618, -0.6940, ..., -1.2779, -1.5299, -1.5905], ..., [ 0.7912, 0.9871, 0.9476, ..., 0.7637, 0.8491, 0.8404], [ 0.6435, 0.6536, 0.5389, ..., 0.2268, 0.2840, 0.7647], [ 0.2871, 0.1747, 0.0099, ..., 0.1704, 0.2587, 0.7739]], [[-0.5360, -0.5255, -0.5556, ..., -1.7426, -1.8394, -1.8137], [-0.5545, -0.5179, -0.5420, ..., -1.6711, -1.7999, -1.8003], [-0.5629, -0.4964, -0.5208, ..., -1.4790, -1.7004, -1.7559], ..., [ 0.8302, 1.0338, 0.9972, ..., 0.5573, 0.6361, 0.5920], [ 0.6521, 0.6515, 0.5315, ..., -0.0175, 0.0396, 0.5232], [ 0.2739, 0.1468, -0.0415, ..., -0.0737, 0.0125, 0.5333]], [[-0.1682, -0.1622, -0.1925, ..., -1.7371, -1.7819, -1.7964], [-0.1791, -0.1565, -0.1929, ..., -1.7217, -1.7671, -1.7829], [-0.1814, -0.1405, -0.1879, ..., -1.6271, -1.7128, -1.7439], ..., [ 0.7712, 1.0543, 1.0645, ..., 0.3954, 0.4138, 0.3209], [ 0.5921, 0.6752, 0.5766, ..., -0.2356, -0.1997, 0.2561], [ 0.2282, 0.1419, -0.0471, ..., -0.3060, -0.2291, 0.2662]]], [[[-1.7039, -1.5743, -0.7309, ..., -1.4899, -1.5223, -1.7014], [-1.6543, -1.3360, -0.7863, ..., -1.4862, -1.4992, -1.6543], [-1.4747, -0.9363, -0.9740, ..., -1.4786, -1.7038, -1.7715], ..., [-1.0359, -0.9016, -0.9339, ..., 1.1125, 1.1213, 0.7437], [-0.9960, -1.1363, -1.0869, ..., 0.8008, 1.0108, 0.9147], [-1.1167, -1.2009, -1.1964, ..., 0.6893, 1.3224, 0.2577]], [[-1.5970, -1.4079, -0.4739, ..., -1.2143, -1.3011, -1.4974], [-1.5914, -1.1409, -0.5364, ..., -1.1936, -1.2596, -1.4214], [-1.4410, -0.7377, -0.7863, ..., -1.1843, -1.4621, -1.5348], ..., [-0.6342, -0.4864, -0.5238, ..., 1.4785, 1.5627, 1.1883], [-0.6105, -0.7573, -0.7006, ..., 1.0617, 1.3020, 1.2896], [-0.7374, -0.8229, -0.8292, ..., 0.9343, 1.6271, 0.5592]], [[-1.7608, -1.7676, -1.6709, ..., -1.7209, -1.7915, -1.7875], [-1.7147, -1.7388, -1.6955, ..., -1.7866, -1.7723, -1.7779], [-1.6931, -1.5459, -1.6110, ..., -1.7689, -1.7772, -1.7837], ..., [-1.5969, -1.4913, -1.5156, ..., -0.6912, -0.3672, -0.7122], [-1.5018, -1.6808, -1.6485, ..., -0.5699, -0.2657, -0.5272], [-1.5775, -1.6876, -1.6868, ..., -0.0425, 0.4928, -0.8241]]], [[[ 1.4877, 1.4668, 1.5048, ..., 2.0390, 2.0327, 2.0266], [ 1.5198, 1.4866, 1.5066, ..., 2.0343, 2.0305, 2.0289], [ 1.5237, 1.4689, 1.5328, ..., 2.0266, 2.0217, 2.0152], ..., [ 1.4687, 1.8343, 1.9416, ..., -1.3055, -1.2876, -1.3210], [ 1.8619, 1.9001, 1.8640, ..., -1.1670, -1.3076, -1.3698], [ 1.8915, 1.8637, 1.8954, ..., -1.1410, -1.3742, -1.3414]], [[ 1.8535, 1.8615, 1.8702, ..., 2.2140, 2.2075, 2.2014], [ 1.8948, 1.9005, 1.8962, ..., 2.2092, 2.2053, 2.2037], [ 1.8926, 1.8579, 1.8944, ..., 2.2014, 2.2056, 2.2128], ..., [ 1.8888, 2.1500, 2.2057, ..., -1.2930, -1.2397, -1.3169], [ 2.2001, 2.2026, 2.1672, ..., -1.2065, -1.2288, -1.3877], [ 2.1885, 2.1623, 2.2023, ..., -1.2037, -1.3027, -1.3650]], [[ 2.0973, 2.0843, 2.1081, ..., 2.4264, 2.4200, 2.4138], [ 2.1341, 2.1351, 2.1528, ..., 2.4216, 2.4177, 2.4161], [ 2.1146, 2.0894, 2.1310, ..., 2.4138, 2.4146, 2.4143], ..., [ 2.3444, 2.4653, 2.4575, ..., -1.1536, -1.0986, -1.1590], [ 2.5056, 2.5002, 2.4774, ..., -1.0442, -1.1040, -1.1783], [ 2.5015, 2.5031, 2.5432, ..., -1.0341, -1.1608, -1.1347]]]], device=&#39;cuda:0&#39;) . . Note: Our stacked image tensor. . . Predictions of the baseline model. . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([1.4670e-06, 1.2070e-06, 8.4748e-07, 1.6964e-07, 7.0972e-06, 9.3213e-07, 1.9146e-06, 4.0787e-07, 1.3208e-06, 1.8394e-06, 1.8446e-08, 2.0282e-05, 9.3669e-04, 9.9753e-01, 4.6090e-06, 4.6171e-05, 8.3924e-05, 4.4448e-04, 3.7151e-07, 7.7943e-07, 6.8438e-06, 7.1965e-07, 2.7995e-07, 1.9403e-06, 1.0657e-06, 7.8017e-07, 1.8254e-05, 5.4245e-06, 4.5678e-06, 8.7494e-07, 3.8811e-06, 1.2178e-06, 6.4576e-07, 1.8837e-05, 8.5143e-04, 1.4807e-06, 1.7899e-06]) . . Note: result for first item that adds up to one. There are 37 outputs for 37 image categories and the results are in percentage for probability of each category. . _ . TensorCategory([13, 35, 8, 36, 3, 10, 10, 14, 22, 1, 5, 5, 5, 0, 4, 7, 11, 33, 18, 25, 20, 3, 33, 0, 25, 15, 27, 9, 17, 25, 19, 26, 9, 0, 35, 5, 6, 1, 31, 14, 7, 9, 8, 27, 2, 7, 21, 13, 26, 17, 25, 30, 31, 5, 19, 17, 4, 12, 29, 8, 21, 33, 18, 9]) . . Note: Category codes . len(preds[0]),preds[0].sum() . (37, tensor(1.0000)) . Prediction for 37 categories that adds up to one. . . FUNCTION FOR CLASSIFIYING MORE THAN TWO CATEGORY . For classifiying more than two category, we need to employ a new function. It is not totally different than sigmoid, in fact it starts with a sigmoid function. . plot_function(torch.sigmoid, min=-4,max=4) . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/RangeFactories.cpp:25.) x = torch.linspace(min,max) . . Note: This is how torch.sigmoid squishes values between 0 and 1. . torch.random.manual_seed(42); . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . . Note: These are random numbers represent binary results of a hypothetical network. First colums represent 3&#8217;s the and second is 7&#8217;s standart deviation of 2. It generally shows how confident the model about the predictions. . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . . Note: If we apply the sigmoid, the result become like this(above). Obviously they aren&#8217;t adds up to one. These are relative confidence over inputs. For example first row says: it&#8217;s a three. But what is the probability? It is not clear. . (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . . Note: If we take the difference between these relative confidence the results become like this above: Now we can say that for the first item, model is 0.6025 (%60.25) confident. . this part is a bit different in the lesson video. so check the video. 1:35:20 . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . . Note: torch.softmax does that in one step. Now results for each item adds up to one and identical. . . Log Likelihood . targ = tensor([0,1,0,1,1,0]) . this is our softmax activations: . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . . Note: Nice trick for getting confidence level for each item. . lets see everything in a table: . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . . Warning: I think the last label is wrong here. It must be the confidence instead. . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . . Warning: There is a caveat here. These are neg of our confidence level, not loss. . Pytorch way of doing the same here: . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . . Note: Anyway, numbers are still not right, that will be addresses in the Taking the Log section below. The reason is F.nll_loss (negative log likelihood loss) needs arguments such that log is already applied to make the calculation right.(loss) . . Taking the Log . . Note: Directly from the book: . . Important: Confusing Name, Beware: The nll in nll_loss stands for &quot;negative log likelihood,&quot; but it doesn&#8217;t actually take the log at all! It assumes you have already taken the log. PyTorch has a function called log_softmax that combines log and softmax in a fast and accurate way. nll_loss is designed to be used after log_softmax. . When we first take the softmax, and then the log likelihood of that, that combination is called cross-entropy loss. In PyTorch, this is available as nn.CrossEntropyLoss (which, in practice, actually does log_softmax and then nll_loss): . pytorch&#39;s crossEntropy: . loss_func = nn.CrossEntropyLoss() . loss_func(acts, targ) . tensor(1.8045) . or: . F.cross_entropy(acts, targ) . tensor(1.8045) . . Note: this is the mean of all losses . and this is all results without taking the mean: . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . . Note: Results above are cross entrophy loss for each image in the list (of course our current numbers are fake numbers) . . Manual calculation log_softmax + nll_loss . First log_softmax: . log_sm_acts = torch.log_softmax(acts, dim=1) log_sm_acts . tensor([[-5.0672e-01, -9.2248e-01], [-6.8903e-01, -6.9729e-01], [-2.0160e+00, -1.4293e-01], [-3.3658e-03, -5.6958e+00], [-5.1760e-01, -9.0621e-01], [-1.0048e+00, -4.5589e-01]]) . Then negative log likelihood: . F.nll_loss(log_sm_acts, targ, reduction=&#39;none&#39;) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . . Note: Results are identical . . REVISITING THE BASELINE MODEL (Model Interpretation) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 8), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 7), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 5)] . this is our baseline we can start improveing from this point. . . IMPROVING THE MODEL . Fine Tune . Fine tune the model with default arguments: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.588707 | 4.300000 | 0.445873 | 00:21 | . epoch train_loss valid_loss error_rate time . 0 | 3.385068 | 2.263443 | 0.510825 | 00:26 | . . Note: This is where we overshot. Our loss just increase over second epoch is there a better way to find a learning rate? . . Learning Rate Finder . learn = cnn_learner(dls, resnet34, metrics=error_rate) . suggested_lr= learn.lr_find() . /home/niyazi/anaconda3/envs/fastbook/lib/python3.8/site-packages/fastai/callback/schedule.py:270: UserWarning: color is redundantly defined by the &#39;color&#39; keyword argument and the fmt string &#34;ro&#34; (-&gt; color=&#39;r&#39;). The keyword argument will take precedence. ax.plot(val, idx, &#39;ro&#39;, label=nm, c=color) . . Warning: There is a discrepancy between lesson and reading group notebooks. In the book we get two values from the function but in reading group, only one. I thing there was an update for this function that not reflected in the book. . suggested_lr . SuggestedLRs(valley=tensor(0.0008)) . print(f&quot;suggested: {suggested_lr.valley:.2e}&quot;) . suggested: 8.32e-04 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=8.32e-04) . epoch train_loss valid_loss error_rate time . 0 | 2.203637 | 0.456601 | 0.139378 | 00:21 | . epoch train_loss valid_loss error_rate time . 0 | 0.631289 | 0.287444 | 0.087280 | 00:26 | . 1 | 0.423191 | 0.263927 | 0.085250 | 00:26 | . At this time it decreases steadily . What&#39;s under the hood of fine_tune . When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things: . Trains the randomly added layers for one epoch, with all other layers frozen | Unfreezes all of the layers, and trains them all for the number of epochs requested | . Lets do it manually . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 8.32e-04) . epoch train_loss valid_loss error_rate time . 0 | 1.806578 | 0.363257 | 0.114344 | 00:21 | . 1 | 0.697060 | 0.258624 | 0.083221 | 00:22 | . 2 | 0.449906 | 0.254586 | 0.087957 | 00:21 | . learn.unfreeze() . Run the lr_find again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn&#39;t appropriate any more: . learn.lr_find() . SuggestedLRs(valley=tensor(0.0001)) . Train again with the new lr. . learn.fit_one_cycle(6, lr_max=0.0001) . epoch train_loss valid_loss error_rate time . 0 | 0.369805 | 0.265072 | 0.085250 | 00:26 | . 1 | 0.379721 | 0.352767 | 0.112314 | 00:26 | . 2 | 0.320787 | 0.257370 | 0.075778 | 00:26 | . 3 | 0.198347 | 0.217450 | 0.066306 | 00:27 | . 4 | 0.143628 | 0.217090 | 0.066306 | 00:26 | . 5 | 0.111457 | 0.216973 | 0.066306 | 00:27 | . So far so good but there is more way to go . . Discriminative Learning Rates . Basically we use variable learning rate for the model. Bigger rate for the later layers and smaller for early layers. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 8.32e-04)# first lr learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(0.00005,0.0005))#second lr with a range . epoch train_loss valid_loss error_rate time . 0 | 1.783345 | 0.370482 | 0.119080 | 00:22 | . 1 | 0.700986 | 0.293102 | 0.096076 | 00:22 | . 2 | 0.448751 | 0.262937 | 0.093369 | 00:22 | . epoch train_loss valid_loss error_rate time . 0 | 0.390943 | 0.245929 | 0.079838 | 00:28 | . 1 | 0.356807 | 0.281976 | 0.088633 | 00:27 | . 2 | 0.344888 | 0.417350 | 0.117727 | 00:27 | . 3 | 0.267143 | 0.284152 | 0.081867 | 00:27 | . 4 | 0.217775 | 0.330306 | 0.092693 | 00:28 | . 5 | 0.172308 | 0.310047 | 0.081191 | 00:27 | . 6 | 0.122903 | 0.299161 | 0.079161 | 00:27 | . 7 | 0.099924 | 0.262270 | 0.074425 | 00:27 | . 8 | 0.059424 | 0.278250 | 0.074425 | 00:27 | . 9 | 0.045987 | 0.253283 | 0.067659 | 00:27 | . 10 | 0.036630 | 0.251685 | 0.068336 | 00:27 | . 11 | 0.034524 | 0.254469 | 0.067659 | 00:27 | . It is better most of the times.(sometimes I don&#39;t get good results, need to arrange the slice values more carefully) . learn.recorder.plot_loss() . . Note: Directly from the book: . As you can see, the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does not mean that it is getting less accurate, necessarily. Take a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we&#39;ve given the computer to help us to optimize. . . Important: I need to think about it how loss increase and accuracy stil becoming better. . Deeper Architectures . In general, a bigger model has the ability to better capture the real underlying relationships in your data, and also to capture and memorize the specific details of your individual images. However, using a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an out-of-memory error. This happens when you try to fit too much inside your GPU and looks like: . Cuda runtime error: out of memory . You may have to restart your notebook when this happens. The way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your DataLoaders with bs=. . The other downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is mixed-precision training. This refers to using less-precise numbers (half-precision floating point, also called fp16) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called tensor cores that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module). . You can&#39;t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let&#39;s try a ResNet-50 now with mixed precision: . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(12, freeze_epochs=3) . epoch train_loss valid_loss error_rate time . 0 | 1.209030 | 0.308840 | 0.097429 | 00:20 | . 1 | 0.562807 | 0.326714 | 0.100812 | 00:21 | . 2 | 0.396488 | 0.263611 | 0.089310 | 00:21 | . epoch train_loss valid_loss error_rate time . 0 | 0.255827 | 0.262954 | 0.080514 | 00:24 | . 1 | 0.215601 | 0.256829 | 0.072395 | 00:24 | . 2 | 0.238660 | 0.392900 | 0.099459 | 00:23 | . 3 | 0.246021 | 0.409503 | 0.107578 | 00:24 | . 4 | 0.196632 | 0.448040 | 0.106225 | 00:23 | . 5 | 0.137433 | 0.353745 | 0.091340 | 00:23 | . 6 | 0.108764 | 0.333932 | 0.085250 | 00:24 | . 7 | 0.078872 | 0.295772 | 0.081867 | 00:24 | . 8 | 0.055900 | 0.273311 | 0.073072 | 00:24 | . 9 | 0.040353 | 0.274645 | 0.070365 | 00:24 | . 10 | 0.020883 | 0.260611 | 0.070365 | 00:24 | . 11 | 0.021018 | 0.259633 | 0.066982 | 00:24 | . learn.recorder.plot_loss() . As above traing time is not changed much. .",
            "url": "https://niyazikemer.com/fastbook/2021/07/22/chapter-5.html",
            "relUrl": "/fastbook/2021/07/22/chapter-5.html",
            "date": " • Jul 22, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Chapter 4 - Under the Hood - Training a Digit  Classifier",
            "content": ". I found this little one in front of my window. Suffering from foot deformity and can&#39;t fly. Now fully recovered and back his/her family. . import fastbook fastbook.setup_book() # below is for disabling Jedi autocomplete that doesn&#39;t work well. %config Completer.use_jedi = False . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . EXPLORING THE DATASET . What untar does? . . Note: &#8217;untar_data&#8217; come from fastai library, it downloads the data and untar it if it didn&#8217;t already and returns the destination folder. . path = untar_data(URLs.MNIST_SAMPLE) . ??untar_data . . Tip: Check it with &#8217;??&#8217; . What is path ? . path . Path(&#39;.&#39;) . . Tip: what is inside the current folder? this where the jupyter notebook works. &#8217;!&#8217; at the beginning means the command works on the terminal. . What is !ls ? . . Note: ls works on the terminal. (-d for only listing directories) . !ls . 2020-02-20-test.ipynb ghtop_images my_icons 2021-07-16-chapter-4.ipynb images README.md . can be used like this too. . !ls /home/niyazi/.fastai/data/mnist_sample/train -d . /home/niyazi/.fastai/data/mnist_sample/train . also like this: . !ls /home/niyazi/.fastai/data/mnist_sample/train/3 -d . /home/niyazi/.fastai/data/mnist_sample/train/3 . What is tree ? . . Note: for seeing tree sturucture of the files and folders (-d argument for directories) . !tree /home/niyazi/.fastai/data/mnist_sample/ -d . /home/niyazi/.fastai/data/mnist_sample/ ├── train │   ├── 3 │   └── 7 └── valid ├── 3 └── 7 6 directories . Path.BASE_PATH = path . What is ls() ? . . Note: &#8217;ls&#8217; is method by fastai similiar the Python&#8217;s list fuction but more powerful. . path.ls() . (#3) [Path(&#39;labels.csv&#39;),Path(&#39;train&#39;),Path(&#39;valid&#39;)] . . Note: Check this usage: . (path/&#39;train&#39;) . Path(&#39;train&#39;) . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . . Note: there are two folders under training folder . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() . . Note: this code returns and ordered list of paths . What is PIL ? (Python Image Library) . im3_path = threes[1] im3 = Image.open(im3_path) type(im3) #im3 . PIL.PngImagePlugin.PngImageFile . NumPy array . The 4:10 indicates we requested the rows from index 4 (included) to 10 (not included) and the same for the columns. NumPy indexes from top to bottom and left to right, so this section is located in the top-left corner of the image. . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . . Note: this is how it looks some part of the image in the NumPy array . Pytorch tensor . Here&#39;s the same thing as a PyTorch tensor: . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . . Note: It is possible to convert it to a tansor as well. . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;OrRd&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . BASELINE: Pixel similarity . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . . Note: &#8217;sevens&#8217; are still list of paths. &#8217;o&#8217; is a path in the list, then with the list comprehension we use the path to read the image, then cast the image into tensor.(Same for threes). &#8217;seven_tensor&#8217; is a list of tensors . show_image(three_tensors[0]); . . Tip: Show image shows the first tensor as image . show_image(tensor(im3)) . &lt;AxesSubplot:&gt; . . Note: check this in more straight way (im3&gt;tensor&gt;image) . Training Set: Stacking Tensors . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . type(stacked_sevens) . torch.Tensor . type(stacked_sevens[0]) . torch.Tensor . type(seven_tensors) . list . . Note: now we turn our list into a tensor size of ([6131, 28, 28]) . len(stacked_threes.shape) . 3 . . Note: This is rank (lenght of the shape) . stacked_threes.ndim . 3 . . Note: This is more direct way to get it. (ndim) . Mean of threes and sevens our ideal 3 and 7. . mean3 = stacked_threes.mean(0) show_image(mean3); . . Note: This is the mean of the all tensors through first axis. &#8217;Ideal Three&#8217; . mean7 = stacked_sevens.mean(0) show_image(mean7); . a_3 = stacked_threes[1] show_image(a_3); . Distance between the ideal three and other threes . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . . Note: Then we need to calculate the distance between the &#8217;ideal&#8217; and ordinary three.Two methods for getting the distance L1 Norm and MSE second one is panelize bigger mistake more havil, L1 is uniform. It is obvious that a_3 is closer to the perfect 3 so our approach worked at this time. (Both in L1 and MSE) . Pytorch L1 and MSE fuctions . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . . Note: torch.nn.functional as F (for mse, manually take the sqrt) . . Important: (from notebook) If you don&#8217;t know what C is, don&#8217;t worry as you won&#8217;t need it at all. In a nutshell, it&#8217;s a low-level (low-level means more similar to the language that computers use internally) language that is very fast compared to Python. To take advantage of its speed while programming in Python, try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors. . Array and Tensor Examples . data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) . arr # numpy . array([[1, 2, 3], [4, 5, 6]]) . tns # pytorch . tensor([[1, 2, 3], [4, 5, 6]]) . Splitting, adding, multiplying tensors . tns[:,1] . tensor([2, 5]) . tns[1,1:3] . tensor([5, 6]) . tns+1 . tensor([[2, 3, 4], [5, 6, 7]]) . tns.type() . &#39;torch.LongTensor&#39; . tns*1.5 . tensor([[1.5000, 3.0000, 4.5000], [6.0000, 7.5000, 9.0000]]) . Validation set :Stacking Tensors . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Manual L1 distance function . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . This is broadcasting: . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1270, 0.1254, 0.1114, ..., 0.1494, 0.1097, 0.1365]), torch.Size([1010])) . . Note: I think this an example of not using loops which slows down the process (check above important tag). Although shapes of the tensors don&#8217;t match, out function still works. Pytorch fills the gaps. . here is another example. Shapes don&#39;t match. . tensor([1,2,3]) + tensor(1) . tensor([2, 3, 4]) . (valid_3_tens-mean3).shape . torch.Size([1010, 28, 28]) . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . here is an another broadcasting for all validation set: . is_3(valid_3_tens) . tensor([True, True, True, ..., True, True, True]) . Accuracy of our &#39;ideal&#39; 3 and 7 . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . STOCHASTIC GRADIENT DECENT (SGD) . Arthur Samues Machine Learning process: . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss (SGD) | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&#39;t want to wait any longer). | . #caption The gradient descent process #alt Graph showing the steps for Gradient Descent gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop GD example . def f(x): return x**2 . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . We need to decrease the loss . . How to calculate gradient: . Now our tensor xt is under investigation. Pytorch will keeps its eye on it. . xt = tensor(3.).requires_grad_() . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . Result is 9 but there is a grad function in the result. . . yt.backward() . backward calculates the derivative. . xt.grad . tensor(6.) . result is 6. . . now with a bigger tensor . xt = tensor([3.,4.,10.]).requires_grad_() xt . tensor([ 3., 4., 10.], requires_grad=True) . def f(x): return (x**2).sum() . yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . again we expect 2*xt: . yt.backward() . xt.grad . tensor([ 6., 8., 20.]) . End to end SGD example . time = torch.arange(0,20).float() time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed) . &lt;matplotlib.collections.PathCollection at 0x7f8f2edd27c0&gt; . Now we are trying to come up with some parameters for our quadratic fuction that predicts speed any given time. Our choice is quadratic but that could be something else too. with a quadratic function our problem would be much easier. . here is the function gets time and parameter as inputs and predicts a result: . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . this our loss function that calculate distance between prediction and target( actual mesurements) . def mse(preds, targets): return ((preds-targets)**2).mean().sqrt() . Step 1: here are initial random parameters: . params = torch.randn(3).requires_grad_() params . tensor([ 0.9569, 0.0048, -0.1506], requires_grad=True) . orig_params = params.clone() . Step 2: calculate predictions: . preds = f(time,params) . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . Step 3: Calculate the loss . loss = mse(preds,speed) loss . tensor(139.3082, grad_fn=&lt;SqrtBackward&gt;) . . The Question is how to improve these results: . Step 4: first we calculate the gradient: . Pytorch makes it easier we just call the backward() on the loss but it calculates gradient for the params &#39;a&#39; &#39;b&#39; and &#39;c&#39;._ . loss.backward() params.grad # this is the derivative of the initial values in other word our slope. . tensor([165.0324, 10.5991, 0.6615]) . params.grad * 1e-5 # scaler at the end is learning rate. . tensor([1.6503e-03, 1.0599e-04, 6.6150e-06]) . params # they are still same. . tensor([ 0.9569, 0.0048, -0.1506], requires_grad=True) . . Step 5: Step the weight. . we picked the learning rate 1e-5 very small step to avoid missing the lowest possible loss. . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . preds = f(time,params) mse(preds, speed) . tensor(139.0348, grad_fn=&lt;SqrtBackward&gt;) . lets create a function for all these steps . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: repeat the step: . for i in range(10): apply_step(params) . 139.03475952148438 138.76133728027344 138.4879150390625 138.2145538330078 137.94122314453125 137.6679229736328 137.39466857910156 137.12144470214844 136.84825134277344 136.5751190185547 . params = orig_params.detach().requires_grad_() . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . . MNIST . Loss Function our 3 and 7 recognizer. Currently we use metric not loss . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . train_x.size() . torch.Size([12396, 784]) . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . How tensor manipulated . temp_tensor = tensor (1) . temp_tensor . tensor(1) . type(temp_tensor) . torch.Tensor . is above tensor is wrong what&#39;s the difference? . . we have a tensor . temp_tensor = tensor([1]) . then we multiuplied the inside of . temp_tensor =tensor([1]*4) . temp_tensor . tensor([1, 1, 1, 1]) . temp_tensor.shape . torch.Size([4]) . temp_tensor.ndim . 1 . temp_tensor.size() . torch.Size([4]) . (temp_tensor).unsqueeze(1) . tensor([[1], [1], [1], [1]]) . . Warning: looked changed but why size is still unchanged why not [4,1] . temp_tensor.shape . torch.Size([4]) . temp_tensor.size() . torch.Size([4]) . How unsqueeze works? . . Warning: Whaaaaaaaaaaaaat? . (temp_tensor).unsqueeze(1) doesn&#39;t work but (temp_tensor*1).unsqueeze(1) you need to unsqueeze it when creating otherwise it doesnt work. I do not believe it. . temp_tensor = tensor([1]).unsqueeze(1) . temp_tensor.shape . torch.Size([1, 1]) . temp_tensor =tensor([1]*1).unsqueeze(1) . Dataset . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,x.ndim,y . (torch.Size([784]), 1, tensor([1])) . we create list of tuples, each tuple contains a image and a target . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . same for validation . . Weights . this is not clear on the videos but consider a layer NN of 728 inputs and 1 output. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . weights.shape . torch.Size([784, 1]) . bias = init_params(1) . . Note: The function weights*pixels won&#8217;t be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its intercept is 0). You might remember from high school math that the formula for a line is y=w*x+b; we still need the b. We&#8217;ll initialize it to a random number too: . bias . tensor([0.0959], requires_grad=True) . Again transposing the weight matrix is not clear but Tariq Rashed&#39;s book would be very beneficial at this point . (train_x[0]*weights.T).sum() + bias . tensor([-5.6867], grad_fn=&lt;AddBackward0&gt;) . for all dataset put this multiplication in a function . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[ -5.6867], [ -6.5451], [ -2.0241], ..., [-14.3286], [ 4.3505], [-12.6773]], grad_fn=&lt;AddBackward0&gt;) . Create a tensor with results based on their value (above 0.5 is 7 and below it is 3) . corrects = (preds&gt;0.5).float() == train_y corrects . tensor([[False], [False], [False], ..., [ True], [False], [ True]]) . . check it . corrects.float().mean().item() . 0.4636172950267792 . almost half of them is 3 and the other half is 7 (since weighs are totally random) . . Why we need a loss Function . Basically we need to have gradients for correcting our weighs, we need to know which direction we need to go . If you dont understand all of these, ckeck khan academy for gradient. . trgts = tensor([1,0,1]) prds = tensor([0.9, 0.4, 0.2]) . def mnist_loss(predictions, targets): return torch.where(targets==1, 1-predictions, predictions).mean() . torch.where(trgts==1, 1-prds, prds) . tensor([0.1000, 0.4000, 0.8000]) . mnist_loss(prds,trgts) . tensor(0.4333) . Sigmoid . We need this for squishing predictions between 0-1 . def sigmoid(x): return 1/(1+torch.exp(-x)) . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . update the fuction with the sigmoid thats all. . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . What are SGD and Mini-Batches . This explains most of it. . coll = range(15) dl = DataLoader(coll, batch_size=5, shuffle=True) list(dl) . [tensor([ 0, 2, 10, 13, 8]), tensor([11, 12, 4, 1, 5]), tensor([ 3, 14, 6, 9, 7])] . but this is only a list however we neeed a tuple consist of independent and dependent variable. . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . DataLoader . then put it into a Dataloader. . dl = DataLoader(ds, batch_size=6, shuffle=True) list(dl) . [(tensor([ 1, 23, 9, 8, 24, 2]), (&#39;b&#39;, &#39;x&#39;, &#39;j&#39;, &#39;i&#39;, &#39;y&#39;, &#39;c&#39;)), (tensor([14, 25, 13, 11, 19, 5]), (&#39;o&#39;, &#39;z&#39;, &#39;n&#39;, &#39;l&#39;, &#39;t&#39;, &#39;f&#39;)), (tensor([ 0, 10, 4, 7, 18, 12]), (&#39;a&#39;, &#39;k&#39;, &#39;e&#39;, &#39;h&#39;, &#39;s&#39;, &#39;m&#39;)), (tensor([ 6, 21, 15, 16, 22, 3]), (&#39;g&#39;, &#39;v&#39;, &#39;p&#39;, &#39;q&#39;, &#39;w&#39;, &#39;d&#39;)), (tensor([20, 17]), (&#39;u&#39;, &#39;r&#39;))] . now we have batches and tuples . . all together . It&#39;s time to implement the process we saw in &lt;&gt;. In code, our process will be implemented something like this for each epoch:&lt;/p&gt; for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; weights = init_params((28*28,1)) bias = init_params(1) . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . valid_dl = DataLoader(valid_dset, batch_size=256) . a small test . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . . predictions . preds = linear1(batch) preds . tensor([[ 8.0575], [14.3841], [-3.8017], [ 5.1179]], grad_fn=&lt;AddBackward0&gt;) . loss . loss = mnist_loss(preds, train_y[:4]) loss . tensor(0.2461, grad_fn=&lt;MeanBackward0&gt;) . gradients . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0010), tensor([-0.0069])) . for the step we need a optimizer . . put all into a function except the optimizer. . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0021), tensor([-0.0138])) . . Warning: if you do it twice results are change. . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0031), tensor([-0.0207])) . weights.grad.zero_() bias.grad.zero_(); . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . little conversion to our results, it&#39;s important because we need to understand that what our model says about the numbers(three or not three) . (preds&gt;0.0).float() == train_y[:4] . tensor([[ True], [ True], [False], [ True]]) . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . . this is training accuracy . batch_accuracy(linear1(batch), train_y[:4]) . tensor(0.7500) . this is for validation for all set . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5136 . Training . one epochs of training . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.7121 . then more . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8656 0.9203 0.9457 0.9549 0.9593 0.9623 0.9652 0.9666 0.9681 0.9705 0.9706 0.9711 0.972 0.973 0.9735 0.9735 0.974 0.9745 0.9755 0.9755 . Optimizer . Let&#39;s start creating our model with Pytorch instead of our &quot;linear1&quot; function. Pytorch also creates parameters like our init_params function. . linear_model = nn.Linear(28*28,1) . w,b = linear_model.parameters() . w.shape, b.shape . (torch.Size([1, 784]), torch.Size([1])) . Custom optimizer . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . opt = BasicOptim(linear_model.parameters(), lr) . new training fuction will be . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.4078 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.8193 0.8418 0.9136 0.9331 0.9477 0.9555 0.9629 0.9658 0.9673 0.9697 0.9717 0.9736 0.9751 0.9761 0.9761 0.9775 0.9775 0.9785 0.9785 . Fastai&#39;s SDG class . instead of using &quot;BasicOptim&quot; class we can use fastai&#39;s SGD class . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.7808 0.8623 0.9185 0.9365 0.9521 0.9575 0.9638 0.9658 0.9678 0.9707 0.9726 0.9741 0.9751 0.9761 0.9765 0.9775 0.978 0.9785 0.9785 . Just remove the &quot;train_model&quot; at this time and use fastai&#39;s &quot;Learner.fit&quot; Before using Learner first we need to pass our trainig and validation data into &quot;Dataloaders&quot; not &quot;dataloader&quot; . Fastai&#39;s Dataloaders . dls = DataLoaders(dl, valid_dl) . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . FastAi&#39;s Fit . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.637166 | 0.503575 | 0.495584 | 00:00 | . 1 | 0.562232 | 0.139727 | 0.900393 | 00:00 | . 2 | 0.204552 | 0.207935 | 0.806183 | 00:00 | . 3 | 0.088904 | 0.114767 | 0.904809 | 00:00 | . 4 | 0.046327 | 0.081602 | 0.930324 | 00:00 | . 5 | 0.029754 | 0.064530 | 0.944553 | 00:00 | . 6 | 0.022963 | 0.054135 | 0.954858 | 00:00 | . 7 | 0.019966 | 0.047293 | 0.961236 | 00:00 | . 8 | 0.018464 | 0.042515 | 0.965162 | 00:00 | . 9 | 0.017573 | 0.039011 | 0.966634 | 00:00 | . Adding a Nonlinearity . The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there&#39;s no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters. (From Fastbook) . Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for w1 and w2 and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the universal approximation theorem._ The three lines of code that we have here are known as layers. The first and third are known as linear layers, and the second line of code is known variously as a nonlinearity, or activation function.(From Fastbook) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.303284 | 0.398378 | 0.511776 | 00:00 | . 1 | 0.142384 | 0.221517 | 0.817959 | 00:00 | . 2 | 0.079702 | 0.112610 | 0.917076 | 00:00 | . 3 | 0.052855 | 0.076474 | 0.942100 | 00:00 | . 4 | 0.040301 | 0.059791 | 0.958783 | 00:00 | . 5 | 0.033824 | 0.050389 | 0.964181 | 00:00 | . 6 | 0.030075 | 0.044483 | 0.966143 | 00:00 | . 7 | 0.027629 | 0.040465 | 0.966634 | 00:00 | . 8 | 0.025865 | 0.037553 | 0.969578 | 00:00 | . 9 | 0.024499 | 0.035336 | 0.971541 | 00:00 | . 10 | 0.023391 | 0.033579 | 0.972522 | 00:00 | . 11 | 0.022467 | 0.032142 | 0.973994 | 00:00 | . 12 | 0.021679 | 0.030936 | 0.973994 | 00:00 | . 13 | 0.020997 | 0.029901 | 0.974975 | 00:00 | . 14 | 0.020398 | 0.028998 | 0.974975 | 00:00 | . 15 | 0.019869 | 0.028198 | 0.975957 | 00:00 | . 16 | 0.019395 | 0.027484 | 0.976448 | 00:00 | . 17 | 0.018966 | 0.026841 | 0.976938 | 00:00 | . 18 | 0.018577 | 0.026259 | 0.977429 | 00:00 | . 19 | 0.018220 | 0.025730 | 0.977429 | 00:00 | . 20 | 0.017892 | 0.025244 | 0.978410 | 00:00 | . 21 | 0.017588 | 0.024799 | 0.979882 | 00:00 | . 22 | 0.017306 | 0.024388 | 0.979882 | 00:00 | . 23 | 0.017042 | 0.024008 | 0.980373 | 00:00 | . 24 | 0.016794 | 0.023656 | 0.980864 | 00:00 | . 25 | 0.016561 | 0.023328 | 0.980864 | 00:00 | . 26 | 0.016341 | 0.023022 | 0.980864 | 00:00 | . 27 | 0.016133 | 0.022737 | 0.981845 | 00:00 | . 28 | 0.015935 | 0.022470 | 0.981845 | 00:00 | . 29 | 0.015746 | 0.022221 | 0.981845 | 00:00 | . 30 | 0.015566 | 0.021988 | 0.982336 | 00:00 | . 31 | 0.015395 | 0.021769 | 0.982336 | 00:00 | . 32 | 0.015231 | 0.021565 | 0.982826 | 00:00 | . 33 | 0.015076 | 0.021371 | 0.982826 | 00:00 | . 34 | 0.014925 | 0.021190 | 0.982826 | 00:00 | . 35 | 0.014782 | 0.021018 | 0.982826 | 00:00 | . 36 | 0.014643 | 0.020856 | 0.982826 | 00:00 | . 37 | 0.014510 | 0.020703 | 0.982826 | 00:00 | . 38 | 0.014382 | 0.020558 | 0.982826 | 00:00 | . 39 | 0.014258 | 0.020420 | 0.982826 | 00:00 | . recorder is a fast ai method . plt.plot(L(learn.recorder.values).itemgot(2)); . Last value . learn.recorder.values[-1][2] . 0.982826292514801 . GOING DEEPER . why deeper if it is two and a nonlinear between them is enough . We already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers. . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.089727 | 0.011755 | 0.997056 | 00:13 | . &lt;/div&gt; .",
            "url": "https://niyazikemer.com/fastbook/2021/07/16/chapter-4.html",
            "relUrl": "/fastbook/2021/07/16/chapter-4.html",
            "date": " • Jul 16, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://niyazikemer.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://niyazikemer.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://niyazikemer.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://niyazikemer.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}